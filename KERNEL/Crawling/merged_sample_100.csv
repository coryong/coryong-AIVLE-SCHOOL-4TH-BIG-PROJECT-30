title,body,image,url
@ITSUB,★경축★ 드디어 아이폰에 통화녹음 된다? 곧 정식으로 지원한다길래 야매(?)로 써봤습니다.,https://img.youtube.com/vi/y1xLSarZHa0/0.jpg,https://www.youtube.com/watch?v=y1xLSarZHa0
따끈따끈한 전사 로그 시스템 전환기: ELK Stack에서 Loki로 전환한 이유,itemname subname 안녕하세요 클라우드모니터링플랫폼팀의 이연수입니다 우아한형제들의 모니터링시스템 구축 및 관리 운영을 하고 있습니다 작년부터 올해 초까지 팀에서 전사 로그 시스템을 전환을 진행했던 계기와 전환 후 성과에 대해 이야기를 나누어보겠습니다 로그 시스템은 항상 도전 과제를 안겨줍니다 각 서비스에서 발생하는 대부분의 로그를 수집해야하기 때문에 종류가 많고 비용이 많이 발생하지 않는지 시스템 운영을 효율적으로 하고 있는지 데이터를 효율적으로 저장하고 있는지 등 항상 풀어야 하는 숙제가 있습니다 특히 로그 시스템은 HighWrite LowRead 패턴으로 많은 데이터를 수집하지만 그에 비해 조회는 수집된 모든 데이터를 대상으로 하지 않습니다 그렇기 때문에 수집과 조회를 각각 세심하게 관리해야 합니다 이 글에서는 ELK에서 Grafana Loki로 전환을 했던 과정을 통해 이런 고민들을 어떻게 해결했는지에 대해 소개하려 합니다 그리고 변화된 환경에서 로그 시스템을 어떻게 새롭게 구축했는지 함께 살펴보겠습니다 기존 로그 시스템 ELK 우아한형제들에서는 로그 시스템으로 ELK Stack을 사용하고 있었습니다 ELK Stack은 Elasticsearch Logstash Kibana의 앞글자를 딴 시스템으로 데이터를 수집 처리 조회하는 데 특화되어 있는 시스템이며 많은 회사에서 데이터 수집용도로 사용하고 있는 시스템입니다 특히 Elasticsearch는 강력한 검색 및 쿼리 기능을 제공하여 다양한 조건으로 로그를 검색하고 원하는 데이터를 쉽게 추출할 수 있습니다 또한 Logstash를 통해 다양한 데이터 형식을 수집하고 변환할 수 있습니다 이는 다양한 종류의 로그 및 이벤트 데이터를 효과적으로 처리할 수 있는 장점을 제공합니다 Elasticsearch에서는 단일 데이터 단위를 도큐먼트 라고 하며 이 도큐먼트를 모아놓은 집합을 인덱스 라고 합니다 이 시스템을 운영할 당시 매일 수집되는 로그의 용량이 컸기 때문에 팀에서는 인덱스를 서비스별 구분을 했고 매일 배치를 통해 인덱스 롤오버를 하여 일자별로 신규로 생성하였습니다 아래는 과거 구축했던 로그 시스템 아키텍처입니다 기존 로그 시스템 아키텍쳐 붉은색 Write Path 푸른색 Read Path ELK 서비스에서 마주한 한계와 문제점 약 3년간 이슈없이 운영되던 로그 시스템이 시간이 지나면서 몇 가지 문제들이 발생하기 시작했습니다 서비스 확장으로 인한 인덱스 부하가 발생했고 롤오버를 해도 인덱스 용량이 커지며 인덱스 내부의 샤드 수도 점점 늘어나 관리의 어려움으로 인해 많은 제약과 문제가 발생했습니다 로그 수집량이 증가하는 만큼 데이터 노드들의 볼륨의 크기가 커져 운영 부담과 비용도 늘어났습니다 스케일링을 위해 인덱스 및 샤드를 옮기는 작업을 할 때 볼륨이 커질 수록 소요되는 시간이 점점 길어졌습니다 이런 비용을 절감하기 위해 볼륨에 저장하는 로그 기간을 줄이기 시작했고 저장 기간별 단기3일 이내 디스크에 저장 중기60일 이내 스냅샷 생성 및 필요시 복원 후 조회 장기S3로 나누게 되었는데 이로인해 조회방법의 파편화가 발생했습니다 Elasticsearch의 높은 비용 대규모 클러스터 확장의 어려움이 점점 많아졌습니다 부하가 발생하면 비정기적으로 전체 클러스터에 대한 API 접근이 실패하여 로그 전송이 되지 않는 경우가 발생했고 그때마다 필요한 튜닝으로 통해 조치를 했지만 비슷한 이슈들이 반복되어 안정성에 대한 의문이 들었던 것도 다른 로그 시스템의 전환을 고민했던 계기가 되었습니다 Grafana Loki를 도입한 이유 ELK Stack에서 느끼는 한계와 이유들로 Grafana Loki로의 전환을 검토하게 되었습니다 Loki는 Grafana의 생태계인 LGTMLoki Grafana Tempo Mirmir의 L을 담당하며 Prometheus에서 영감을 받아 개발된 수평 확장이 가능하고 가용성 높은 다중 테넌트 로그 집계 시스템입니다 다른 로그 시스템과 달리 Loki는 로그에 대한 메타데이터만 인덱싱한다는 아이디어 즉 레이블을 기반으로 구축되었습니다 로그 내용을 색인화하는 것이 아니라 각 로그 스트림에 대한 레이블 세트를 색인화하기 때문에 레이블 단위로 로그는 별도로 압축되어 청크 단위로 저장이 되고 압축률에 따라 데이터가 감소 됩니다 로그에 대한 메타데이터인 레이블을 인덱싱한다는 아이디어를 중심으로 구성이 되었고 로그를 조회하기 위해 먼저 레이블 기반으로만 조회를 하고 매칭되는 레이블들과 매칭된 압축 로그 데이터를 가져와서 사용자에게 보여줍니다 레이블을 스트림화하고 청크를 별도 구분하여 압축하는 부분을 설명하는 그림 httpsgrafanacomdocslokiv28xfundamentalsarchitecturewritepath 또한 아래처럼 로그시스템을 구축할 때 Loki 내부적으로도 Write 컴포넌트Distributor Ingester와 Read 컴포넌트QueryFrontend QueryScheduler Querier IndexGateway를 나눌 수 있어 효율적인 운영이 가능하고 언제든 스케일링 가능한 유연성이 매력적으로 다가왔습니다 신규 로그 시스템 아키텍쳐 붉은색 Write Path 푸른색 Read Path Loki는 단기간의 데이터만 메모리에 저장하고 나머지는 S3와 같은 오브젝트 스토리지에 저장하기 때문에 로그 수집량과 저장기간이 늘어날수록 리소스와 관리 비용이 늘어났던 Elasticsearch 대비 비용과 관리 측면에서 절감할 수 있을 것이라 기대했습니다 또한 오브젝트 스토리지에 저장된 로그는 볼륨대비 비용이 적기때문에 제한기간을 두지 않고 조회를 할 수 부분이 장점이라고 생각했습니다 그리고 수집 조회 컴포넌트들이 구분되어 있어 이슈가 발생할 경우 특정 컴포넌트를 특정해서 조치할 수 있어 이슈 및 장애 조치도 쉬울 것이라 판단했습니다 하지만 걱정되는 부분들도 있었습니다 풀텍스트 검색은 Elasticsearch에 비해 느린 부분 쿼리 언어LogQL를 엔지니어들이 학습해야하는 부분 아직 사용하는 회사가 많지 않아 레퍼런스가 부족했습니다 하지만 쿼리 속도는 특정 컴포넌트 튜닝을 통해 성능향상이 가능하고 LogQL은 당시 메트릭 조회를 위해 사용하고 있는 PromQL과 유사한 부분이 많아 적응하는데 오래 걸리지 않을 것이라고 생각했습니다 그리고 부족한 레퍼런스의 경우도 필요하면 오픈소스의 코드를 보면서 분석하면 된다고 생각하여 여러 우려스러운 부분들은 충분히 극복할 수 있을 것이라고 판단했고 이런 고민들을 통해 Loki로 전환을 결정했습니다 Elasticsearch Loki 장단점 Grafana Loki 도입 후 변화와 실질적인 성과와 혜택 현재 우아한형제들의 로그 시스템은 Loki로 전환 완료되었습니다 전환 후 기존 ELK Stack를 사용하며 불편함을 느꼈던 부분들 즉 보관 기간에 관계 없이 장기간의 로그도 Grafana를 통해서 조회 가능하게 되었습니다 피크 타임때도 로그 유입이 증가하더라도 유연하고 빠르게 확장이 가능해졌습니다 주간에는 수집과 조회 컴포넌트에 각각 오토 스케일링을 적용하였고 야간에도 주간의 50 이하로 기준을 설정하여 운영하고 있습니다 로그 데이터는 약 65 압축되어서 저장소에 저장되고 있습니다 결과적으로 기존의 ELK Stack을 사용했던 과거에 비해 로그 수집량이 16배로 늘었지만 비용은 오히려 과거보다 약 23 절감하여 운영하고 있습니다 현재도 지속적으로 절감 포인트를 찾아 비용을 줄이고 있습니다 Grafana Loki를 통해 얻은 새로운 기회와 가능성 Loki는 Grafana의 LGTM 생태계를 이용하여 확장성이 있게 구성이 가능하며 OpenTelemetry Tempo와의 연동을 통해 Observability 시스템을 고도화할 수 있는 장점이 있습니다 Tempo는 OpenTelemetry에서 수집된 데이터를 처리하여 서비스의 가시성을 향상시키는 분산추적 시스템입니다 로그에 TraceID를 추가하면 Loki와 Tempo의 연동이 가능해 Grafana를 통해서 특정 로그 이벤트와 분산 추적 데이터를 연결하여 조회가능합니다 현재 팀에서도 OpenTelemetry Tempo 파일럿을 진행하며 도입을 검토하고 있습니다 고도화된 모니터링과 Observability 시스템을 통해 실시간 분석을 가능하게 하고 가시성이 향상된 시스템을 만들려고 합니다 그리고 효율적인 운영 자동화를 통해 리소스 및 운영 비용 절감을 지속적으로 진행하고 웹서버 어플리케이션 등의 로그 메트릭 분산추적을 연결하여 통합된 모니터링 시스템을 개발하고 이를 통해 복잡한 이슈를 손쉽게 추적하고 해결할 수 있도록 로드맵을 설정하여 모니터링 분야의 미래에 대한 가능성을 모색하고 있습니다 다음 편 예고 다음에는 로그 시스템 전환 과정과 사내 문화를 중심으로 소개할 예정입니다 사내 모든 엔지니어가 사용하는 시스템인 만큼 전환을 하기까지 쉽지 않은 과정과 시간들이 있었는데 이런 어려움들을 전사 엔지니어들과 어떻게 극복할 수 있는지에 대한 내용들도 정리해서 소개하겠습니다 마지막으로 이 글을 읽으시고 궁금한 사항이 생기셨거나 논의하고 싶은 부분이 있으신 분은 댓글에 내용을 남겨주세요 이연수 서비스 상태의 가시성을 향상시켜 시스템 성능을 개선하고 비용을 절감하기 위해 노력하고 있습니다,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/14505/
@codingapple,개발할 때 이런거 쓰지말라고 몇번을,https://img.youtube.com/vi/8TnUKFs-zH0/0.jpg,https://www.youtube.com/watch?v=8TnUKFs-zH0
"업스테이지-콴다-KT, 챗GPT 뛰어넘는 수학 특화 언어모델 개발",,,https://n.news.naver.com/mnews/article/001/0014430469?sid=105
"엔씨소프트, CBO 3인 체제로 조직 개편",,,https://n.news.naver.com/mnews/article/009/0005241377?sid=105
"카카오 준신위, '준법 시스템'·'신뢰·상생' 소위원회 신설",,,https://n.news.naver.com/mnews/article/215/0001142694?sid=105
@jocoding,최신 파이썬 기초 - 4강 프로그램의 구조를 쌓는다! 제어문 | 2023 점프 투 파이썬,https://img.youtube.com/vi/Sg33k9IVRr8/0.jpg,https://www.youtube.com/watch?v=Sg33k9IVRr8
"ChatGPT, LLaMA 그리고 이젠 Alpaca?",Alpaca A Strong Replicable InstructionFollowing Model Hello Alpaca 지난번에 포스팅된 Meta AI에서 개발한 ChatGPT의 대항마 LLaMA 에 이어서 최근 아주 핫한 모델이 있습니다 바로 Alpaca라는 모델인데요 오늘은 Stanford에서 공개한 오픈소스인 Alpaca에 대해서 간단히 소개해보려합니다 Alpaca는 지난번에 포스팅된 LLaMA라는 언어모델을 Stanford 박사과정 학생들이 사용자의 명령어에 언어모델이 잘 답변할 수 있도록 Instructionfollowing 데이터로 파인튜닝한 모델입니다 언어모델은 기본적으로 다음 단어를 예측하는 문제를 풀기 때문에 일반적인 사용자의 명령어에 자연스럽게 답변하기가 어려운데요 그럼에도 불구하고 ChatGPT 같은 모델이 답변을 잘하는 것은 사용자의 의도에 맞게 모델을 Instructionfollowing 데이터로 튜닝 Alignment 했기 때문이라고도 볼 수 있습니다 결국 사용자가 언어모델을 잘 활용하기 위해서는 Instruction tuning은 꼭 거쳐야하는 관문이라고 할 수 있습니다 LLaMA를 튜닝한 모델이니 아마 라마와 비슷한 생김새 가진 알파카를 이름으로 지은게 아닌가 싶네요 Alpaca는 논문이 따로 발표되진 않았지만 어떤 데이터로 어떻게 학습을 했는지 코드와 함께 공개가 되어있어서 지금도 LLaMA의 많은 변형과 비슷한 어플리케이션들이 나오고 있는데요 이렇게 핫한 Alpaca에 대해서 지금부터 한번 알아보도록 하겠습니다 Alpaca를 왜 만들었을까 Stanford 학생들은 ChatGPT Claude Bing Chat등 다양한 모델이 이미 훌륭한 성능을 보여주고 있지만 그럼에도 불구하고 아직은 부족한 점이 있다고 지적합니다 예를 들면 잘못된 정보를 생성하거나 사회적인 편견 및 불편한 말들을 생성하는 것이죠 이러한 문제를 해결하기 위해 학계와의 협업이 필요하지만 OpenAI의 textdavinci003과 같은 모델은 접근하기 힘든 closedsource model이기 때문에 연구에 어려움이 있다고 말합니다 마침 Meta에서 LLaMA를 공개했고 기존에 알려진 연구를 바탕으로 훨씬 저렴한 비용으로 모델을 학습할 수 있도록 데이터 및 모델 학습 방법을 재현 가능하도록 공개한 것으로 보입니다 결과적으로 Alpaca는 textdavinci003175B보다 훨씬 작은 7B 모델이지만 유사하게 동작한다고 합니다 Gradio 기반 데모 페이지도 공개했는데 접속은 가끔 안되는 것 같네요 Alpaca는 academic research에 한해서만 사용이 가능하고 상업적 사용은 금지하고 있는데요 이유는 LLaMA의 라이센스가 noncommercial 라이센스라는점 그리고 OpenAI의 tetdavinci003에서 얻어낸 데이터를 활용했다는 점등을 이유로 제시하고 있습니다 학습 방법 Alpaca는 기본적으로 7B 크기의 LLaMA를 Backbone으로 두고 Instruction tuning을 한 모델입니다 모델은 이미 공개되어있기 때문에 가장 중요한 것은 데이터인데요 기존에 Instructionfollowing 데이터를 생성하기 위한 많은 연구가 있었고 작년 12월인 최근에 공개된 selfInstruct라는 연구를 참고해서 데이터를 생성했습니다 selfInstruct 핵심은 LLMLarge Language Model로 데이터를 생성해서 그 데이터로 다시 LLM을 학습한다는 것인데요 한 마디로 튜닝을 위한 데이터도 모델이 생성하는 자가수급 시스템이라고 볼 수 있습니다 Alpaca에서는 selfInstruct의 방법론을 조금 단순화하되 모델은 더 좋은 모델GPT3davinci GPT35textdavinci003을 사용해서 데이터를 생성했습니다 데이터 생성 예시 Alpaca에서 공개한 방법대로 데이터가 생성되는지 OpenAI playground에서 테스트를 해보았습니다 아래 보이는 예시와 같이 데이터 생성이 잘 되는 것을 확인 할 수 있습니다 위와 같은 과정을 반복하면서 사람이 직접 만든 175개의 seed 데이터셋을 기반으로 데이터를 약 52000개까지 추가 생산을 하고 이 데이터를 학습셋으로 활용했습니다 데이터의 품질이 매우 뛰어나다고 할 수는 없겠지만 모델을 Alignment하기에는 어느정도 충분한 데이터를 생성한 것으로 보입니다 52000건의 데이터를 생성하는데는 500 정도의 비용이 들었다고 합니다 selfInstruct를 통해 생성한 데이터로 A10080GB 8대의 환경에서 Supervised FintuningSFT을 하면 3 epoch에 3시간정도 소요되며 일반적인 명령어에도 잘 답변할 수 있는 모델이 탄생하게 됩니다 이때 발생한 비용은 100 이하로 들었다고 합니다 Blind 테스트로 학습한 모델을 검토했을때 90Alpaca vs 89textdavinci003으로 Alpaca가 미세하게 앞서면서 textdavinci003과 유사한 성능을 보이는 것으로 나타났다고 합니다 마치며 Stable Diffusion과 같이 LLaMA도 공개가 되면서 다양한 후속 모델들이 빠르게 나오고 있습니다 벌써 8bit 양자화와 LoRALowRank Adaption등을 활용해서 대형언어모델을 개인 PC에서도 쉽게 사용할 수 있게 하는 프로젝트들이 개발되고 있고 이미 한국어로 튜닝된 7B KoAlpaca 모델 65B 모델도 등장했습니다 앞으로는 이전보다 더 저렴한 비용으로 더 괜찮은 성능의 모델들을 점점 더 많이 사용할 수 있게 될까요 개선할 부분은 많겠지만 오픈소스 생태계에서 Alpaca의 등장은 그러한 미래에 분명 도움이 될 것으로 보입니다 먼 미래가 아니라 지금이라도 조금은 더 똑똑하고 재미있는 자신만의 언어모델을 만들고 싶다면 지금 당장 Alpaca를 사용해보시는건 어떨까요 참고자료 Alpaca A Strong Replicable InstructionFollowing Model Alpaca github KoAlpaca,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164659&boardType=techBlog&searchData=&page=&subIndex=
GitHub Action을 이용하여 Release 자동화 후기,1 release 자동화가 필요하게된 이유 최근에 개발을 하면서 git의 branch 전략 dev staging mainmaster를 정의하면서 git branch에 대한 정의를 하였다 이때 dev staging mainmaster에서 개발이 완료됨에 따라 CICD중 CD에 해당하는 사용자외부로 노출되어야하는 서비스 제품등 release가 필요한 상황이 생겼다 release가 주기적으로 일어남에 따라 배포된 내용과 버그픽스 기능추가등등 문서를 정리해야 할일들이 발견 되었는데 git commit 내용을 취합해서 문서화하는것도 어느정도 귀찮은 상황이 발생하였음 2 이때 협업하는 사람들과의 아이디어 접근 github action을 통하여 tagrelease를 자동화하면서 문서화까지도 자동화 구축하자 3 사용하게된 도구 github의 action marketplace에 있는 tagrelease 자동화 tool mathieudutourgithubtagactionv61 ncipolloreleaseactionv1 31 github action code github action code 요약설명 github develop branch에 commit후 main으로 pullrequest하여 trigger를 동작시켜 deploy하도록 진행 name github action tag release on push branches main jobs setupbuilddeploy name Setup Build and Deploy runson ubuntulatest permissions packages write contents write idtoken write steps name Bump version and push tag id tagversion uses mathieudutourgithubtagactionv61 with githubtoken secretsGITHUBTOKEN name Create a GitHub release uses ncipolloreleaseactionv1 with tag stepstagversionoutputsnewtag name Release stepstagversionoutputsnewtag body stepstagversionoutputschangelog 32 commit 로그 설명1 github에 commit한 log를 기반으로 release version이 update되면서 commit로그를 바탕으로 배포로그까지 생성이된다 설명2 init modify는 버전으로 기록이 되지 않는다 fix feat 두가지의 github log가 기록으로 되어 github tag 버전에 정상적으로 배포로그하단의 34 참조가 남게된다 commit 메시지 종류 33 github action 실행 결과 main으로 push되었을때 trigger가 됨을 알 수 있다 34 배포 결과 위의 32 commit로그를 토대로 배포로그가 정상적으로 생성되었으며 semver 형태로 버전이 정상생성됨을 알 수 있다 4 회고 및 후기 실제로 commit 로그를 토대로 배포로그를 생성해줌으로 동시작업한 github시 불필요작업이 줄어들었음을 알 수 있다 github action workflow를 여러개 작성했을시 release는 project에서 1개로 관리됨에 따라서 버전이 여러개 올라가는 경우를 볼 수 있다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164861&boardType=techBlog&searchData=&page=&subIndex=
코드 리뷰 문화를 리뷰해 봐요 (코드 리뷰 프로세스 개선 ＆ PR Reminder Bot 개발 이야기),누구나 그럴싸한 리뷰 계획을 가지고 있다 PR이 쌓이기 전까지는 제가 1년 넘게 몸담고 있는 헤렌은 빠른 속도로 프로덕트를 개발해 왔습니다 점점 개발 조직이 Scaleup 되면서 저와 함께 공비서 서비스를 개발하는 웹프론트셀 구성원 규모도 제가 합류했던 시점과 비교했을 때 2배 늘어났어요 구성원이 늘어나면서 다양한 기능을 동시에 병렬적으로 개발하기도 규모가 큰 기능을 도메인 별로 작게 쪼개어 여러 구성원이 함께 개발하는 날들이 많아지게 되었습니다 헤렌 웹프론트셀은 main develop 브랜치를 포함해 feature branch 에 변경 사항을 적용할 때 동료 구성원의 코드 리뷰가 완료되어 Approve를 받아야 하는 규칙을 운영하고 있는데요 구성원이 적을 때에는 서로 대략 어떤 일감을 진행하고 있는 지 알고 있었기 때문에 코드 리뷰에 시간이 별로 소요되지 않았습니다 하지만 구성원이 많아지면서 PR이 쌓이는 속도가 점점 빨라지기 시작했습니다 동시에 여러 기능을 개발하다 보니 동료 구성원이 개발하고 있는 기능의 요구 사항이나 비즈니스 적인 성격을 PR을 리뷰하면서 처음 접하는 케이스가 많아졌어요 코드는 결국 현실 세계의 문제를 프로그래밍 언어로 풀어낸 결과물인데 현실 세계의 문제가 무엇인지 알지 못한 채 PR을 마주하게 되니 자연스럽게 리뷰어들이 리뷰하는 데 소요되는 시간과 에너지도 따라서 증가하게 되었습니다 매일 매일 쌓여가는 PR과 함께 몰려 오는 불안감 쌓여 가는 PR들과 PR Approve가 있어야 부모 Branch에 코드 반영이 가능한 Git Branch 전략은 웹프론트셀에게 있어 선택의 기로에 서도록 만들었습니다 그저 쭉 훑어 봤을 때 문제가 없어 보이면 동료를 믿고 Approve를 제출하는 그저 통과 의례 같은 코드 리뷰를 할 지 구성원 모두가 함께 비즈니스 요구 사항을 기반으로 코드 위에서 신나게 토론하고 피드백하며 프로덕트의 품질을 보장하는 코드 리뷰를 하는 대신 막대한 에너지와 시간을 투자할 지 말이죠 모두 개발과 프로덕트에 진심인 만큼 두 번째 방법을 따르면서 동시에 코드 리뷰가 전체적인 프로덕트 개발에 병목이 되지 않도록 아래의 사진처럼 코드 리뷰만 기다리다가 아까운 시간을 낭비하지 않을 수 있는 방안을 찾기 시작합니다 이번 코드 리뷰 개선 프로세스를 진행하면서 뱅크샐러드 기술 블로그와 우아한형제들 기술 블로그에서 많은 아이디어를 얻을 수 있었습니다 이 자리를 빌어 감사하다는 말씀 드립니다 Step 1 작게 밀도 있게 빠르게 코드 리뷰 프로세스의 변화 PR 리뷰에 점점 많은 시간과 에너지를 필요로 하게 되고 이로 인해 병목이 발생하게 된 원인은 앞서 언급한 사례를 포함해 다음과 같이 추려볼 수 있습니다 PR이 어떠한 요구사항을 해결하기 위해 어떠한 변경사항을 담고 있는 것인지 Reviewer가 이해하고 리뷰하는 과정에 시간과 에너지가 많이 소모된다 일부 PR의 경우 변경 사항의 사이즈가 커서 리뷰하는 데 있어 확인해야 하는 범위가 크고 부담으로 다가온다 Review를 언제까지 완료해야 한다는 데드라인이 없다 보니 기능 개발이 바쁠 경우 자연스럽게 후순위로 밀리게 된다 방대한 양의 변경된 코드는 세상을 아름답게 바라보도록 만들어 줍니다 이러한 3가지 요인을 해소하는 것을 코드 리뷰 프로세스 개선에 있어 핵심적인 가치로 판단했습니다 개선 과정에 있어 가장 중요한 포인트는 Reviewer에게 있어 코드 리뷰는 메인 업무가 아니라는 점입니다 앞으로 나올 변화들은 모두 이러한 포인트를 기반으로 Reviewer가 적은 노력으로도 밀도 있고 빠르게 코드 리뷰를 할 수 있도록 도와주는 것에 초점을 두고 있습니다 PR Template 개선 Assginee는 Reviewer가 PR을 리뷰하는 데 있어 적은 노력만으로도 빠르게 나와 같은 문제 상황에 빠져 들어 현실 세계의 문제를 바라보고 내가 제시한 해답에 대해 함께 고민해 줄 수 있도록 도와 주어야 합니다 따라서 단순히 내가 PR에서 코드적으로 어떠한 변화를 일으켰는지에 대해서만 적는 것은 Reviewer에게 최선의 피드백을 끌어내기 힘든 결과를 낳을 확률이 큽니다 내가 개발한 기능과 요구사항을 처음 접하더라도 내가 어떠한 문제를 해결하기 위해 이러한 코드를 작성하게 되었는지 Assignee의 시간과 리소스를 투자해 최대한 자세하게 설명해 주는 것이 좋아요 Reviewer가 파악하고 있는 정보는 Assignee가 코드 구현을 위해 문제를 바라보고 탐구하며 얻은 정보보다 훨씬 적기 때문입니다 따라서 구현된 코드에 대한 전체적인 문맥을 Reviewer가 적은 노력으로도 충분히 비슷한 수준으로 파악할 수 있도록 자세하게 설명해 주어야 합니다 개선된 PR Template은 기술적인 변경점 뿐만 아니라 어떤 요구사항을 해결하기 위해 이러한 변화가 발생하게 되었고 Assginee가 개발하면서 애매하다고 느꼈거나 Reviewer 들이 집중해서 리뷰 해 주면 좋을 것 같은 점들을 적을 수 있도록 구성했습니다 이를 통해 PR을 Open 한 후 Slack이나 대면 커뮤니케이션 등을 통해 추가로 설명하는 일을 최소화하고 웹프론트셀 구성원 모두가 일관된 양식으로 PR 본문을 작성할 수 있도록 유도했습니다 이 PR을 통해 해결하려는 문제가 무엇인가요 어떤 기능을 구현한건지 이슈 대응이라면 어떤 이슈인지 PR이 열리게 된 계기와 목적을 Reviewer 들이 쉽게 이해할 수 있도록 적어 주세요 일감 백로그 링크나 다이어그램 피그마를 첨부해도 좋아요 이 PR에서 핵심적으로 변경된 사항은 무엇일까요 문제를 해결하면서 주요하게 변경된 사항들을 적어 주세요 핵심 변경 사항 외에 추가적으로 변경된 부분이 있나요 없으면 없음 이라고 기재해 주세요 Reviewer 분들이 이런 부분을 신경써서 봐 주시면 좋겠어요 개발 과정에서 다른 분들의 의견은 어떠한지 궁금했거나 크로스 체크가 필요하다고 느껴진 코드가 있다면 남겨주세요 이 PR에서 테스트 혹은 검증이 필요한 부분이 있을까요 테스트가 필요한 항목이나 테스트 코드가 추가되었다면 함께 적어주세요 PR 진행 시 이러한 점들을 참고해 주세요 Reviewer 분들은 코드 리뷰 시 좋은 코드의 방향을 제시하되 코드 수정을 강제하지 말아 주세요 Reviewer 분들은 좋은 코드를 발견한 경우 칭찬과 격려를 아끼지 말아 주세요 Review는 특수한 케이스가 아니면 Reviewer로 지정된 시점 기준으로 3일 이내에 진행해 주세요 Comment 작성 시 Prefix로 P1 P2 P3 를 적어 주시면 Assignee가 보다 명확하게 Comment에 대해 대응할 수 있어요 P1 꼭 반영해 주세요 Request Changes 이슈가 발생하거나 취약점이 발견되는 케이스 등 P2 반영을 적극적으로 고려해 주시면 좋을 것 같아요 Comment P3 이런 방법도 있을 것 같아요 등의 사소한 의견입니다 Chore Assignee를 위한 CheckList ToDo Item 이렇게 개선된 PR Template을 활용해 웹프론트셀 구성원들은 함께 기능을 개발하는 이해 관계자가 아니어도 어떤 요구 사항을 해결하기 위해 어떠한 변화가 있었고 어떤 부분을 신경써서 봐야 할 지에 대해 적은 노력만 기울여도 한 눈에 파악할 수 있게 되었습니다 Pn 룰 정의 Pn 룰은 상단의 PR Template에 적혀 있는 것처럼 Reviewer가 피드백을 남길 때 Assignee에게 얼마나 해당 피드백에 대해 강조하고 싶은 지 표현하기 위한 규칙입니다 P1 꼭 반영해 주세요 Request Changes 이슈가 발생하거나 취약점이 발견되는 케이스 등 P2 반영을 적극적으로 고려해 주시면 좋을 것 같아요 Comment P3 이런 방법도 있을 것 같아요 등의 사소한 의견입니다 Chore 아래의 사진은 Pn 룰을 활용해 동작 상 오류는 없지만 통일성 있는 코드 스타일을 유지하고 재사용성을 증가시킬 수 있을 것 같은 아이디어에 대해 텍스트 기반으로 이루어지고 있는 커뮤니케이션입니다 PR Label 세팅 Dn 룰 PR이 어떤 상태인지 List 페이지부터 한 눈에 알아볼 수 있도록 Label도 다양하게 세팅했습니다 곧 등장할 PR 리뷰 중인 공그리 Bot의 도움으로 Dn Label은 매일 자동으로 세팅되었고 Status와 Type Label은 해당 PR의 Assignee가 프로젝트 구성원들에게 해당 PR에 대해 간략하게 설명할 수 있는 도구로 활용할 수 있도록 자율적으로 세팅하도록 구성했습니다 Dn 리뷰어들의 Review 마감 시점까지 얼마나 남았는지 표시 StatusStatus PR의 진행 상황을 표시 TypeType PR이 어떠한 변화를 가지고 있는 지 표시 Step 2 개선된 코드 리뷰 프로세스에 기술 한 스푼 투하 코드 리뷰 프로세스와 관련해 다 함께 리뷰를 집중적으로 진행하는 시간부터 효율적인 텍스트 기반 비동기 커뮤니케이션을 도와줄 다양한 수단을 마련했습니다 하지만 이런 일련의 규칙과 약속 만으로는 해결할 수 없는 문제가 있습니다 바로 반복적인 수작업 입니다 Dn 룰은 Assignee가 셀 구성원들에게 있어 리뷰를 특정 기한까지는 완료해 달라고 어필할 수 있는 수단입니다 하지만 매일 아침마다 Github Pull Request 페이지에 진입해 하루 줄어든 기한의 Dn Label로 변경하는 건 또 하나의 병목이 발생하는 지점입니다 코드 리뷰가 업무의 집중도에 있어 악영향을 미치게 되는 거죠 개발자라는 직업의 장점은 일상의 문제를 기술로서 해결할 수 있다는 겁니다 이러한 반복적인 수작업은 기술로서 해결하기 가장 좋은 타깃이고 Github과 같은 많은 개발자들이 애용하는 플랫폼은 분명 연동할 수 있는 API를 제공할 거라고 판단했습니다 예상대로 요구사항에 들어 맞는 라이브러리를 발견하게 되고 구현을 하려는 순간 아이디어 하나가 스쳐 지나갔어요 라이브러리를 활용해서 Label을 자동으로 변경하는 김에 매일 아침마다 Slack 코드 리뷰 채널에 리뷰가 필요한 PR들을 메시지로 리마인드 해 주면 리뷰가 보다 빨리 끝나지 않을까 평소에 Slack Bot을 개발해 보고 싶었던 저는 바로 Pull Request Reminder Bot 개발로 목표를 새롭게 설정하고 바로 실행으로 옮깁니다 우선 Python으로 Github REST API를 손쉽게 활용할 수 있는 PyGithub 라이브러리를 설치합니다 PyGithub 라이브러리를 활용해 우리는 원하는 Repository의 다양한 데이터를 얻을 수 있습니다 해당 Repo에 열려 있는 Open 상태의 PR 리스트 Repo에 세팅되어 있는 label 들이 해당하죠 httpsgithubcomPyGithubPyGithub PyGithub 라이브러리에서 제공하는 Github Class에 Access Token을 넘겨 instance를 생성합니다 해당 instance를 활용해 우리는 우리가 원하는 Repository에 접근해 우리가 필요로 하는 데이터들을 가져옵니다 label의 경우 다음과 같이 간단하게 Github Class가 제공하는 API를 활용하면 됩니다 g GithubPRIVATEACCESSTOKEN repo ggetrepoTARGETGITHUBREPO labels repogetlabels PR List 데이터를 얻기 위해선 일련의 과정이 필요합니다 우리가 Pull Request Reminder Bot을 통해 알고 싶은 정보는 Open 상태의 리뷰를 필요로 하는 PR들이기 때문입니다 getpulls API는 arguments 없이 쓸 경우 모든 상태의 PR List를 불러오기 때문에 필터링 조건을 명시해서 넘겨 줍니다 def gettotalpullrequests count 0 pullrequestslist 현재 열려있는 PR 목록들을 가져온다 for pull in repogetpulls stateopen sortupdated pullrequestslistappendpull count 1 return count pullrequestslist gettotalpullrequest 메소드의 첫 번째 return 값인 count의 value가 0보다 클 경우 두 번째 return 값인 pulls list를 for 문을 통해 순회하면서 1 Dn 라벨을 decreased 된 값으로 업데이트 하고 2 Slack의 PR 리뷰 중인 공그리에게 전송할 메시지를 구성하는 로직을 수행하게 됩니다 count pulls gettotalpullrequests if count 0 for pull in pulls prlink makeprlinkwithnopullnumber ddaylabel listfilterlambda x xnamestartswithD or xname OverDue pulllabels beforelabel ddaylabel0name if lenddaylabel 0 else afterlabel getdecreasedlabelbeforelabel setchangedlabelpull beforelabel afterlabel prmsgtoslack getprmsgtoslackprlink afterlabel pulltitle 이런 로직들을 거쳐 PR 리뷰 중인 공그리 Bot은 매주 평일 아침 9시마다 Target Repository에 Open 상태로 남아 있는 PR이 있는지 있다면 PR 리뷰 마감까지 얼마나 남았는지 코드 리뷰 채널을 통해 구성원들에게 알려주고 있습니다 리뷰를 기다리고 있는 PR이 존재할 때 리뷰를 기다리고 있는 PR이 남아있지 않을 때 PR 리뷰 중인 공그리는 가끔 이런 사소한 감동을 선사해 주었답니다 Review on 코드리뷰문화를리뷰해봐요 코드 리뷰 문화를 리뷰하고 개선한 지 어느덧 56개월이 지났습니다 산 더미같이 쌓인 PR에 Reviewer는 기겁하고 Assignee는 Slack으로 코드 리뷰를 리마인드 하며 하염없이 기다리고 PR이 Open 된 Github이 아닌 Slack 대면 커뮤니케이션 등 소통 채널이 일원화 되지 않던 과거의 모습들은 최근에 와서 많이 사라졌다고 생각됩니다 6개월에 가까운 시간 동안 서로 다른 프로젝트를 진행하면서 끊임없이 변화된 코드 리뷰 프로세스로 코드 리뷰를 진행하면서 익숙해 진 결과 이제는 서로 다른 기능을 개발하고 있더라도 빠르게 요구 사항에 대해 파악해 에너지를 적게 소모하고 PR이 작고 명확한 단위로 자주 열리게 되어 빠르게 해결되고 있는 편이에요 지금 이 순간에도 코드 리뷰와 관련해 개선해 보면 좋을 부분들에 대해 끊임없이 토론하고 변화에 대해 실험해 보고 있고 이러한 순간들이 모여 전체적으로 더 나은 개발 문화와 프로덕트를 만들어 나갈 것이라고 기대합니다D 참고 httpstechblogwoowahancom7152 httpsblogbanksaladcomtechbanksaladcodereviewculture 블로그 원본 출처 httpsblogteddykimcomimprovecodereview,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165255&boardType=techBlog&searchData=&page=&subIndex=
int는 몇 바이트 인가요?,int는 몇 바이트 인가요 과거 혹은 현재에도 면접에서 물어보던 질문 입니다 단순히 int 만이 아닌 다른 데이터 타입의 경우에도 32bit 프로세서를 쓰던 시대에서 64bit로 살아가는 지금은 어떻게 바뀌었는지 한번 알아 보겠습니다 ChatGPT에게 한번 물어봤습니다 ChatGPT는 위와 같이 대답했습니다 매우 그럴듯해 보이지만 틀린 대답일 수도 있으니 과연 맞는 답인지 검증해 보겠습니다 코드로 직접 확인 C Code include iostream include format int main stdcout stdformatSize of an int in C sizeofint Formatting library of C20 Run This Code C 코드를 돌린 결과는 다음과 같습니다 Size of an int in C 4 구현 결과 C의 int형 데이터 타입은 4 바이트로 확인 되었습니다 C를 해보신 분이라면 stdformat이 생소 하실 수도 있습니다 C20에서 추가된 Formatting 라이브러리인데 아직은 반쪽짜리이고 C23 표준에서 그나마 제 기능을 한다고 볼 수 있습니다 해당 부분에 대해선 다음 기회에 C23의 신규 기능 중 Print를 소개할 기회가 있다면 좀 더 자세히 다뤄 보겠습니다 그렇다면 다른 언어에선 어떻게 나오는지 확인해 보겠습니다 JAVA Code class Size public class Main public static void mainString args int sizeOfInt IntegerBYTES SystemoutprintlnThe size of int is sizeOfInt bytes Run This Code Java Code를 돌린 결과는 다음과 같습니다 The size of int is 4 bytes Java 역시 마찬가지로 4 바이트가 나왔습니다 그럼 int는 4 바이트 인걸까요 이번에는 Python으로 확인해 보겠습니다 Python Code import sys printThe size of int is sysgetsizeofint Run this code Python에서 int의 값을 구한 결과는 다음과 같습니다 The size of int is 24 이상하게도 Python은 C과 Java와는 완전히 다른 값인 24 바이트가 나왔습니다 왜 이런 값이 나온걸까요 Python의 int는 C과 Java같이 자료형이 아닌 객체Object이기 때문입니다 그래서 getsizeof로 int객체를 조사 했을 떄 자료형의 사이즈가 아닌 객체의 크기인 24 바이트가 나온 것입니다 다시 돌아와서 Python의 int의 최대 크기로 사이즈를 확인해 보겠습니다 Python2 에서 int의 최대 크기를 확인하는 방법인 sysmaxint가 Python3에서 sysmaxsize로 변경되었습니다 import sys printThe max size of int is sysmaxsize Run this code sysmaxsize의 값은 다음과 같습니다 The max size of int is 9223372036854775807 9223372036854775807 2의 64 1승 즉 8바이트가 나왔습니다 종합해보면 C과 Java는 각 4바이트 Python의 경우는 8바이트가 나왔습니다 일단 언어 마다 다를 수 있다는 생각을 하게 되었습니다 그럼 언어마다 다른게 맞을까요 다음으로는 64bit 데이터 모델에 대해 알아보겠습니다 64bit Data Models 이제 본격적으로 32bit 와 64bit 컴퓨터의 데이터 모델이 어떻게 다른지 확인해보겠습니다 연산처리장치의 레지스터가 한번에 처리할 수 있는 값이 32bit인 경우 32bit 컴퓨터 64bit인 경우 64bit 컴퓨터라고 부르게 됩니다 즉 한번에 처리할 수 있는 주소의 양 역시 32bit 컴퓨터의 경우엔 4byte 64bit의 경우는 8byte 입니다 기본적으로 주소를 한번에 처리할 수 있는 양 부터 차이가 발생한다고 볼 수 있습니다 주소 뿐 아니라 다른 데이터 타입의 경우도 플랫폼 별로 차이가 나는데 해당 차이를 표현하는 방법으로 LP64 LLP64 같은 방식을 사용하고 있습니다 LP64는 Long Pointer 64를 의미합니다 즉 Long 타입과 Pointer 타입은 64bit8 Byte로 처리한다는 뜻 입니다 64 Bit Data Models Data model shortinteger int longinteger long long pointerssizet Sample operating systems ILP32 16 32 32 64 32 x32 and arm64ilp32 ABIs on Linux systems MIPS N32 ABI LLP64 16 32 32 64 64 Microsoft Windows x8664 and IA64 using Visual C and MinGW LP64 16 32 64 64 64 Most Unix and Unixlike systems eg Solaris Linux BSD macOS Windows when using Cygwin zOS ILP64 16 64 64 64 64 HAL Computer Systems port of Solaris to the SPARC64 SILP64 64 64 64 64 64 Classic UNICOS versus UNICOSmp etc 출처 httpsenwikipediaorgwiki64bitcomputing64bitdatamodels 위의 표를 보면 64bit 컴퓨터를 지원하는 플랫폼별로 어떤 데이터 모델을 사용하고 있는지 알 수 있습니다 가장 많은 플랫폼에서 사용하는 모델은 LP64 입니다 하지만 가장 많이 사용하는 플랫폼 중 하나인 Windows의 경우 LLP64 모델을 사용하고 있습니다 그래서 특히 크로스 플랫폼을 지원하는 프로그램을 개발 할 때는 언제나 타입에 유의해야 합니다 예를들어 Linux 에서 개발하지만 Windows에서도 동작해야 하는 프로그램을 짤 때 Long을 사용해야 할 경우 주의 해야 합니다 LP64 모델인 Linux에서는 8byte로 동작하지만 LLP64 모델인 Windows에서는 4byte로 동작하기 때문입니다 같은 이유로 ILP64 모델을 지원하는 플랫폼의 경우엔 제목과 같이 int는 몇 바이트 인가요 의 대답은 8byte가 됩니다 그렇다면 Python의 경우는 왜 8byte가 나온걸까요 값이 다른 이유를 알아보도록 하겠습니다 값이 다른 이유 왜 Python은 8byte였을까요 사실 Python의 경우 int의 크기는 정해져 있지 않습니다 기본적으로 정해진 값4 또는 8byte를 넘어가더라도 자동으로 데이터 값을 증가시켜주기 때문입니다 sysmaxsize가 8byte가 나온 이유는 64bit컴퓨터였기 때문이고 즉 컴퓨터의 연산 처리량에 맞추어 값이 변한다고 볼 수 있습니다 혹시 미래에 128bit 컴퓨터가 나온다면 Python의 sysmaxsize는 16byte로 나올 것입니다 플랫폼에 독립적이냐 의존적이냐의 차이도 발생합니다 기본적으로 C C의 경우엔 플랫폼에 의존적인 언어 입니다 Unix나 Linux Windows의 경우도 C로 짜여진 커널을 사용하고 있기 때문에 컴파일러의 경우 두 플랫폼 모두 지원하지만 내부 커널의 데이터 처리 방식도 다르고 그로 인해 데이터 모델의 차이가 발생합니다 그 부분이 64bit로 변화하면서 Unix Linux는 LP64모델을 Windows는 LLP64모델을 지원하게 되었습니다 컴파일러는 플랫폼에서 정의한 데이터 모델대로 구현되어 있고 따라서 데이터 모델 역시 플랫폼에 의존적일 수 밖에 없습니다 물론 데이터 모델 뿐만 아니라 컴파일러를 통해 만들어진 바이너리를 동작 시키기 위한 부분 역시 플랫폼 종속적입니다 반면 플랫폼에 독립적인 언어들 인터프리터 언어인 Python이나 각 플랫폼 위에서 독립적인 가상머신JVM을 제공하여 어떤 플랫폼에서도 동일한 코드로 동일한 바이너리를 만들어낼 수 있는 Java의 경우엔 플랫폼과 상관없이 늘 동일한 값을 보장해 줍니다 위의 예제에서 Java의 경우 int가 4byte가 나왔고 그렇다면 64bit 데이터 모델이 ILP64인 플랫폼에서는 8byte가 나올 것 같지만 Java의 JVM에서 int는 4byte로 정의해 놓았기 때문에 ILP64 모델에서도 Java는 여전히 4byte로 나오게 될 것 입니다 마찬가지로 Python 역시 동일한 값을 출력할 것입니다 하지만 C나 C의 경우엔 ILP64 모델의 플랫폼에선 8byte로 출력 될 것입니다 결론 int는 몇 바이트 인가요의 결론을 내보겠습니다 처음은 int의 사이즈를 각 언어에 맞는 코드를 작성하여 값을 알아봤습니다 그 결과 C Java는 4byte 그리고 Python은 8byte라는 결과를 얻을 수 있었습니다 다음으로는 데이터 모델을 살펴보았습니다 플랫폼에서 어떤 모델을 사용하느냐에 따라 int의 경우도 4byte 혹은 8byte의 값을 가질 수 있다는 것을 알 수 있었습니다 마지막으로 차이가 발생한 이유를 알아봤습니다 64bit컴퓨터를 기준으로 봤을 때 해당 프로그래밍 언어가 플랫폼에 독립적인지 의존적인지에 따라 다를 수 있으며 독립적인 언어의 경우 언어 별로 같은 타입int이라도 값의 차이는 다를 수 있으나Java은 4byte Python 기본적으론 8byte 무제한 플랫폼에 따라 변경되지는 않는 반면 플랫폼에 의존적인 언어의 경우 플랫폼이 지원하는 데이터 모델이 다르면 같은 언어의 같은 타입이라도 달라질 수 있습니다 그래서 C나 C의 경우에도 플랫폼에 민감한 프로그램을 작성하는 경우에는 반드시 값이 명시된 자료형을 사용해야 합니다 int32t int64t 등등 결론적으로 처음 질문인 int는 몇 바이트 인가요가 면접에서 질문으로 나온다면 개인적으로 생각하는 답변은 int는 64bit 컴퓨터 기준 언어마다 다르지만 특히 플랫폼에 독립적인 언어의 경우엔 반드시 데이터를 몇 byte에 넣어서 사용할 것인지 명시적으로 선언하여 사용해야 한다라고 얘기하고 싶습니다 물론 질문한 사람의 의도에 따라 단순히 4byte의 답을 원했을 수도 있습니다 그러므로 이런 다양한 답이 나올 수 있는 질문을 하는 면접관이 있는 회사라면 심각하게 입사를 재고해 보셔도 좋을 것 같습니다 3줄 요약 언어마다 다를 수 있다 플랫폼에 의존적인 언어의 경우엔 같은 언어일 지라도 플랫폼에 따라 다른 값이 나올 수 있다 이런 질문을 하는 면접관이 있는 회사는 피하자 물론 농담입니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164788&boardType=techBlog&searchData=&page=&subIndex=
"티빙, 2024∼2026년 프로야구 유무선 중계권 우선 협상자 선정",,,https://n.news.naver.com/mnews/article/014/0005125017?sid=105
@codingapple,애플비전프로 몰래 먼저 써보는 법,https://img.youtube.com/vi/uxym-VxK8NA/0.jpg,https://www.youtube.com/watch?v=uxym-VxK8NA
웹 프론트엔드 개발자가 웹 성능 최적화를 해야 하는 이유,시작하는 글 웹 성능 최적화개선은 웹페이지를 효율적으로 동작하게 하는 작업입니다 웹 성능 최적화를 위해 백엔드 인프라 최적화 등도 중요한 요소지만 오늘은 웹 프론트엔드에 집중한 웹 성능 최적화 방법을 이야기 해보겠습니다 물론 이 글에서 말하는 웹 성능 최적화 작업은 상품 관리자 프로젝트 매니저 디자이너 기획자도 알아두면 좋습니다 상품 관리자 프로덕트 매니저에게 웹 페이지가 얼마나 빨리 로딩되는지가 그들이 관리하는 서비스 사용자 경험UX User Experience에 영향을 줄 것이며 자연스레 매출 및 수익에 영향을 줄 수 있기 때문에 웹 성능 최적화를 대략적으로 이해하는 것은 중요합니다 그리고 기획자 디자이너 역시 페이지 주요 영역을 디자인하는 복잡도에 따라 나쁜 사용자 경험을 줄 수도 있고 반대로 좋은 사용자 경험을 줄 수도 있습니다 그렇기 때문에 기획자 디자이너에게도 웹 성능 최적화에 대한 이해도는 중요한 측면이 있습니다 결국 웹 성능 최적화 작업은 웹 서비스에 비즈니스적인 긍정적인 기여를 할 수 있습니다 즉 웹 프론트엔드 개발자의 웹 성능 최적화 작업으로서 사용자에게 좋은 사용자 경험을 제공할 수 있고 이것은 비즈니스의 성공과도 직결될 수 있습니다 이 포스팅에서 웹 성능 최적화를 왜 해야 하는지 웹 성능 최적화는 어떤 지표로 나눠져 있고 어떻게 개선할 것인지 다뤄보도록 하겠습니다 웹 성능 최적화를 해야 하는 이유 1 비즈니스 관점 쉽게 말하면 웹 성능 최적화를 한다는 것은 사용자에게 제공하려는 웹 사이트를 빠르게 보여주는 것입니다 이는 자연스레 비즈니스에 영향을 줍니다 일반적으로 생각해 볼 때 웹 사이트가 사용자에게 빨리 로딩되면 좋은 사용자 경험UX User Experince을 제공할 수 있습니다 반대로 사용자가 웹 사이트에 들어갔을 때 하얀 화면을 노출시키거나 웹 사이트 로딩 및 렌더링에 지연이 발생하면 사용자는 자연스레 이탈하게 되고 이 때 해당 웹 서비스는 방문 사용자에 대한 비즈니스 기회를 잃어버리게 됩니다 아래는 2017년 Google의 모바일 부문 글로벌 제품 리드인 Daniel An이 말한 Think with Google의 기사에서 발췌한 내용으로 웹 페이지 로딩 시간과 사용자 이탈률에 대해 이야기 했습니다 아래 자료에 따르면 페이지 로딩 시간이 1초에서 3초로 증가하면 페이지 이탈률이 32로 증가한다고 하고 1초에서 10초가 되면 무려 페이지 이탈률이 123나 된다고 합니다 Google SOASTA Research 2017 전세계 2위 이동 통신사인 Vodafone은 2021년 웹 성능 개선 건에 대한 AB 테스트를 통해 웹 성능 지표 중 하나인 LCPLargest Contentful Paint 웹 페이지 내 주요 콘텐츠가 렌더링되어서 사용자에게 표출되는 시간를 31 개선함에 따라 판매가 8 증가함을 확인했습니다 그리고 성공적 잠재고객으로 전환한 사용자 비율이 15가 증가하였고 결제 장바구니 방문률도 11나 증가하기도 했습니다 인도네시아 최대 전자상거래 회사인 Tokopedia는 LCP를 약 55 감소 시킴에 따라 평균 세션 시간Average Session Duration이 23 증가했습니다 이 밖에 수 많은 기업들이 웹 성능 개선 작업을 통해 비즈니스 관점으로 많은 기회를 얻었습니다 2 검색 엔진 최적화 관점 SEO Search Engine Optimization 2021년 5월부터 Google에서는 웹 성능 지표웹 성능 최적화가 어느정도 되었는가는 검색엔진 순위에 영향을 미친다고 발표했습니다 아래 Google이 소개한 표에서 보면 Core Web Vitas에 있는 LCP FIDFirst Input Delay 최초 입력 지연 시간 CLSCumulative Layout Shift 페이지 로드시 축척된 화면의 움직임를 말합니다 3 사용자 경험 UX User Experience 관점 사용자 경험 측면에서도 웹 성능 개선은 중요합니다 무엇보다 웹 페이지 로딩 속도가 빨라야 하는 것은 당연합니다 웹 사이트가 초기에 빨리 로딩 된다고 하더라도 화면이 로딩되는 순서에 따라서 사용자에게 안 좋은 경험을 제공할 수 있습니다 이것은 웹 성능 수치에 영향을 끼치는데 Google의 Core Web Vital 항목 중 CLS라는 정량적인 수치로 측정이 가능합니다 CLS에 관해서는 다음 단락웹 성능 최적화를 위해 개선할 포인트에 CLS에 관한 설명과 예시가 있으니 참고하시면 좋을 것 같습니다 로딩 속도 못지 않게 중요한 것은 사용자가 보이는 뷰view 즉 사용자 시선에 따라 콘텐츠가 어떻게 로드되어 보여지는지는 것이 사용자 경험에 큰 영향을 미칩니다 일반적으로 사용자의 시선은 위에서 아래로 이동하고 한국과 같은 LTR 국가Left to Right 텍스트 작성 순서가 왼쪽에서 오른쪽으로 되어 있는 국가에서는 시선이 왼쪽에서 오른쪽으로 이동 하는게 자연스럽습니다 반면 중동이 대다수인 RTL 국가 Right to Left 텍스트 작성 순서가 오른쪽에서 왼쪽으로 되어 있는 국가는 시선이 오른쪽에서 왼쪽으로 이동되는 것이 자연스럽습니다 한국을 포함한 LTR 국가 사용자들은 위에서 아래 왼쪽에서 오른쪽이 자연스럽습니다 이처럼 웹 개발을 하면서 위부터 왼쪽부터 콘텐츠가 로딩 되도록 신경을 쓴다면 자연스러운 사용자 경험을 제공할 수 있습니다 참고로 콘텐츠 순서가 웹 성능 점수에 영향을 미치지 않지만 사용성에 영향을 끼치니 웹 성능 작업을 하면서 콘텐츠 로딩 되는 순서를 되짚어 보고 개선 했으면 합니다 예를 들어 웹 사이트 표출 시 사이트 윗부분인 Heder 부분이 늦게 뜨고 아래서부터 페이지가 로딩 된다면 어색한 사용자 경험을 제공할 것입니다 웹 성능 최적화를 위해 개선할 포인트 웹 프론트엔드에서 웹 성능 최적화를 위해서는 웹 사이트의 로딩 성능Loading Performance 렌더링 성능Rendering Performance 등을 개선하는 것에 집중해야 합니다 웹 사이트 로딩 성능은 서버에서 웹 페이지에 필요한 HTML CSS Javascript 미디어 소스Image Video 등의 리소스를 다운로드할 때의 성능을 말합니다 그리고 렌더링 성능은 페이지 화면에 주요 리소스가 페이지에 그려질 때의 성능을 말합니다 Google은 필수적인 웹 성능 지표인 Core Web Vital을 만들었습니다 널리 사용되는 성능 측정 도구 중 하나인 Google의 Lighthouse를 이용하면 Core Web Vital에서 이야기하는 지표들을 확인하고 개선할 수 있습니다 LCP CLS는 Lighthouse에서 정량적인 수치로 확인할 수 있습니다 그러나 Lighthouse에 FIDFirst Input Delay는 없지만 Lighthouse에서 측정 가능한 수치인 TBTTotal Blocking Time과 연관이 높습니다 아래는 Google에서 발표한 Lighthouse 10을 이용하여 웹 성능 측정을 할 때 항목과 그에 따른 비중을 나타낸 표입니다 Audit Weight FCPFirst Contentful Paint 10 Speed Index 10 LCPLargest Contentful Paint 25 TBTTotal Blocking Time 30 CLSCumulative Layout Shift 25 Core Web Vital에서 다루는 지표가 Lighthouse의 측정 지표의 80를 차지한다는 것을 알 수 있습니다 1 LCP Largest Contentful Paint LCP는 페이지에서 핵심 요소의 콘텐츠 로딩 속도를 측정합니다 일반적으로 페이지에서 가장 큰 영역으로 해당 영역에 있는 이미지 영상 텍스트 부분 등 요소의 렌더링 성능을 기반으로 측정 됩니다 웹 페이지에서 가장 의미 있고 사용자가 접하는 가장 중요한 첫 요소이기 때문에 해당 영역이 늦게 나타나면 페이지 이탈률이 자연스레 늘어날 수 밖에 없고 그렇기 때문에 Core Web Vital에서도 중요한 지표로 다루고 있습니다 2 FID First Input Delay FID는 콘텐츠의 상호작용 속도를 측정합니다 사용자가 페이지와 처음 상호 작용링크 클릭이나 버튼 클릭할 때 이에 대한 응답으로 브라우저가 이벤트를 처리하기 까지의 시간을 측정합니다 Lighthouse에서는 TBTTotal Blocking Time라는 지표와는 다르지만 Lighthouse에서 성능을 측정할 시 TBT를 개선하는게 FID에도 개선 효과를 줍니다 3 CLS Cumulative Layout Shift CLS는 페이지 방문자의 시각적인 안정성을 측정합니다 쉽게 말하면 CLS는 사용자에게 웹 페이지가 표시되고 최종적으로 변화되는 정도를 측정한 수치입니다 본 지표는 사용자 경험UX에 중요한 영향을 끼칩니다 페이지 렌더링 초기에 화면 전환이 심하면 사용자가 원하는 동작을 할 수 없습니다 아래의 예시처럼 사용자는 버튼을 클릭하고 싶지만 지속적인 화면의 변화로 사용자가 원하는 동작을 못할 수도 있습니다 구매하고 싶은 상품 구매를 못하던지 반대로 취소하고 싶은 상품을 구매한다던지 중요한 UX에 사용자에게 악영향을 줄 수 있습니다 마치는 글 이번 글을 통해 웹 프론트엔드 개발자가 웹 성능 최적화를 해야 하는 이유 어떤 측정 지표를 고려해서 웹 성능 최적화를 하면 좋은지 간단하게 알아보았습니다 다시 한번 강조하자면 웹 프론트엔드 성능 최적화은 웹 서비스를 매개로 한 비즈니스의 성패에 직결될 수 있으므로 제품 관리자 및 제품 이해 관계자도 중요시 생각해야 할 작업입니다 다음은 웹 성능 최적화에 대한 측정 도구 및 웹 성능 최적화를 위한 직접적인 방법에 대해 알아보도록 하겠습니다 긴 글 읽어주셔서 감사합니다 Reference httpswebdevvitalsbusinessimpact httpsdeveloperchromecomdocslighthouseperformanceperformancescoring httpsdevelopersgooglecomsearchblog202011timingforpageexperience httpsrequestmetricscomwebperformancecumulativelayoutshift,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164863&boardType=techBlog&searchData=&page=&subIndex=
@ITSUB,방수가 되는 태블릿의 등장이라.. 출시 임박한 갤럭시탭 S9 루머 총정리!,https://img.youtube.com/vi/iPjYwcVqzmc/0.jpg,https://www.youtube.com/watch?v=iPjYwcVqzmc
@coohde,Visual Studio Code에서 Git으로 협업하기 - 3. 협업하기,https://img.youtube.com/vi/srN5UchM26A/0.jpg,https://www.youtube.com/watch?v=srN5UchM26A
이상 탐지 3부-머신 러닝으로 이상 탐지하기,들어가며 안녕하세요 클라우드 AI팀 박현목입니다 지난 글에서는 통계적 기법을 활용한 이상 탐지에 대해 소개해 드렸습니다 1부와 2부는 아래 경로에서 다시 읽어 보실 수 있습니다 1부정상과 비정상 그리고 이상 탐지 2부통계적 기법으로 이상 탐지하기 3부머신 러닝으로 이상 탐지하기 4부딥 러닝으로 이상 탐지하기 이번 글은 위와 같이 4부로 나눈 글 중 3부머신 러닝으로 이상 탐지하기에 대한 이야기입니다 1 Machine learning 사실상 현재 개발되고 있는 최신 이상 탐지 모델들은 대부분 머신 러닝에 기반을 두고 있습니다 뉴럴 네트워크 기반의 딥 러닝 기법들은 4부에서 더 자세하게 설명해 드릴 예정이기에 지금은 고전 머신 러닝 위주로 소개 드리겠습니다 대표적인 머신 러닝 기반 이상 탐지에는 분류classification nearestneighbor clustering 등이 있습니다 그리고 추가로 reconstructionbased라는 기법이 있는데 현재 가장 대중적으로 쓰이는 방법 중 하나입니다 11 Classification 분류classification를 간략하게 한 문장으로 요약하면 경계를 긋는 일이라고 할 수 있습니다 이상 탐지는 정상이상 데이터를 구분하는 경계를 찾는 이진 분류 문제라고 볼 수 있습니다 다만 이상 탐지에서 분류를 적용하기에는 큰 제약 사항이 하나 있습니다 지난 글에서도 말했지만 이상 탐지는 데이터의 불균형이 매우 심한 분야입니다 그런데 분류는 데이터가 균등할 때 잘 동작하는 알고리즘입니다 그런 이유로 데이터 불균형을 해결하기 위해 하나의 클래스만 학습시키는 방법들이 연구되었지만 그럼에도 분류는 이상 탐지에서 활발하게 쓰이지는 않습니다 대표적인 분류 이상 탐지 모델로는 support vector machine이하 SVM이 있습니다 SVM의 과정은 간략하게 설명한다면 데이터 공간에 클래스별로 라벨링된 데이터가 있습니다 여기서는 간단하게 2차원 공간과 11 라벨링만 고려하겠습니다 우리는 클래스들을 가장 잘 나눌 수 있는 경계면hyperplane을 찾는 것이 목적입니다 이때 고려해야 할 사항은 얼마나 클래스를 잘 나누었는가 그리고 경계면과 데이터들이 충분히 떨어져 있는가margin입니다 아래 오른쪽의 그림을 보시면 SVM에 대해 쉽게 이해하실 수 있습니다 초록색 실선으로 표현된 hyperplane이 파란색 원 빨간색 사각형을 나눌 수 있는 경계면이 되어 주고 있으며 이때 색칠된 데이터들과 hyperplane 사이의 거리를 margin이라고 부릅니다 그리고 색칠된 데이터들을 support vector라고 부르며 hyperplane 형성에 가장 큰 영향을 주는 데이터들입니다 그럼 이상 탐지는 어떻게 SVM으로 novelty detection을 하기 위해 나온 것이 one class support vector machine이하 OCSVM입니다 OCSVM의 기본적인 동작 원리는 SVM과 동일합니다 하지만 이름에서 알 수 있듯이 OCSVM은 하나의 클래스만을 학습합니다 대신에 hyperplane을 형성하는 기준에 원점이 추가되었습니다 학습 데이터와 원점을 가장 잘 나눠 주는 hyperplane을 찾는 것이 OCSVM의 목적입니다 비슷한 모델로 support vector data description이하 SVDD이라고 하는 분류 모델도 있습니다 SVDD는 OCSVM과 성격이 매우 유사한데 이번에는 hyperplane이 아닌 hypersphere를 찾는 게 목적입니다 아래의 그림을 보시면 OCSVM SVDD의 동작 과정을 쉽게 알 수 있습니다 주의 사항으로 이상 탐지에서는 두 방법 모두 정상 데이터만을 학습해야 합니다 둘 중에 무엇이 더 좋다고 단언하기는 어렵고 데이터를 바라보는 관점이 다르다 정도로 이해하시면 될 것 같습니다 위의 3가지 기법들은 모두 kernel trick이라는 데이터 확장 기법과 함께 사용하는 일이 많습니다 Kernel trick은 데이터의 차원을 확장하여 비선형 분류 문제를 선형 문제로 바꿔 주는 기법입니다 아래의 그림처럼 2차원 공간에서는 경계면을 찾기 어려운 문제를 kernel trick을 통해 3차원으로 데이터를 확장하면 쉽게 경계면을 찾을 수 있습니다 단점 분류 이상 탐지 모델들은 준수한 성능을 보여 주었고 특히 SVDD와 딥 러닝을 결합한 DeepSVDD는 2018년에 발표되어 현재까지도 많은 연구의 베이스라인으로 활용되고 있습니다 하지만 분류 모델은 아래와 같은 단점 때문에 실제 활용이 어렵다는 의견이 많습니다 라벨링된 데이터를 필요로 한다 비선형 분류 문제를 다루지 못한다 OCSVM SVDD는 라벨링 데이터를 요구하지 않지만 좋은 이상탐지 모델을 만들기 위해서는 모두 정상 데이터만 학습해야 한다는 추가 제약 사항이 있습니다 두 번째는 비선형 분류 문제를 다루기 어렵다는 것입니다 Kernel trick을 쓴다면 선형 문제로 변환할 수는 있지만 언제나 만능은 아닙니다 12 NearestNeighbor NearestNeighbor 방법은 하나의 가정에서 출발합니다 정상적인 데이터들이라면 데이터 차원상에서 서로 밀집되어 있을 것이라는 가정입니다 쉽게 말한다면 끼리끼리 논다 정도로 표현할 수 있을 것 같습니다 데이터의 분포에 대한 어떠한 가정도 하지 않으며 순수하게 데이터 자체만을 보기 때문에 모델의 효율성에 따라 이상 탐지 성능이 크게 달라질 수 있습니다 이웃 데이터들의 거리 정보distance와 밀도 정보density를 활용하여 데이터마다 점수를 부여하고 임곗값을 넘으면 이상 데이터로 간주하는 방식입니다 Distancebased 이웃한 데이터들과의 거리정보를 고려하여 점수 부여 ex KNearest NeighborKNN Densitybased 이웃한 데이터들의 밀도를 고려하여 점수 부여 ex Local Outlier Factor 거리 기반 이상 탐지의 대표적인 예시인 KNN은 인접한 K개의 데이터까지의 거리의 평균 합 최댓값 최솟값 등을 고려해서 데이터에 점수를 부여합니다 이 점수가 임곗값을 넘으면 이상 데이터로 간주합니다 굉장히 직관적이면서도 간단한 원리로 동작하는 방법입니다 하지만 거리 기반 이상 탐지에는 한 가지 단점이 존재합니다 아래의 오른쪽 그림을 보시면 쉽게 이해하실 수 있습니다 빨간색 원으로 표시된 O1 O2 데이터는 둘 다 outlier입니다 사람의 눈으로 관찰한다면 두 데이터 모두 outlier라는 걸 쉽게 알 수 있지만 거리 기반 이상 탐지를 한다면 모델은 O2 데이터를 정상으로 판단할 가능성이 높습니다 왜냐하면 C1으로 표현된 군집으로 인해서 O2와 C2 군집까지의 거리가 충분히 가깝다고 판단하기 때문입니다 그래서 제안된 방법이 인접한 데이터들의 밀도까지 고려하는 Local Outlier FactorLOF 입니다 LOF를 자세히 다루면 글을 따로 하나 더 작성해야 할 정도로 내용이 많기 때문에 밀도를 고려한다는 것 정도만 알고 계시면 좋을 것 같습니다 13 Clustering Clustering 방법은 NearestNeighbor와 유사한 느낌이 있지만 최종 목표가 군집을 찾는 것이라는 차이점이 있습니다 그리고 이상 데이터라면 어느 군집에도 속하지 않을 것이라고 가정합니다 대표적인 예시로 KMeans가 있습니다 NearestNeighbor와 동일하게 라벨링된 데이터를 필요로 하지 않는다는 장점이 있지만 군집의 개수를 미리 지정해야 한다는 점 알고리즘의 효율성에 따라 성능이 크게 차이가 난다는 점 등의 단점이 있습니다 Clustering 방법 자체가 이상 탐지에 적절하지 않다는 의견도 있습니다 왜냐하면 대부분의 군집화 알고리즘들은 모든 데이터가 어느 군집이든 무조건 포함되도록 하는 경우가 많은데 이것이 이상 탐지의 취지와 맞지 않다는 것입니다 14 Reconstructionbased Reconstructionbased 방법은 현재 이상 탐지의 대세 중 하나입니다 Reconstructionbased 방법은 주어진 데이터에 숨어 있는 유효한 특성feature을 추출한 후에 다시 데이터를 복원했을 때 정상 데이터라면 본래대로 복원되지만 비정상 데이터라면 제대로 복원되지 않을 것이라고 가정합니다 대표적으로 방법으로 principal component analysis이하 PCA가 있습니다 PCA는 본래 고차원 데이터를 다루기 위한 차원 축소법으로 주어진 데이터를 분산이 가장 커지는 축으로 환원하는 기법입니다 이때 데이터를 축소하는 변환 정보를 기억해 둔다면 본래의 데이터를 어느 정도 다시 복원할 수 있습니다 이 개념을 응용해서 비정상 데이터라면 제대로 복원되지 않을 것이라는 게 이론적 배경입니다 주의 사항으로는 데이터를 축소할 때 정상 데이터만을 사용해야 한다는 점입니다 하지만 PCA는 선형 변환이라는 한계점이 존재했고 다차원 데이터 간의 연관성을 다루기에는 부족함이 있었습니다 그래서 사용된 것이 바로 뉴럴 네트워크 기반의 오토인코더입니다 오토인코더도 데이터를 압축한 후에 복원한다는 점에서 PCA와 동일하지만 비선형 변환이 가능하다는 차별점이 있습니다 오토인코더는 지금까지도 최신 모델에 많이 적용되고 있을 정도로 성능이 입증된 모델이기 때문에 기억해 두시는 걸 추천합니다 2 고전 머신 러닝 vs 딥 러닝 21 왜 딥 러닝을 써야 할까 위에서 소개한 모델들은 일부를 제외하고 대부분 고전 머신 러닝 모델들입니다 고전 모델들도 좋은 성능을 보여 주었지만 현재는 딥 러닝 기반 모델들이 많이 사용되고 있습니다 다양한 이유가 있지만 근본적인 이유는 데이터의 양이 점차 많아지고 있기 때문입니다 데이터의 양도 문제지만 데이터의 차원이 높아지고 차원 간의 연관성도 높아지면서 고전 머신 러닝 모델들은 충분한 성능을 보여 주지 못하는 상황입니다 하지만 고전적인 접근법이 전혀 사용되지 않는 것은 아닙니다 고전 머신 러닝의 개념과 딥 러닝을 결합한 모델들이 많이 연구되고 있으며 그 중에서는 뛰어난 성능을 보여주는 모델도 많습니다 22 시계열 이상 탐지에서의 딥 러닝 시계열 이상 탐지도 위에서 소개해 드린 방법들을 적용할 수 있습니다 하지만 아쉽게도 성능이 그다지 좋지 않습니다 시계열 데이터에 이상 탐지를 적용하려면 추가적인 고려 사항이 있습니다 데이터 차원 간의 연관성 데이터 시간 축에서의 연관성 첫 번째 고려 사항은 사실 모든 이상 탐지 데이터에서 고려해야 할 사항입니다 하지만 두 번째 고려 사항은 확실히 시계열 데이터에서 나타나는 특성입니다 위에서 소개해 드린 기법들은 별도의 연산을 추가하지 않는다면 시계열 축에서의 연관성을 찾아내지 못합니다 딥 러닝에서는 시간 축의 연관성을 찾아내기 위해서 TemporalCNN RNN Transformer 등 시퀀스 데이터에 특화된 모델을 많이 사용합니다 현재 시계열 이상 탐지는 데이터의 불균형 라벨링 데이터 부족 등의 문제로 정상 데이터만을 학습시키는 비지도 학습법이 대세를 이룹니다 그 안에서도 다양한 범주가 존재하지만 가장 큰 틀에서 모델들은 나눈다면 prediction reconstructionbased로 나눌 수 있습니다 각 방법론에 대한 자세한 설명과 예시는 다음 글에서 소개하도록 하겠습니다 Predictionbased 주어진 시계열을 분석하여 다음에 올 값을 예측하고 실제 값과의 차이를 시계열 이상 탐지에 사용하는 방법입니다 Reconstructionbased 예측과 달리 주어진 시계열 그 자체를 그대로 복원하여 그 차이를 시계열 이상 탐지에 사용하는 방법입니다 둘 중에서 어느 것이 더 성능이 뛰어나다고 말하기는 어렵지만 predictionbased는 예측을 시작한 시점에서 시간이 지날수록 오차가 점차 누적될 가능성이 있습니다 그런 이유로 일반적으로 reconstructionbased 방법이 더 안정적인 성능을 보여 줍니다 나가며 2 3부에 걸쳐 전통적인 이상 탐지 기법들에 대해 소개해 드렸습니다 다음 글은 시리즈의 마지막 이야기인 4부딥 러닝으로 이상 탐지하기로 최신 딥 러닝 이상 탐지 모델들과 제가 직접 경험했던 시계열 데이터를 다룰때 주의해야 할 점들에 대해 소개해 드리겠습니다 이미지 출처 및 참고 자료 Sherenaz W AlHaj Baddar 외 Anomaly Detection in Computer Networks A StateoftheArt Review ResearchGate December 2014 Pier Paolo Ippolito Support Vector Machines June 2019 Grace Zhang What is the kernel trick Why is it important November 2018 Markus M Breunig 외 LOF Identifying DensityBased Local Outliers Erik Bernhardsson Nearest neighbor methods and vector models part 1 September 2015 javaTpoint KMeans Clustering Algorithm javatpointcom Victor Dibia 외 Deep Learning for Anomaly Detection January 2020 Safaa Laqtib 외 A deep learning methods for intrusion detection systems based machine learning in MANET ResearchGate October 2019,https://i.ibb.co/v4t0vVt/2024-01-04-173902.png,https://meetup.nhncloud.com/posts/368
@ITSUB,국내최초! 삼성 갤럭시 북4 프로 달라진점 총정리 해봤습니다. 성능이 2배..증가했다?,https://img.youtube.com/vi/5doNJ10o2Gc/0.jpg,https://www.youtube.com/watch?v=5doNJ10o2Gc
카카오 '준신위' 2차 회의…카카오엔터·페이·뱅크 준법시스템 보고,,,https://n.news.naver.com/mnews/article/030/0003171588?sid=105
@jocoding,"하루 매출 1억 세이클럽, 350억 첫눈 엑싯, 5억 사용자 B612, 쉬운 딥러닝, 보이저엑스 창업 | 조코딩의 팟캐스트 #1 Part 1",https://img.youtube.com/vi/LeNe13HbEBw/0.jpg,https://www.youtube.com/watch?v=LeNe13HbEBw
AWS ALB로 부터 반환되는 502 bad gateway 에러 트러블슈팅,최근 신규 연동했던 서비스에서 502 에러응답이 온다는 클라이언트쪽 문의가 들어와서 이부분 트러블슈팅을 진행하였습니다 API에서 502를 응답하는 케이스는 없었기에 502를 어디서 응답하는지부터 파악해보았는데요 아키텍쳐를 고려했을때 AWS를 사용하고 있었고 NLB ALB ECSAPI 로 이어지는 구조였기때문에 502 응답이 올수 있는곳은 ALB 뿐이라고 판단했고 실제로 이 ALB의 Cloudwatch MetricHTTPCodeELB502Count을 확인했을때 해당 지표가 일부 시간대에서 0이상 값을 가지는 걸 확인했습니다 AWS 가이드에 따르면 정확히 어떤 이유로 ALB가 502를 응답하는지 확인하기 위해서는 해당 로드밸런서의 Access Log를 수집해야합니다 특별한 조치가 없을때는 디폴트로는 Access Log를 수집하지 않습니다 수집을 위해서는 몇가지 절차가 필요합니다 aws 문서 에 잘 나와있으니 참고하시면 됩니다 이를 요약해보자면 수집된 로그가 저장될 s3 버킷을 미리 만든다 해당 버킷의 Bucket Policy에 적절한 권한LB가 S3 object를 추가할수 있는 권한을 추가한다 ALB의 Access Log 옵션을 활성화하고 만들어둔 S3 버킷을 Destination 으로 추가한다 위 절차가 모두 완료되어 Access Log 가 S3 버킷에 잘 남는 걸 확인한 후에는 이제 Access Log 분석이 필요합니다 s3 에는 각 로그가 텍스트의 압축파일 형태로 저장되어 일일히 분석하기가 쉽지 않습니다 이때 활용할 수 있는것이 athena 입니다 Athena 는 s3에 저장된 데이터를 source로 하여 쿼리할 수 있는 서비스입니다 먼저 s3 에 저장된 데이터 기반으로 Athena 테이블을 만들어야하는데요 CREATE TABLE 로 시작하고 데이터 내 필드명등을 포함하는 Athena Query 를 작성하여야합니다 친절하게도 AWS 는 ALB Log를 Athena에서 쿼리할수 있도록 사전에 ALB 테이블 생성 쿼리를 만들어두었습니다 만들어진 Athena 테이블에 일반적인 SQL 쿼리를 사용할 수 있습니다 예를 들어 저희는 502 응답한 요청들을 확인해야하기 때문에 아래와 같은 쿼리를 사용하였습니다 SELECT FROM alblogs WHERE elbstatuscode 502 위 쿼리를 이용하여 문제가 되었던 요청들을 자세히 살펴보겠습니다 먼저 Count를 보겠습니다 Access log 를 활성화했던 26일 저녁부터 27일 점심정도까지 30건입니다 총 요청은 약 200만건정도로 502 응답의 비율은 00015정도입니다 매우 적은 수준이긴 했습니다 위의 가이드에서 확인해보라고 조언하는 세가지 지표requestprocessingtime targetprocessingtime and responseprocessingtime를 확인해보겠습니다 각 지표는 아래와 의미를 가집니다 requestprocessingtime은 Load Balancer이하 LB 가 요청을 받고 타겟으로 요청을 보내기까지의 시간 targetprocessingtime은 타겟이 로드밸런서가 요청을 보내고 타겟이 응답 헤더를 보내기까지의 시간입니다 responseprocessingtime 은 로드밸런서가 타겟으로부터 응답헤더를 받고 클라이언트로 응답을 보내기시작할때까지의 시간 세 필드의 값을 확인해봤을때 앞의 두 지표는 0이거나 매우 적은 값을 가지는 데 비해 responseprocessingtime는 1값을 가지는 것이 확인되었습니다 responseprocessingtime 값만 1을 가지는 케이스에 대해서 위의 AWS 가이드에 따르면 LB가 타겟에 요청을 보내는 중에 타겟이 TCP RST 나 TCP FIN으로 커넥션을 닫은 경우에 발생할 수 있다고 설명하고 있습니다 또한 보통 타겟의 keepalive timeout 이 LB의 idle timeout보다 낮은 경우에 발생하니 keepalive timeout을 idle timeout값보다 높혀야한다고 추가적으로 예시까지 들어주고 있습니다 ALB Target group 간의 커넥션에서 LB는 Client Target group은 Server 의 역할을 합니다 Keepalive timeout은 커넥션의 재사용 기능인 KeepaliveHTTP11 부터 디폴트로 적용의 제한 시간을 의미하고 서버에서 적용합니다 지금 우리 케이스의 경우에는 Target group인 API 서버에서 적용하는 내용입니다 Idle timeout의 경우에는 클라이언트인 로드밸런서가 서버와의 커넥션을 관리할때 커넥션들 중 특정시간동안 사용하지 않았던 유휴Idle 커넥션을 삭제하기 위해 삭제주기를 관리하기 위한 설정입니다 특별한 설정이 없으면 기본적으로 ALB는 Idletimeout 이 60초입니다 즉 60초동안 요청이 없었던 커넥션은 삭제하기 위함이지요 그렇다면 Target group의 keepalive timeout은 어떻게 설정되어있을까요 저희는 물리 서버 없이 ECS Fargate 로 서버를 운영중이고 웹서버로 Gunicorn 을 사용중에 있습니다 특별한 keepalive 설정없이 사용중이었고 확인해보니 기본 셋팅으로 사용하면 keepalive timeout 이 2초입니다 즉 위 이슈의 원인은 Gunicorn 과 LB 간의 Connection에 서버인 Gunicorn 은 요청이 없이 2초가 지났기 때문에 연결을 끊었고 클라이언트인 LB는 Idletimeut 이 60초이기 때문에 해당 커넥션이 끊긴지 모르고 닫힌 커넥션에 요청을 해서 응답을 받지 못해 502 응답을 하게 된것입니다 따라서 이 부분은 Target Server의 Keep alive timeout 을 Idletimeout인 60초보다 더 크게 65초로 증가시켜서 신규 배포하여 대응하였습니다 정확히 제가 AWS ALB의 동작방식을 알지는 못하지만 LB 와 Target Server 간에 통신시 적절한 크기의 Connection pool 을 두고 통신하고 있기 때문에 서버가 Keepalive timeout 을 좀 더 길게 가져가도 괜찮고 Default인 2초는 이러한 통신방식에서는 확실히 너무 짧다고 판단됩니다 Timeout 조정 작업 이후로는 502 응답은 더이상 발생하지 않는것으로 확인되었습니다 한줄요약 LB 와 Target group 간의 통신시 Target group server의 keepalive timeout은 꼭 LB Connection Idle timeout 보다 길게 가져가도록 하세요,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165428&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,좋코딩2 비하인드영상 【시즌2】,https://img.youtube.com/vi/qCX6OMbtRuQ/0.jpg,https://www.youtube.com/watch?v=qCX6OMbtRuQ
@ITSUB,스탠바이미의 대항마로 LG에서 새로 출시한 엘탠바이미ㅋㅋㅋ 이..이게뭐지..,https://img.youtube.com/vi/sqDiZn8uMLk/0.jpg,https://www.youtube.com/watch?v=sqDiZn8uMLk
ChatGPT ＆ Whisper를 코랩에서 빠르게 사용해보자 (쏘심플!),ChatGPT로 연일 핫한 OpenAI 의 API를 구글 코랩에서 쉽게 사용해서 ChatGPT 와 Whisper 모델을 한 번 체험해보자 OpenAI 의 API 사용법에 대해 간단히 알아보고 ChatGPT 와 Whisper API 사용을 위한 prompt 를 코랩에서 작성해보자 API 나 여러 함수들을 빠른 시간 안에 후딱 사용해보기 위해서라면 코랩만큼 간편한 환경이 없는 것 같다 ChatGPT 에 대한 이론적인 설명은 생략하겠다 0 OpenAI API KEY 받기 OpenAI 에서 제공하는 API 를 사용하기 위해서는 패키지 설치와 API Key 가 우선 필요하다 OpenAI에서 회원가입을 하면 3개월 안에 사용할 수 있는 크레딧 18 를 받을 수 있다 API Pricing 에 대한 자세한 내용은 여기를 참고 1 아래 링크 누르고 sign up 후 sign in 하기 httpschatopenaicomauthlogin 2 MyPage 에 들어가면 API Keys 를 클릭해보자 Create new secret key 를 누르면 API Key generated 라고 표시되고 파란색 블록 부분이 나의 API key 이다 API 사용량 도 아래와 같이 확인할 수 있다 18 Credit 중 얼마나 썼는지도 바로 확인 가능하다 1 OpenAI 설치 pip install upgrade openai APIKEY 입력 OPENAIAPIKEY 에 나의 API KEY 를 입력하면 된다 OPENAIAPIKEY lbkFJ1M9Tj 이 부분에 API KEY 입력하면 된다 import openai openaiapikey OPENAIAPIKEY completion openaiCompletion 아래는 사용 가능한 모델들의 리스트이다 gpt35turbo도 있는 걸 확인할 수 있다 참고로 ChatGPT는 GPT35를 2022년 초에 finetuned 한 모델이라고 보면 된다고 OpenAI 블로그에 나와있다 openaiModellist prompt 는 아래 형식이 기본이고 보통 text classification sentiment analysis text editting 과 같은 부분은 아래와 같은 함수를 사용하고 Chatcompletion 이라고 별도로 있는데 이게 소위 말하는 ChatGPT 의 API 라고 보면 될 것 같다 이메일이나 작문을 해주거나 파이썬 코드를 써주고 역할을 주어주면 대화가 가능한 agents 를 만들어주는 생성 을 해 준다 우선 아래는 우리가 소위 ChatGPT의 생성 부분은 아니고 전신 모델인 davinchi 모델의 basic use 라고 보면 되겠다 사실 basic use 라고 하지만 너무나 훌륭 각 API method 에 대한 자세한 설명을 보고 싶다면 official doc 참고 바란다 response completioncreate modeltextdavinci003 prompt Decide whether a Tweets sentiment is positive neutral or negative and scale it 0 to 10 nnTweet I loved the new Batman movienSentiment temperature0 default는 10 이며 02 사이 설정 0에 가까울수록 정제된 문장 높은 에 가까울수록 창의적이고 random한 문장 maxtokens64 대부분의 모델들은 최대 2048 token 까지 지원하나 최신 모델은 4096 토큰까지 지원 가능 topp10 모델이 사용할 토큰에 대한 threshold 라고 보면 된다 temperature 의 대안 둘 중 하나만 조정하기를 추천 presencepenalty00 특정 단어나 phrase 를 포함하지 않도록 22 까지 조정 가능한테 2에 가까울수록 penalty 가 커진다 frequencypenalty00 반복적이지 않은 텍스트를 생성하도록 유도 반복되며 penalty 부여되며 2에 가까울수록 penalty 가 커진다 prompt 를 보내면 OpenAI API 예측값을 아래 response 처럼 얻을 수 있다 printresponse choices finishreason stop index 0 logprobs null text Positive 10 created 1678685517 id cmpl6tV4n8fx4umxG5e9qPg2sVdT4xV7x model textdavinci003 object textcompletion usage completiontokens 3 prompttokens 39 totaltokens 42 printresponsechoices0text Positive 10 함수로 감싼다면 def questionprompt openaiapikey OPENAIAPIKEY response openaiCompletioncreate modeltextdavinci003 prompt fprompt temperature07 maxtokens256 topp1 frequencypenalty0 presencepenalty0 answer responsechoices0text return answer prompt Who is the president of France AIanswer questionprompt printAIanswer The president of France is Emmanuel Macron 2 ChatGPT API 로 QA 질문 던져 보기 ChatGPT 는 gpt35turbo 에 기초한 모델이며 위에서 언급했듯 이 모델은 이메일글 작문 QA answering 코드 작성 등 생성 application 을 만들 수 있게 되어 있다 Chat 모델은 여러 메세지들을 input 으로 받고 modelgenerated message 를 output 으로 리턴한다 gpt35turbo 를 사용하면 가장 안정화된 모델로 제공 받을 거고 만약 gpt35turbo0301 과 같은 specific version 을 사용할 수도 있다 모델 업데이트에 관한 내용은 여기서 자세히 확인할 수 있다 Fine Tuning 도 가능한다고 하니 gpt3 를 finetuning 해보고 싶다면 finetuning guide 를 보면 될 것 같다 GPT 35 는 현재 finetuning 은 제공되지 않으며 davinci curie babbage ada 모델들은 가능하다 ChatCompletion aka ChatGPT API 호출하기 input에는 크게 model 과 messages 가 들어가는데 messages 는 role과 content로 구성되어 있다 role system user assistant 중 선택 가능 content 메세지 내용 message의 content 에 대화 히스토리가 자세히 포함되어 있을수록 desired behavior 에 대한 힌트를 좀더 주는 셈이므로 사용자developer 입장에서 문맥 이해를 돕기 위한 대화 히스토리를 최대한 자세히 주는 것이 원하는 행동양식답변을 얻을 확률이 높아진다고 생각된다 system 의 message가 이 가장 첫 대화에 시작되는데 assistant 의 행동을 셋팅한다고보면 된다 user 의 message 가 assistant 에게 궁극적으로 물어보는 Question 인 셈 assitant 의 message 도 미리 주어줄 수 있는데 쉽게 말해서 이전 대화prior responses에 대한 답변을 어떻게 했는지 알려주는 거라고 보면 된다 API KEY는 위에서 주었기 때문에 바로 사용해보기 response openaiChatCompletioncreate modelgpt35turbo messages role system content You are a helpful assistant role user content Who won the world series in 2020 role assistant content The Los Angeles Dodgers won the World Series in 2020 role user content Where was it played printresponsechoices0messagecontent The 2020 World Series was played at Globe Life Field in Arlington Texas 최근 ChatGPT 의 그럴듯한 정확하지 않은 답변으로 논란이 많다 정확도가 높은 답변을 원한다면 초기에 셋팅되는 system 메세지에 좀더 specific 하게 instruction 을 주는 것을 추천한다고 한다 21년도 데이터까지로 훈련되었기 때문에 최근 정보에 대한 신뢰성은 확보될 수 없다는 점 참고 최근에 Twitter CEO가 Elon Musk 였다는 내용을 답변하는 ChatGPT 의 모습이 보이면서 추가적으로 training 이 된 것이 아니냐는 이야기도 있던데 3 Whisper API 도 써보자 Whisper 는 speechtotext model 로 Transcribe 과 Translatetakes supported languages into English 이 가능하다 마찬가지로 사용법은 너무 간단하다 31 Transcribe 해보기 먼저 audio 샘플 파일을 다운로드 받자 Ted Talk Try something new for 30 days 라는 Short Talk 을 Ted Talk 웹사이트에서 다운로드 받고 Colab에 MattCutts2011Ump3 라는 파일로 업로드 했다 위에서 API KEY 를 다 주었기 때문에 아래와 같이 간단하게 함수 호출만 해서 적용 해주면 된다 쏘 심플 audiofile opencontentMattCutts2011Ump3 rb transcript openaiAudiotranscribewhisper1 audiofile transcripttext Ted Talks are recorded live at the TED Conference This episode features engineer Matt Cutts This talk contains powerful visuals Download the video at TEDcom Heres Matt Cutts A few years ago I felt like I was stuck in a rut So I decided to follow in the footsteps of the great American philosopher Morgan Spurlock and try something new for 30 days The idea is actually pretty simple Think about something youve always wanted to add to your life and try it for the next 30 days It turns out 30 days is just about the right amount of time to add a new habit or subtract a habit like watching the news from your life Theres a few things that I learned while doing these 30day challenges The first was instead of the months flying by forgotten the time was much more memorable This was part of a challenge I did to take a picture every day for a month And I remember exactly where I was and what I was doing that day I also noticed that as I started to do more and harder 30day challenges my selfconfidence grew I went from deskdwelling computer nerd to the kind of guy who bikes to work for fun Even last year I ended up hiking up Mount Kilimanjaro the highest mountain in Africa I would never have been that adventurous before I started my 30day challenges I also figured out that if you really want something badly enough you can do anything for 30 days Have you ever wanted to write a novel Every November tens of thousands of people try to write their own 50000word novel from scratch in 30 days It turns out all you have to do is write 1667 words a day for a month So I did By the way the secret is not to go to sleep until youve written your words for the day You might be sleepdeprived but youll finish your novel Now is my book the next great American novel No I wrote it in a month Its awful But for the rest of my life if I meet John Hodgman at a TED party I dont have to say Im a computer scientist No no If I want to I can say Im a novelist So heres one last thing Id like to mention I learned that when I made small sustainable changes things I could keep doing they were more likely to stick Theres nothing wrong with big crazy challenges In fact theyre a ton of fun but theyre less likely to stick When I gave up sugar for 30 days day 31 looked like this So heres my question to you What are you waiting for I guarantee you the next 30 days are going to pass whether you like it or not So why not think about something you have always wanted to try and give it a shot for the next 30 days Thanks That was Matt Cutts recorded at TED 2011 in Long Beach California March 2011 For more information on TED visit TEDcom 32 Translate 마지막으로 한글 음성 파일을 영어로 번역해보는 Translate 기능을 사용해자 한글 음성 파일을 input 으로 주었을 때 영어 로 translated 되어 리턴하는 것을 볼 수 있다 audiofilekorean opencontent샘플 오디오 파일mp3 rb translated openaiAudiotranslatewhisper1 audiofilekorean translatedtext I prefer people who are in need I want to make a lot of money Because I was really poor I think thats really good People who have only experienced failure People who have worked in a company once but they have not succeeded in the first time I dont think anyone is thirsty for the first success The reason I started the business is How honest and close to my essence Is it closer to my nature Because thats how much motivation There is no basis to judge whether it will last long What can you see that this person will work hard for 10 years This person has a noble and grand vision If you meet a lot of ordinary people in the middle It can be gone soon If there is something like the most essential and most realistic deficiency I think its a very good energy source I think its an energy source that can not be exhausted I tend to ask if there are such things 써보니 어떤가 나의 간단 사용 후기 우선 API 를 사용하기 쉽게 잘 만들어 놓았다는 생각이 든다 함수만 바꾸면 기능이 조금씩 달라지게끔 해놓았기 때문에 prompt 를 한 번 익히면 직관적이고 큰 무리 없이 원하는 기능으로 사용할 수 있는 것 같다 gpt 35 turbo 의 경우 굉장히 저렴하게 나왔다고 알고 있는데 pricing 을 다른 플랫폼들과 비교해보지는 않았지만 OpenAI 의 마이페이지에서 나의 사용량을 확인할 수 있는 usage 인터페이스가 있고 API 상에서도 확인할 수 있다 quick 하게 돌려볼 때 유용할듯 23317 Update 2일전 OpenAI에서 GPT4 model 을 발표했는데 GPT4 API 는 waitlisted 이니 곧 출시 예정인 듯 싶다 Reference httpsplatformopenaicomdocsguideschatintroduction httpsplatformopenaicomdocsguidesfinetuning httpsopenaicomblogintroducingchatgptandwhisperapis httpsgithubcomopenaiopenaicookbook,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164646&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,"AI 뉴스 - 일론 xAI Grok 출시, OpenAI 유출, 감정 공감 AI, 영상 AI 혁신, 성우 논란, 바이든 AI 규제, K-AGI 등",https://img.youtube.com/vi/g7Dpu14iuok/0.jpg,https://www.youtube.com/watch?v=g7Dpu14iuok
Kubernetes 클러스터에 배포할 애플리케이션 로컬 디버깅 및 개발하기,Kubernetes 환경에서 로컬 디버깅 및 개발 방식 보통 Kubernetes 환경에서 로컬 개발은 아래와 같은 다양한 선택지 중 하나를 취사선택하여 개발할 수 있습니다 dockercompose 혹은 minikube 등으로 직접 로컬에서 클러스터 환경을 구성하여 개발 새로운 개발 환경을 구성할 경우 복잡한 환경 설정 및 설치 작업 등으로 인하여 부담이 될 수 있습니다 리소스 부족으로 인하여 로컬 환경이 느려지는 경우가 발생할 수 있습니다 모든 애플리케이션을 로컬에서 실행하여 개발 번과 같이 리소스 부족으로 인하여 로컬 환경이 느려지는 경우가 발생할 수 있습니다 다른 애플리케이션과의 의존성으로 인해 내가 담당하고 있지 않은 다른 애플리케이션까지 로컬에서 구동할 수 있어야 합니다 다양한 언어 및 프레임워크로 개발되는 MSA 특성상 복잡한 설치 작업 혹은 환경설정 작업이 수반될 수 있습니다 일부 애플리케이션만 실행하여 개발한 후 Kubernetes 클러스터 환경에 배포하여 확인 개발할 애플리케이션으로 들어오는 요청 및 응답을 직접 구성하여 테스트가 필요하며 기능 확인을 위해서 Kubernetes 환경에 배포하여 디버깅하는 작업이 필요합니다 수정된 애플리케이션을 클러스터 환경에 배포하여 확인하는 작업에 시간이 많이 소요될 경우 생산성이 저하될 수 있습니다 Telepresence는 빠른 로컬 개발 및 디버깅을 위한 도구이며 이러한 단점들을 해결할 수 있습니다 Telepresence 란 먼저 Telepresence의 사전적 의미는 공간적으로 떨어져 있는 장소 또는 가상의 장소를 경험하는 것입니다 이러한 단어의 뜻과 유사하게 Telepresence는 원격 Kubernetes 클러스터로 서비스를 프록시하여 로컬에서 개발 및 디버깅하는 과정을 용이하게 하는 도구입니다 Kubernetes 클러스터 환경에서 디버깅을 하려면 코드 컴파일하고 컨테이너 빌드하고 버전 바꿔서 태깅하고 푸시 한 다음 YAML 형식으로 변경해서 반영하고 실행 후 외부에서 접속이 가능하도록 네트워크 주소를 변환해 주는 포트 포워딩을 통해서 실행 결과를 확인해야 하는데 이러한 과정 없이도 디버깅을 편하게 할 수 있습니다 Telepresence는 2018년 5월 15일에 CNCF의 샌드박스 단계로 채택이 되었습니다 레거시 Telepresence와 Telepresence 2가칭와의 차이점 레거시 Telepresence의 경우 pod에서 동작하고 있는 서비스가 Telepresence proxy로 스와핑되는 방식으로 동작합니다 그리고 이 proxy는 서비스로 향하는 트래픽을 받아 해당 트래픽을 로컬 컴퓨터로 전달하는데 이러한 방식을 swapdeployment방식이라고 합니다 이 방식은 단순하지만 아래와 같은 결점이 있습니다 클러스터로의 연결이 유실될 경우 Deployment가 불안정한 상태로 남겨질 수 있습니다 즉 pod를 교체하는 데 시간이 걸리게 됩니다 Telepresence 2의 경우 이러한 문제를 해결하는 intercept를 중심으로 구축된 새로운 아키텍처를 도입하였습니다 Intercept란 sidecar proxytraffic agent가 pod에 주입되어 서비스로 향하는 트래픽을 받아서 로컬 컴퓨터로 라우팅하는 동작을 의미합니다 이러한 방식의 장점은 서비스가 항상 실행 중이기 때문에 모든 요청을 라우팅하지 않고 특정 유저에 속한 것으로 지정된 트래픽만 라우팅하여 개발자들끼리 서로 방해받지 않고 자유롭게 개발 및 디버깅을 진행할 수 있다는 것입니다 Telepresence의 아키텍처 살펴보기 Telepresence의 주요 구성요소는 아래와 같습니다 Telepresence CLI Telepresence CLI는 오케스트레이션을 수행합니다 즉 Telepresence Daemons를 실행하고 Ambassador Cloud에 대한 인증 및 Telepresence Daemons 중 하나인 User Daemon을 위한 사용자 친화적인 인터페이스를 제공합니다 Ambassador Cloud는 Telepresence를 개발한 Ambassador Labs가 제공하는 서비스로 Kubernetes 워크플로우에 맞춰 서비스를 개발하고 관리해 주는 인터페이스입니다 Telepresence Daemons Telepresence는 클러스터와 통신하고 intercept된 요청을 다루기 위해 로컬 컴퓨터에서 실행되고 클러스터 네트워크에 대한 통신의 주요 포인트가 되는 2가지 타입의 Daemon이 있습니다 UserDaemon Traffic Manager와 통신하여 intercept의 생성 및 삭제를 다룹니다 RootDaemon 가상 네트워크 장치VIF를 설정하여 로컬 컴퓨터와 클러스터 간의 트래픽을 처리하는데 필요한 네트워킹을 관리합니다 Traffic Manager Traffic Manager는 클러스터의 Traffic Agent와 로컬 컴퓨터의 Daemon 간 통신의 중심점입니다 intercepted pod에 Traffic Agent sidecar를 주입시켜 모든 관련된 유입 및 유출 트래픽을 프록시하고 active intercepts를 추적하는 역할을 합니다 Traffic Agent intercepts를 용이하게 하는 sidecar 컨테이너를 의미합니다 Telepresence의 기본 개념 Telepresence는 2개의 코어 아키텍처로 구성되어 있습니다 clientside Telepresence binary 로컬 컴퓨터에 설치된 CLI Tool clusterside trafficmanager and trafficagent 원격 쿠버네티스 클러스터에 위치 먼저 telepresence connect 명령어는 trafficmanager로 하여금 로컬 컴퓨터와 원격 클러스터 환경과의 양방향 proxied tunnel을 만듭니다 이것은 원격의 Kubernetes 서비스를 마치 로컬에서 동작하는 것처럼 인식하게 해줍니다 이후 telepresence intercept servicename 명령어는 trafficmanager를 트리거 하여 타깃 서비스와 연관된 Pod 내에서 실행되는 trafficagent proxy container를 설치하도록 합니다 이것은 개발 및 디버깅을 위해 원격 트래픽을 로컬 컴퓨터로 라우팅할 수 있게 해줍니다 그럼 간단한 튜토리얼을 통해서 Telepresence를 어떻게 활용할 수 있는지 살펴보도록 하겠습니다 Telepresence를 사용한 Remote Debugging 튜토리얼 로컬의 kubeconfig 파일을 원격 Kubernetis cluster 환경의 config 파일로 대체하여 로컬에서 원격 cluster 개발 환경 구성필수 MacOS 기준이며 사전에 docker desktop for Mac 및 Kubernetes 사용 설정과 kubectl 설치가 되어있다고 가정합니다 이후 Telepresence 및 관련 라이브러리 설치 설치하기 문서 참고 Install via brew Apple silicon Macs에 해당 다른 OS의 경우 설치 문서 참고 brew install datawireblackbirdtelepresencearm64 Make the binary executable 실행 권한 부여 sudo chmod ax opthomebrewbintelepresence traffic manager 설치 필요Traffic Manager installed successfully 글씨가 나오면 성공 어떤 helm chart가 install되는지 보려면 httpsgithubcomtelepresenceiotelepresencetreereleasev2chartstelepresence 참조 telepresence helm install 클러스터 환경의 traffic manager 실행 telepresence connect 명령어 실행 만약 오류 발생 시 vi UsersnhnLibraryLogstelepresenceconnectorlog에서 오류 내용 확인 가능 특정 위치에 파일이나 폴더가 없다는 오류가 발생하면 mkdir p UsersUSERLibraryCachestelepresence USER는 해당 컴퓨터의 사용자 이름으로 repleace 필요 Connected to context kubernetesadminkubernetes https1016252156443 메시지가 뜨면 성공 telepresence status 명령어로 상세 정보 확인 User Daemon과 Root Daemon이 실행 중 인지 확인 클러스터 환경에 배포되어 있는 pod 및 deployment 확인 사전에 8080 포트로 호출하면 Hello World 라고 출력되는 웹 애플리케이션아래의 leejeongwhapipelinetest가 해당이 배포되어 있어야 합니다 kubectl describe pods leejeonwhapipelinetest559f9b644ffgp2h kubectl describe deployment leejeonwhapipelinetest 노출할 서비스 생성 kubectl expose deployment leejeonwhapipelinetest typeLoadBalancer kubectl describe services leejeonwhapipelinetest 명령어로 IP 확인 후 호출 curl 101053438080으로 서비스 호출해 보기 IP 및 Port는 클러스터 환경 및 애플리케이션 설정에 따라 다를 수 있습니다 위에서 수행한 telepresence connect 명령어로 인해 로컬에서 원격 클러스터 환경에 직접 접근이 가능하게 됩니다 telepresence list 명령어로 intercept 가능한 리스트 확인 intercept를 통해 Kubernetes로 들어온 요청을 가로채기 telepresence intercept leejeonwhapipelinetest port 90008080 Kubernetes 클러스터의 8080으로 들어온 요청을 로컬의 1270019000 으로 전달 Kubernetes의 해당 app을 호출한 뒤 로컬에서 내용 변경 후 다시 호출해 보기 curl 101053438080 로컬 프로젝트 내용 변경 후 다시 호출하여 변경된 내용이 출력 되면 success 종료 telepresence quit Telepresence의 기타 기능 위의 예제로 살펴본 기능은 global intercept모든 요청을 intercept에 대한 부분이고 private intercept를 이용할 경우 특정 헤더 혹은 특정 경로에 대해서만 intercept할 수 있어 여러 개발자가 서로 간섭 없이 로컬 환경에서 개발 및 디버깅이 가능합니다 또한 Telepresence는 공유 가능한 preview URL을 생성할 수 있고 해당 환경을 팀원과 공유하여 두 명이 하나의 프로그램을 개발하는 Pair 프로그래밍을 할 수도 있습니다 Telepresence는 해당 미리 보기 URL에서 오는 요청만 로컬 환경으로 라우팅하게 됩니다인그레스에 대한 요청은 평소와 같이 클러스터로 라우팅 됩니다 Telepresence 사용 후기 처음 팀 내 프로젝트에 Telepresence를 사용하면서 global하게 요청을 intercept해서 다른 개발자들이 개발 클러스터 환경에서 테스트 중인 요청들도 모두 가로채어 문제가 되었는데 Team mode방식이 private interceptor를 지원해 준다는 사실을 알게 되었습니다 이후 Team mode 방식으로 새로 Telepresence를 설치한 뒤 요청 헤더에 개발자 별로 고유한 ID를 담아 다른 요청이 intercept 하지 않도록 하여 협업에 용이한 상태로 개발을 진행할 수 있었습니다 개인적으로 느낀 부분은 로컬 환경에서 원하는 component만 띄워서 개발 및 디버깅을 할 수 있는 장점이 있었으나 각 component에 사용자가 개별 정의한 header를 전달해 주는 세팅이 필요하며 모든 개발자들이 Telepresence 사용하는 방법에 협의를 하고 사용 방법에 대해서 어느 정도는 인지하고 있어야 한다는 단점이 있었습니다 이미지 출처 및 참고자료 httpswwwgetambassadoriodocstelepresencelatestreferencearchitecture httpswwwgetambassadoriodocstelepresencelatestreferenceclient httpswwwgetambassadoriodocstelepresencelatestconceptsintercepts httpswwwgetambassadoriodocstelepresencelatestinstall httpswwwdigitaloceancomcommunitytutorialshowtousetelepresenceonkubernetesforrapiddevelopmentonubuntu2004,https://i.ibb.co/v4t0vVt/2024-01-04-173902.png,https://meetup.nhncloud.com/posts/364
참여 후기: 한국소프트웨어종합학술대회 (KSC2022),Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/01/09/ksc-2022/
[Cloud Native 기반의 WebRTC 채팅 서비스 첫걸음] WebRTC 소개,소개 쿠버네티스 환경에서 다양한 서비스 개발운영 경험을 바탕으로 생성형 AI Use Case 발굴을 위한 Pilot 프로젝트를 진행하고자 합니다 이를 위해 Cloud Native 기반 기술을 활용하여 생성형 AI 기술과 연동하기 위한 WebRTC 기반의 인스턴스 메신저 서비스 ex Discord를 개발하고 이를 지탱하기 위한 플랫폼으로써 쿠버네티스 환경에서의 DevOps 기술에 대해 소개합니다 향후 이러한 메신저 서비스를 에이전트로 활용하여 생성형 AI 서비스와 연계하기 위한 인터페이스로 확장해 나갈 계획입니다 향후 소개할 목차는 다음과 같으며 첫 걸음으로 WebRTC 소개를 시작합니다 이어질 목차는 꾸준히 개발운영하면서 관련 기술문서 올릴 예정입니다 향후 목차 WebRTC 소개 Cloud Native 기반 WebRTC 아키텍처 설계 Kubernetes 클러스터 구축 Istio Service Mesh 를 이용한 Web RTC Traffic Management 활용 방안 WebRTC 서비스 개발 MERN Stack MongoDB Express React NodeJS GitOps Pattern 을 이용한 CICD ArgoCD 서비스 통합 모니터링 Grafana Prometheus WebRTC 소개 WebRTC는 웹 기반의 실시간 음성 영상 및 데이터 통신을 제공하는 오픈 소스 프로젝트 입니다 WebRTC를 사용하면 브라우저에서 플러그인이나 어플리케이션 설치 없이 다른 사용자와 실시간 연결하여 채팅 화상 회의 파일 공유 등의 서비스를 제공할 수 있습니다 WebRTC는 Google Mozilla Opera 등의 브라우저 업체와 IETF W3C 등의 표준화 기구에서 지원하고 있으며 이를 이용한 다양한 서비스가 개발되고 있습니다 WebRTC는 주로 JavaScript와 HTML5를 이용하여 개발되며 사용하기 쉽고 빠른 응답 시간을 보장합니다 또한 P2PPeertoPeer 기술을 이용하기 때문에 중간 서버를 거치지 않아 높은 보안성과 저렴한 비용을 제공합니다 WebRTC는 현재 다양한 분야에서 활용되고 있으며 브라우저 기반의 음성 영상 통화 서비스 뿐만 아니라 IoT ARVR 등의 새로운 분야에서도 활용 가능합니다 WebRTC 주요기능 1 실시간 음성영상 통신 WebRTC는 브라우저 상에서 플러그인이나 어플리케이션의 설치 없이 두 사용자 간 실시간 화상 회의나 음성 통화를 제공할 수 있습니다 이를 통해 음성 영상과 같은 멀티미디어 데이터는 P2P 통신을 통해 Low Latency 제공하여 지연 없이 빠르게 상대방과 커뮤니케이션이 가능합니다 2 데이터 채널 WebRTC는 데이터 채널을 통해 브라우저 간에 데이터를 전송할 수 있습니다 이는 파일 전송 채팅 등 다양한 용도로 활용될 수 있습니다 3 NAT Traversal WebRTC는 NATNetwork Address Translation Traversal 기술을 이용하여 중간 서버 없이 P2P 연결을 구성할 수 있습니다 이를 통해 높은 보안성과 저렴한 비용을 제공합니다 4 확장성 WebRTC는 다양한 플랫폼과 브라우저를 지원하며 다양한 기능을 확장할 수 있습니다 이를 통해 다양한 분야에서 활용이 가능합니다 WebRTC 아키텍처 WebRTC 아키텍처는 크게 3가지로 구성됩니다 1 Application Application은 WebRTC를 이용하여 개발된 응용 프로그램입니다 브라우저에서 실행 되며 JavaScript API를 이용하여 미디어 스트림을 제어하고 데이터 채널을 생성하고 P2P 연결을 설정합니다 2 Signaling Signaling은 WebRTC를 이용한 응용 프로그램에서 P2P 연결을 설정하기 위한 메타 데이터를 교환하는 과정입니다 Signaling은 미디어 스트림 ICEInteractive Connectivity Establishment 정보 코덱 정보 등의 메타 데이터를 교환하며 이를 통해 브라우저 간에 P2P 연결을 설정합니다 3 STUNTURN Server STUNTURN Server는 WebRTC에서 P2P 연결을 설정하기 위해 사용되는 중간 서버입니다 NAT Traversal을 위한 STUNSession Traversal Utilities for NAT 서버와 브라우저 간에 P2P 연결이 불가능할 경우 릴레이 서버 역할을 수행하는 TURNTraversal Using Relays around NAT 서버로 구성됩니다 Signalig 기술 WebRTC 기술은 P2P 통신에 최적화가 되어 있습니다 WebRTC에 사용되는 기술은 여러 가지가 있지만 크게 3가지의 클래스에 의해서 실시간 데이터 교환이 일어납니다 MediaStream 카메라와 마이크 등의 데이터 스트림 접근 RTCPeerConnection 암호화 및 대역폭 관리 및 오디오 비디오의 연결 Signaling 과정 RTCDataChannel 일반적인 데이터의 P2P 통신 이 3가지의 객체를 통해서 데이터 교환이 이뤄지며 RTCPeerConnection들이 적절하게 데이터를 교환할 수 있게 처리해 주는 과정을 시그널링Signaling이라고 합니다 WebRTC 주요 네트워크 기술 NAT Network Address Translation WebRTC P2P 통신을 위해 각 디바이스는 자신의 고유한 이름을 통해서 통신해야 합니다 이 고유한 이름이 Public IP 입니다 Public IP는 정적static 동적dynamic IP 가 있기 때문에 상황에 따라서는 디바이스를 식별할 수 있는 고유한 값이 될 수도 있고 안될 수도 있습니다 더 나아가 회사망내부망LAN 환경은 Private IP 망이기 때문에 복잡한 네트워크 환경에서 연결된 각 다비이스가 P2P 연결이 가능하기 위해서는 Public IP 가 반드시 필요하고 디바이스와 Public IP 관계를 맵핑시키기 위한 NAT 기술이 필요합니다 NATNetwork Address Translation은 사설 IP 주소를 공용 IP 주소로 변환하여 인터넷에 연결하는 기술입니다 NAT는 이를 이용하여 사설 네트워크에서 인터넷을 사용할 수 있도록 합니다 NAT를 사용하면 사설 IP 주소를 여러 대의 컴퓨터에서 공유할 수 있기 때문에 보안성이 향상되고 IP 주소의 부족 문제를 해결할 수 있습니다 NAT는 주로 가정에서 사용되며 공공 장소나 기업에서는 공인 IP 주소를 사용합니다 하지만 WebRTC 는 P2P 통신을 위해 Peer 의 Public IP를 알고 있어야 하지만 NAT 환경에서 클라이언트가 외부 퍼블릭망으로 요청을 전송할 때마다 Private IP Public IP NAT 변환 과정에서 Public IP 는 언제든지 변경 되어 WebRTC P2P 통신에 많은 문제를 야기시킬 수 있습니다 결국 네트워크는 NAT FW Router 다양한 네트워크 장비들이 혼재되어 있고 이러한 구조에서 WebRTC P2P 통신을 가능하게 하기 위한 근본적인 해결책으로 STUN TURN 를 활용할 수 있습니다 ICE Interactive Connectivity Establishment WebRTC의 ICEInteractive Connectivity Establishment Agent는 NATNetwork Address Translation Traversal을 위한 기술입니다 쉽게 설명하면 ICE는 Peer 가 서로 통신할 수 있는 최적의 경로를 찾을 수 있도록 도와주는 프레임워크 입니다 ICE Agent는 브라우저에서 사용 가능한 IP 주소와 포트를 수집하고 미디어 스트림을 전송하기 위해 브라우저 간에 P2P 연결을 설정하는 과정에서 이를 사용합니다 또한 ICE Agent는 NAT Traversal을 위해 STUNSession Traversal Utilities for NAT 서버와 TURNTraversal Using Relays around NAT 서버를 사용할 수 있습니다 STUN 서버는 브라우저가 공용 IP 주소와 포트를 얻을 수 있도록 도와줍니다 TURN 서버는 브라우저 간에 직접적인 연결이 불가능할 경우 릴레이 서버 역할을 수행하여 데이터 전송을 가능하게 합니다 ICE를 사용하는 이유는 다음과 같습니다 모든 단말은 각자의 네트워크 환경 학교 내부망 사내망 홈네트워크 등 다양하기 때문에 P2P 통신을 위한 네트워크가 단순하게 연결되지 않는다 방화벽이 존재하는 환경에서는 방화벽 룰 설정을 통해 트래픽이 통과해야 하고 Public IP가 없다면 NAT IP 를 할당해야 하고 라우터가 P2P 연결을 허용하지 않을 때는 별도의 Relay 서버가 필요하다 ICE 를 이용하면 각 Peer 환경에서 NAT 통신을 위해 필요한 모든 포트를 오픈하고 P2P 통신을 위한 엔드포인트 Public IP Port 정보를 모두 스캐닝하여 가지게 됩니다 결국 위 그림처럼 WebRTC P2P 통신을 위한 워크플로우는 다음과 같습니다 각 Peer 브라우저 환경에서 getUserMedia API를 이용하여 미디어 스트림Audio Video 캡쳐 RTCPeerConnection을 통해 P2P 연결을 설정 Signlaing 프로토콜을 통해 브라우저 간 세션 설정을 위한 메타 데이터를 교환 미디어 스트리밍 데이터를 중간 서버를 거치지 않고 STUN or Turn 서버를 통해 P2P 전송 STUN Session Traveral Utilities for NAT ICE 는 각 Peer 브라우저 환경에서 P2P 통신이 가능한 네트워크 환경을 스캐닝 하여 Public IP Port 정보를 STUN 서버로 전송하고 STUN 서버는 P2P 통신이 가능한 Public IP 주소를 맵핑관리함으로써 P2P 통신 시 Remote Peer의 Public IP 를 전송하는 역할을 수행합니다 즉 STUN은 P2P 연결을 확인하고 NAT 바인딩을 유지하기 위한 P2P 연결 유지 프로토콜로 활용합니다 TURN Traversal Usgin Relay around NAT NAT 보안 정책이 엄격한 곳이나 Router 장비에서 P2P 통신을 제한하는 경우에는 STUN 서버가 완벽한 해결책이 될 수 없습니다 이 경우 STUN 서버의 확장하여 Relay 기능을 제공하는 TRUN 서버를 활용할 수 있습니다 각 사설 로컬 네트워크 망에 존재하는 Peer 들은 TURN 서버를 통해 Remote Public IP 를 확인하고 Peer 들 간에 직접 통신하는게 아니라 Relay 서버 TURN 서버를 통해 경유합니다 TURN 서버는 복잡하고 보안이 강화된 네트워크 환경에서도 P2P 통신이 항상 가능한 환경을 제공하는 장점도 있지만 STUN에 비해 리소스 낭비가 큽니다 따라서 ICE Candidate 과정에서 Remote IPLocal IPor Public IP를 연결 가능한 후보군을 모두 찾아낸 이후 불가능할 경우 최후의 수단으로 TURN 을 활용해야 합니다 WebRTC API WebRTC 개발을 위해 JS 라이브러리에서 제공하는 주요 클래스 API 는 다음과 같습니다 주요 클래스 MediaStream 카메라와 마이크 등의 데이터 스트림 접근 RTCPeerConnection 암호화 및 대역폭 관리 및 오디오 비디오의 연결 RTCDataChannel 일반적인 데이터의 P2P 통신 Video Codec WebRTC 에서 오디오비디오 데이터를 압축하여 전송하기 위해 사용하는 코덱이 필요합니다 각 브라우저Chrome Edge Firefox Safari 밴더마다 오디오비디오에 대해 지원하는 코덱 정보 확인이 가능합니다 Video Codec WebRTC는 모든 호환되는 브라우저가 지원해야 하는 기본 코덱 세트baseline를 설정합니다 일부 브라우저는 다른 코덱도 허용하도록 선택할 수 있습니다 아래는 WebRTC를 완전히 준수하는 브라우저에 필요한 비디오 코덱과 필요한 프로필 및 실제로 요구 사항을 충족하는 브라우저 표 입니다 Mandatory video codecs Codec name Profiles Browser compatibility VP8 Chrome Edge Firefox Safari 121 AVC H264 Constrained Baseline CB Chrome 52 Edge Firefox Safari 필수 코덱 외에도 일부 브라우저는 추가 코덱도 지원합니다 다음 표에 나와 있습니다 Other video codecs Codec name Profiles Browser compatibility VP9 Chrome 48 Firefox Audio Codec RFC 7874에서 모든 WebRTC 호환 브라우저가 지원해야 하는 오디오 코덱은 아래 표에 나와 있습니다 Codec name Browser compatibility Opus Chrome Edge Firefox Safari G711 PCM Alaw Chrome Firefox Safari G711 PCM law Chrome Firefox Safari 필수 오디오 코덱 외에도 일부 브라우저는 추가 코덱도 지원합니다 다음 표에 나와 있습니다 Codec name Browser compatibility G722 Chrome Firefox Safari iLBC Chrome Safari iSAC Chrome Safari WebRTC Library WebRTC는 계속 발전하고 있고 각 브라우저마다 지원 수준이 다릅니다 따라서 각 브라우저 호환성을 위해서 Google에서 제공하는 adapterjs 라이브러리 사용을 추천하고 있습니다 Adapterjs는 shim 및 polyfill을 사용하여 다양한 플랫폼에서 WebRTC 구현 간의 다양한 차이점을 없애줍니다 또한 WebRTC 개발 프로세스를 전체적으로 쉽게 수행 할 수 있도록 접두사와 다른 이름 지정의 차이점을 처리하며보다 광범위하게 호환되는 결과를 제공합니다 라이브러리는 NPM 패키지로도 제공됩니다 따라서 향후 MERN Stack 을 이용하여 개발시 해당 라이브러리 적용을 검토할 예정입니다 WebRTC VS WebSocket Vs Socketio 비교 WebRTC WebSocket Scokeio 기술 차이점과 주요 장단점에 대해 살펴봅니다 WebScoket WebSocket은 HTTP와 달리 양방향 통신을 할 수 있는 기술입니다 클라이언트가 서버와 연결을 맺고 난 후 서버와 클라이언트는 상호간에 자발적으로 메시지를 보내고 받을 수 있습니다 이를 통해 서버와 클라이언트 간 실시간으로 데이터를 주고 받을 수 있습니다 WebSocket은 이전에는 불가능했던 실시간 웹 애플리케이션을 구현하는 데 매우 유용합니다 하지만 WebSocket은 실시간으로 데이터를 처리하기 위한 기능만을 제공하므로 WebRTC와 같은 비디오 및 오디오 스트리밍 처리와 같은 다른 기능은 제공하지 않습니다 websocket 장단점 장점 적은비용 낮은 복잡도 빠름 단점 지원하지 않는 브라우저가 다수 존재 Socketio SocketIO는 WebSocket 프로토콜을 기반으로하는 실시간 양방향 통신을 위한 JavaScript 라이브러리 입니다 웹 브라우저와 서버 간의 실시간 통신을 위해 설계 되었으며 WebSocket 이외의 다른 프로토콜에 대한 지원도 제공합니다 SocketIO를 사용하면 서버에서 푸시 알림 실시간 채팅 등을 구현할 수 있습니다 socketio 장단점 장점 새로운 사람이 채팅방에 들어왔음을 연결된 모든 사용자들에게 한번에 알려야하는 경우 socketio는 연결된 모든 클라이언트에 메세지를 브로드캐스팅 할 수 있지만 websocket은 연결된 사용자들의 리스트를 받아와서 한명씩 메시지를 보내야합니다 또한 소켓 연결 실패 시 socketio는 fallback을 통해 다른 방식으로 알아서 reconnect하지만 websocket은 reconnect를 시도하지 않습니다 단점 많은 비용 자원 boilerplate 코드 요약하면 빠르게 적은 비용으로 많은 데이터를 처리하는 경우에는 websocket을 사용하고 연결된 클라이언트들을 세밀하게 처리하고 Broadcasting 기능이 필요한경우에는 socketio 사용합니다 WebRTC WebRTC와 WebSocket은 모두 웹 기술이지만 다른 목적을 가지고 있습니다 WebRTC는 WebScoket 처럼 브라우저와 서버 통신 방식이 아닌 중간서버 경유 없이 P2P 통신에 중점을 둔 기술로 오디오 비디오 및 데이터를 브라우저 간에 직접적으로 전송할 수 있습니다 반면 WebSocket은 서버와 클라이언트 간의 양방향 통신을 지원하기 위한 기술로 HTTP 연결을 유지하면서 양방향 통신을 가능하게 합니다 WebSocket에 연결된 클라아언트를 세밀하게 처리하고 Broadcasting 이 필요한 경우 WebSocket 프로토콜을 이용한 JS 라이브러리 socketio 를 활용하기도 합니다 즉 WebSocket은 서버와 클라이언트 간의 실시간 양방향 통신에 적합하고 WebRTC는 P2P 통신에 적합합니다 WebSocket 을 통해 영상을 주고 받을 수 있는데 WebRTC를 사용하는 이유는 무엇일까요 WebRTC 는 영상 오디오 임의의 데이터의 통신이 highperformance hightquality 이도록 설계되었기 때문입니다 WebRTC 는 브라우저간 직접 통신이어서 훨씬 빠릅니다 WebRTC 지연시간이 훨씬 짧습니다 lowlatency 따라서 WebRTC 다음의 요구사항을 충족하는 서비스에 적합합니다 브라우저 또는 모바일 단말에서 저지연 시간의 비디오 오디오 전송 입력 인터페이스 키보드 마우스 게임패드 등 입력의 이벤트 지연 시간이 짧은 스트리밍 브라우저 또는 이동형 단말에 대한 게임 스트리밍 서비스 반대로 WebRTC 에 적합하지 않은 서비스는 다음과 같습니다 사전 녹화된 미디어 또는 렌더링된 대규모 스트리밍 서비스 최신 브라우저 에서 지원되지 않는 동영상 포멧에 대한 스트리밍 브라우저간 호환성이 문제 소지가 있는 서비스 WebRTC 에서 시그널링 과정을 통해 세션을 생성하고 미디어 데이터를 스트리밍하기 까지 모든 과정을 WebRTC 만으로 제어하기 어렵습니다 따라서 WebSocket or Socketio 를 이용해서 WebRTC 세션 생성을 위한 시그널링을 처리를 별도 서버를 구축하는 방법론이 일반적입니다 Next 다음 번에는 WebRTC 서비스를 위한 Cloud Native 기반의 아키텍처 설계 안 에 대해서 소개합니다 Cloud Native 기반 WebRTC 아키텍처 설계,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164885&boardType=techBlog&searchData=&page=&subIndex=
@ITSUB,형보다 괜찮은 아우? 애플 아이폰 15 달라진 핵심기능 7가지!,https://img.youtube.com/vi/GtWhC-ITUsI/0.jpg,https://www.youtube.com/watch?v=GtWhC-ITUsI
브라우저를 IPFS 노드로 쓰는 것에 관하여,IPFS는 Inter Planetary File System 행성간 파일시스템입니다 웅장한 이름만큼 웹3 세상에서 큰 역할을 할 프로토콜이라고 생각합니다 관련 과제를 진행하다 메모한 것을 공유합니다 IPFS 들어봤던 건데 정도의 느낌만 가져가셔도 성공입니다 IPFS 실행체의 종류 스탠드얼론 노드 kubo 예전에 goipfs라고 불리던 것 rustipfs nodejsipfs jsipfs를 nodejs 환경에서 실행한 것 브라우저 노드 jsipfs를 크롬 파이어폭스 사파리에서 실행한 것 브라우저 노드의 제약 1 네트워크 전송 수단으로 WebSocket과 WebRTC만 쓸 수 있다 당연히 TCP와 UDP를 사용할 수 없다 2 Secure Context에서만 사용 가능하다 localhost 서버 또는 httpS 서버만 사용할 수 있다 3 DHT를 일부만 지원한다 DHT Distributed hash tables는 peer 검색과 컨텐츠 검색에 이용된다 브라우적 노드는 피어 탐색과 컨텐츠 탐색을 다른 서버서비스에 의존해야 한다 IPFS in JS 관련 용어 Kubo goipfs에서 바뀐 프로젝트 이름 jsipfs JavaScript로 만든 IPFS 구현체 Helia가 릴리즈되면 개발을 중단할 예정 Helia 개발 진행 중인 JavaScript IPFS 구현체 최종 이름은 TBD 아직 확정되지 않음 Delegate node 위임 노드 브라우저 노드는 DHT 기능이 부족하기 때문에 Delegate node의 도움이 필요 Delegate node는 apiv0dht API를 제공 Preload node 브라우저 노드는 브라우저 탭을 닫으면 종료됨 수시로 운영이 중단되는 불안정한 IPFS 노드 브라우저 노드가 IPFS 네트워크에 추가하는 데이터를 안정적으로 유지하기 위해 Preload node가 필요 Preload node는 apiv0refs API를 제공 IPFS in JS 개발 현황 시간이 지나면서 드러난 IPFS in JS의 사용 패턴 Kubo를 원격으로 제어 장점 기능 제약 없는 IPFS 노드를 제어 단점 Kubo 서버 별도 운영이 필요 Nodejs 환경에서 jsipfs를 실행 장점 아키텍처가 간단하고 디버깅이 용이 단점 항상 Kubo에 비해 뒤쳐지는 개발 진도 브라우저 환경에서 jsipfs를 실행 장점 진정한 P2P WebRTC를 이용한 브라우저브라우저 통신 단점 가용성 확보를 위해 Centralized 인프라 의존 필요 WebRTC 통신 수립을 위한 star 시그널링 서버 2022년 2023년 개발 계획 브라우저 친화적 p2p 전송 계층을 구현할 것이다 Cetnralized 인프라 의존성 제거 목표 2023년 Helia를 릴리즈 할 것이다 브라우저 electron node deno bun 등 모든 웹 실행체를 위한 개발할 예정 기존 jsipfs와의 호환성을 생각하지 않고 JS 개발자에게 친화적인 API를 제공할 예정 기존 라이브러리를 활용할 예정 jslibp2p jsbitswap 브라우저 사용성에 집중할 예정 브라우저가 Helia의 특별한 런타임 Helia가 릴리즈되면 jsipfs에 대한 메인테넌스는 중단될 것이다 브라우저 IPFS 베스트 프랙틱스 1 star 시그널링 서버를 직접 운영하라 IPFS 개발팀이 운영하는 디폴트 star 서버들은 안정적이지 않다 사용 서비스에 사용하면 안된다 2 preload 노드를 직접 운영하라 IPFS 개발팀이 운영하는 디폴트 preload 노드는 안정적이지 않다 사용 서비스에 사용하면 안된다 3 인프라를 운영하고 싶지 않다면 Web3 Storage 같은 IPFS pinning 서비스의 사용을 고려하라 Ref Web3를 위한 DB OrbitDB IPFS 기반 탈중앙화 DB 소개 IPFS in JS 현황 httpsblogipfstechstateofipfsinjs IPFS in JS의 과거 현재 미래 GitHub libp2pwebrtcstar 소스레포 httpsgithubcomlibp2pjslibp2pwebrtcstar webrtcstarsignalling 서버 설치실행 방법 안내 GitHub jsipfs 소스레포 httpsgithubcomipfsjsipfs Web3 세상의 데이터 네트워크와 블록체인 서비스 인프라 현황 Tech 세미나 영상 httpsdevoceanskcomvlogviewdoid318vcodeA03,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164565&boardType=techBlog&searchData=&page=&subIndex=
RBAC 과 Service Accounts 를 사용하여 사용자 권한 제어하기,Kubernetes 에서 ServiceAccount 를 생성하면 122 버전까지는 자동으로 token 을 생성하였다 그러나 123 부터는 토큰을 자동으로 생성해 주지 않기 때문에 수동으로 생성해야 한다 이 바뀐 기능은 ServiceAcount 와 RBAC 을 연동하여 권한을 제어하고자 할 때 문제가 되므로 수동으로 만드는 방법을 살펴본다 1 네임스페이스 SA 생성 먼저 테스트할 네임스페이스를 만든다 kubectl create ns ask ask 네임스페이스에 서비스 어카운트를 생성한다 kubectl create sa asksa n ask 124 버전부터는 sa 를 생성해도 token 이 자동으로 생성되지 않는다 참고 httpsgithubcomkuberneteskubernetesblobmasterCHANGELOGCHANGELOG124mdnoreallyyoumustreadthisbeforeyouupgrade token 은 Secret 타입이므로 Secret 을 조회해 보면 token이 자동 생성되지 않았음을 알 수 있다 참고 httpskubernetesiodocsconceptsconfigurationsecretserviceaccounttokensecrets 2 Token 생성 asksa 에 해당하는 token 을 수동으로 생성한다 cat EOF kubectl apply f apiVersion v1 kind Secret metadata name asksa namespace ask annotations kubernetesioserviceaccountname asksa type kubernetesioserviceaccounttoken EOF token 을 생성할 때 어노테이션으로 연결될 서비스 어카운트를 지정한다 kubectl get secret n ask NAME TYPE DATA AGE asksa kubernetesioserviceaccounttoken 3 7s 조회를 하면 serviceaccounttoke 타입으로 secret 이 생성되었음을 알 수 있다 혹은 token 을 수동으로 생성하는 방법도 있다 kubectl create token asksa boundobjectkind Secret boundobjectname asksa duration999999h n ask output xxxxxxxxxxxxxxxxxxxxxxx base64 로 변환하여 secret 에 datatoken 값으로 저장한다 kubectl create token asksa boundobjectkind Secret boundobjectname asksa duration999999h n ask base64 w 0 output xxxxxxxxxxxxxxxxxxxxxxx kubectl edit secret asksa n ask data token xxxxxxxxxxxxxxxxxxxx 3 Role 과 RoleBinding 생성 Role 과 RoleBinding 은 네임스페이스 별로 연결된다 그러므로 생성한 권한은 해당 네임스페이스에만 권한이 주어진다 먼저 Role 을 생성한다 cat EOF kubectl apply f apiVersion rbacauthorizationk8siov1 kind Role metadata name askrole namespace ask rules apiGroups resources verbs EOF apiGroup 에서 은 core API group 으로 다음의 출력으로 확인할 수 있다 APIVERSION 이 v1 인 리소스들이 core API group 이 되면 이들에 대해서 권한을 사용하겠다는 뜻이다 kubectl apiresources o wide NAME SHORTNAMES APIVERSION NAMESPACED KIND VERBS bindings v1 true Binding create componentstatuses cs v1 false ComponentStatus get list configmaps cm v1 true ConfigMap create delete deletecollection get list patch update watch endpoints ep v1 true Endpoints create delete deletecollection get list patch update watch events ev v1 true Event create delete deletecollection get list patch update watch limitranges limits v1 true LimitRange create delete deletecollection get list patch update watch namespaces ns v1 false Namespace create delete get list patch update watch nodes no v1 false Node create delete deletecollection get list patch update watch persistentvolumeclaims pvc v1 true PersistentVolumeClaim create delete deletecollection get list patch update watch persistentvolumes pv v1 false PersistentVolume create delete deletecollection get list patch update watch pods po v1 true Pod create delete deletecollection get list patch update watch podtemplates v1 true PodTemplate create delete deletecollection get list patch update watch replicationcontrollers rc v1 true ReplicationController create delete deletecollection get list patch update watch resourcequotas quota v1 true ResourceQuota create delete deletecollection get list patch update watch secrets v1 true Secret create delete deletecollection get list patch update watch serviceaccounts sa v1 true ServiceAccount create delete deletecollection get list patch update watch services svc v1 true Service create delete deletecollection get list patch update watch 다음은 Rolebinding을 생성한다 cat EOF kubectl apply f apiVersion rbacauthorizationk8siov1 kind RoleBinding metadata name askrolebinding namespace ask subjects kind ServiceAccount name asksa namespace ask roleRef kind Role name askrole apiGroup rbacauthorizationk8sio EOF ServiceAcount 인 asksa 와 askrole Role 을 서로 연결 시킨 다는 의미이다 이렇게 되면 이제 asksa sa 는 askrole role 에 대한 권한만을 사용할 수 있다 4 kubeconfig 생성 sa 를 만들었으니 이를 연동할 kubeconfig 를 만들어 본다 token 을 조회해 보자 kubectl get secret n ask asksa ojsonpathdatatoken base64 d output xxxxxxxxxxxxxxxxxxxxxxxxx token 값으로 kubeconfig 의 user 접속 token 에 넣는다 apiVersion v1 clusters cluster certificateauthoritydata xxxxxxxxxxxxxxxxxxxxxxxx server httpsxxxxxxxxxxapnortheast2eksamazonawscom name mycluster contexts context cluster mycluster user asksa namespace ask name mycluster currentcontext mycluster kind Config users name asksa user token xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx 위의 kubeconfig 로 접속하면 ask 네임스페이스에 대해서 kubectl 명령어를 실행할 수 있다 kubectl kubeconfig askkubeconfig get pods Error from server Forbidden pods is forbidden User systemserviceaccountaskasksa cannot list resource pods in API group in the namespace default default 네임스페이스에는 권한이 없으므로 권한 없음 에러가 리턴된다 kubectl kubeconfig askkubeconfig get pods n ask No resources found in ask namespace ask 네임스페이스의 파드는 정상적으로 조회된다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165215&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,"심화되는 AI 저작권 논란, 미국 저작권청의 결정은?",https://img.youtube.com/vi/z9Vst07CX88/0.jpg,https://www.youtube.com/watch?v=z9Vst07CX88
@coohde,React 서버 통신에 회의가 든다면 - RTK Query,https://img.youtube.com/vi/pnpO3o8mLBU/0.jpg,https://www.youtube.com/watch?v=pnpO3o8mLBU
@jocoding,코딩 1년 배우고 전국 학교 뒤흔든 레전드 중학생 개발자 인터뷰,https://img.youtube.com/vi/brYCFFWA_6M/0.jpg,https://www.youtube.com/watch?v=brYCFFWA_6M
DEVOCEAN 메타분석 - 1. 데이터 탐색,DEVOCEAN 메타분석 1 데이터 탐색 Dev Ocean Devocean 코드를 거래하는 플랫폼과 비즈니스가 있나요 2017년 여름 데이터 분석에 입문하는 첫 수업에서 제가 교수님께 드린 질문입니다 파이썬으로 할 수 있는 것들에 대해 듣던 도중으로 기억하는데 아는 파이썬이라고는 스타크래프트1의 맵 밖에는 없었던 제게는 코드를 몇 번 타이핑 하는 것으로 놀라운 결과물들이 나오는 모습이 마냥 신기해 보였던 것이겠지요 마법의 재료가 되는 코드들을 사고 파는 곳을 상상했던 것 같습니다 교수님의 답변이 기억나지는 않지만 개방과 공유가 tech 업계의 성과를 만들어 냈음을 이제는 이해하고 있습니다 google io reinventAWS SK Tech Summit 배우고 알아낸 것들을 공유하는 이유는 천차만별입니다 이제 막 공부를 시작하여 작심삼일에 그치지 않기 위해 기록을 시작하는 분도 있고 취업 및 진학을 위한 포트폴리오를 작성하거나 개인적 성장을 위한 개발 블로그유튜브 등도 요즘엔 쉽게 찾아볼 수 있습니다 위와 같은 모습은 개인에게만 국한된 것은 아닙니다 IT 기업들도 공유에 적극적이고 해외뿐 아니라 국내의 기업들도 Tech 블로그를 관리하며 Tech 컨퍼런스를 개최하는 것이 일반적인 모습이 되었습니다 인류의 발전에 기여하고 타인에 도움이 되고자 Committed to significantly imporving the lives of as many people as people Google Founders Letter 2004 라는 이유도 있겠으나 너무도 낭만적인 이야기이고 자사의 상품서비스를 알리고 커뮤니티생태계를 확장하기 위함이 주된 목적이겠지요 그렇게 만들어진 선순환의 사이클의 강력함을 모두가 경험하고 있으니까요 어떤 기술이 망하지 않는 가장 확실한 방법은 그 기술이 널리 쓰여버리는 일입니다 디지털 전환기의 고객 기업들은 늘 새로운 기술에 갈증을 느낍니다 디지털 기술이 결정적인 차별화를 만들어내는 주체가 되어가고 있음을 이제는 깨닫게 되어서지요 고객이 만족할 수밖에 없는 체험을 정확하고 신속히 만들어 내기 위해 디지털 기술이 활용되면서 이 마법을 실현해 내는 주인공개발자에도 관심이 갈 수밖에 없습니다 그들이 우리 기술을 쓰도록 하고 또 우리 안에도 그러한 주인공들을 늘리는 일이 중요해집니다김국현의 트렌드툰 009DRDeveloper Relations 그리고 이 곳 데보션DEVOCEAN도 마찬가지로 Tech 공유의 공간들 중에 하나입니다 데보션에는 SK그룹의 Tech 관련 구성원들의데보션 마스터 글들이 실립니다 스터디 관련 글 데보션 Young 분들의 글을 포함해 대부분의 글들은 각 분야에 대한 생각고민들을 담고 있습니다 데보션 마스터 분들의 글에는 관련 분야의 전문가답게 컨텐츠의 퀄리티가 높은데 문득 궁금해졌습니다 언제 어떤 글들이 업로드 되었는지 데보션에는 어떤 주제의 글들이 주로 업로드 되었을까 주제에는 당시의 화제성이 반영되었을까 등등 그래서 데보션에 등록된 글들에 대해 분석해 보고자 합니다 우선 데보션에 올라온 글들의 데이터부터 수집해야겠지요 데이터 수집 1 수집 데이터 데보션 Tech Blog 메뉴의 글들 2 데이터 수집기간 20230430까지 등록된 글 3 수집 방법 스크래핑python 사전에 스크래핑에 대한 허가 받음 수집 데이터 데이터 수집을 했으니 이제 본격적으로 분석을 할 차례입니다 분석을 하는 뚜렷한 목적이나 증명하고 싶은 가설은 딱히 없습니다 현재까지 모아둔 데이터를 그저 탐색해보겠습니다 데이터 탐색 하나의 숫자가 모든 것을 말해주지 않듯 하나의 그래프도 마찬가지이다 There is no more reason to expect one graph to tell all than to expect one number to do the sameJohn Tukey Exploratory Data Analysis 1977 고전적인 통계학은 적은 표본샘플으로 더 큰 모집단에 대한 결론을 도출하기 위해 복잡한 과정에 집중했었고 가설 검정통계적 추론에 치우친 경향이 있었습니다 미국의 통계학자 존 투키는 이런 통계학에 대해 개혁을 요구했고 1977년 이젠 고전이 된 그의 저서 Exploratory Data Analysis를 통해 요약 통계량과 시각화를 통해 데이터가 갖고 있는 본연의 의미에 접근하는 방법을 정립하였습니다 통계에 컴퓨터 과학을 접목하기 위해 끊임없이 노력했고 비트 소프트웨어 같은 용어를 만들고 정의하기도 했습니다 하나의 그래프에 다양한 정보들을 담고 있는 box plot 역시 그의 작품이지요 존 투키의 오랜 신념을 따라서 우리도 모아둔 데이터에서 의미를 밝혀내보도록하지요 데보션 우상향 Tech Blog 메뉴에 첫 글이 올라온 것이 2018년 8월 28일이고 2023년 4월말 현재까지 총 570개의 글이 작성되었습니다 현재까지 150명의 작성자분들이 작성해주셨고 이 글들의 조회수를 모두 더하면 약 107만회입니다 기간별로 보자면 2021년 4월까지는 한달 평균 4개 정도의 글이 작성되었지만 21년 5월부터 꾸준히 늘어 현재는 한 달에 평균 20개 정도의 글이 쓰이고 있네요 글을 작성하는 데보션 마스터분들도 21년 4월까지는 한 달에 평균 두 분이었지만 현재는 평균 열 다섯분 정도입니다 이렇게 보면 21년 5월부터 데보션에 어떤 중대한 변화가 있었던 것 같은데 현재 갖고 있는 데이터만으로는 어떤 일이 때문에 발생한 패턴의 변화인지 확인하기 어렵네요 글들이 언제 올라왔는지 확인했으니 이제 어떤 글들의 조회수가 높았는지 확인해보겠습니다 어떤 요일에 글들이 많이 등록되었는지도 확인해보았습니다 월요일에서 주말로 갈수록 등록한 글의 비율이 줄어드는데 월요일부터 계단식으로 줄어드는 것이 인상적입니다 금요일과 주말엔 역시 휴식이지요 그럼에도 공휴일에 작성해서 올린 글들도 10개나 되었습니다 이와는 별개로 한국 공휴일 달력이 제대로 되어 있는 python 라이브러리가 없는 것 같습니다제가 못찾았는지도 모르겠습니다 한국 공휴일은 대체 공휴일 선거 등이 있어서 공공데이터 포털에서 가져오는 것이 가장 좋은 것 같습니다 한국 공휴일 만드는 코드근로자의 날 따로 추가 pip install holidays 사용안함 공공API 활용 from datetime import datetime import requests import json import pandas as pd from pandas import jsonnormalize 데이터 원본 httpswwwdatagokrindexdo key 인증키decoding url httpapisdatagokrB090041openapiserviceSpcdeInfoServicegetRestDeInfo datelst dates for year in range2018 2024 for month in range113 params solYear stryear solMonth strmonthzfill2 typejson ServiceKey key res requestsgeturl params params dic jsonloadsrestext if dicresponsebodytotalCount 0 pass elif dicresponsebodytotalCount 2 for i in rangedicresponsebodytotalCount dates locdate dicresponsebodyitemsitemilocdate datename dicresponsebodyitemsitemidateName isholiday dicresponsebodyitemsitemiisHoliday datelstappenddates else dates locdate dicresponsebodyitemsitemlocdate datename dicresponsebodyitemsitemdateName isholiday dicresponsebodyitemsitemisHoliday datelstappenddates tmp pdDataFramedatelst tmplocdate pdtodatetimetmplocdate formatYmd dayofweek 0 Mon 1 Tue 2 Wed 3 Thu 4 Fri 5 Sat 6 Sun 샘플 날짜 생성 krcal pdDataFramedates pddaterangestart20180101 end20231231 krcaldayofweek krcaldatesdtdayofweekmapdayofweek krcal krcalmergetmp lefton dates righton locdate how leftdroplocdate axis1 krcaldatenamefillnaN inplaceTrue krcalisholidayfillnaN inplaceTrue 달력 잘 되어 있나 확인 caldfcaldfdatesbetween2023050120230531 근로자의날 없는 것 확인 추가하기 caldfloccaldfdatesstrslice510 0501datename 근로자의날 caldfloccaldfdatesstrslice510 0501isholiday Y 조회수 Top5 데보션의 글 중 5000 이상의 조회수를 기록한 글은 전체의 7 약 50개입니다 가장 높은 조회수를 기록한 글은 Notion에 대한 글이었네요 사실 전체 글 중에 Notion에 대한 글이 하나 더 있는데 해당 글도 조회수가11000건으로 매우 높습니다 같은 기간20182023 Notion에 대한 인기도 꾸준히 늘었는데구글 트렌드 아마 Notion을 검색하다 데보션에 있는 Notion 관련 글을 조회한 사람들이 많았던 것 같습니다 Notion 구글 트렌드 글 속에 이미지가 많을 수록 조회수가 많았을까 싶었는데 그렇지는 않았습니다 Tech Blog 글에는 좋아요도 표시할 수 있습니다 아래처럼요 좋아요가 많은 글들은 어떤 글들일까요 조회수 이미지 개수 댓글수와 좋아요의 상관관계를 확인해보면 아래와 같습니다 좋아요 Top 5 댓글좋아요가 많을 수밖에 없는 이벤트 관련 글들을 제외했음에도 댓글 수와 좋아요는 꽤 관련이 있는 것 같습니다 댓글의 내용이 좋은 정보 공유 감사하다는 내용인 것을 보면 확실히 그런 것 같네요 현재 갖고 있는 데이터셋에서 의식의 흐름대로 데이터를 탐색해 보았습니다 한 달에 평균적으로 20개의 글들이 대략 15분의 마스터 분들이 작성한다는 것을 알게 되었고 Notion은 확실히 인기가 많은 것 같구요 데보션의 댓글과 좋아요는 상관관계가 높은데 이는 주로 긍정적인 코멘트의 댓글이 많기 때문인 것으로 풀이됩니다 이제 수집한 데이터의 조회수 날짜 등 정형 데이터에서 벗어나 글의 내용 및 주제에 대해 좀 더 확인해보도록 하지요 2편에서 계속 남은 이야기 글을 쓰다 보면 자연스레 검색을 많이 하게 됩니다 내가 제대로 이해하고 있는 것이 맞는지 기억이 잘 못 되었는지는 않았는지 제가 쓰는 글이 굉장한 영향력권위를 갖고 있진 않아도 잘못된 정보가 담긴 채로 다른 분들에게 전해지는 것은 여러모로 바람직하지 않으니까요 이번 글을 쓰면서 가장 많이 찾아본 것은 존 투키입니다 EDA 단어를 만든 저명한 통계학자 정도로 알고 있었는데 그의 제자가데이비드 도노호 쓴 50 Years Of Data Science를 보면서 데이터 분석 분야를 확장 하기 위한 그의 열정이 얼마나 대단했는지 알 수 있었네요 반면 존 투키가 남긴 EDA라는 용어를 반가워 하지 않는 분도 있더군요 미시간 대학교에서 정보시각화Infovis를 담당하고 있는 Eytan Adar 교수는 자신의 수업에서 학생들에게 항상 이렇게 말한다고 합니다 No exploration No exploratory No exploring No explore 그러면서 존 투키가 남긴 다른 멋진 단어들software bit cepstrum quefrency에 비해 EDAExploratory Data Analysis는 근본적으로 모호한 단어라는 것이 문제라고 얘기합니다 탐색은 어떤 목적을 갖는 것으로 풀이 될 수도 있고seek something 비목적성을 갖는다고seek nothing in particular 해석될 수도 있기 떄문입니다 자기가 볼 때 존 투키는 전자를 가리킨 것인데 학생들은 후자로 받아들인다면서 인사이드 놀라움 탐험탐색 모두 모호하여 적어도 수업에서는 적절하지 않다는 것이 그의 주장입니다 빈약하지만 제 경험에 비추어봐도 작정 데이터 탐색부터 시작하면 노력에 비해 얻는 것이 많지 않았던 것 같습니다 하지만 가설을 설정하여 접근하는 것도 기존 도메인에 묶인다는 한계가 있지요 결국 가장 바람직한 것은 도메인으로부터 나온 몇가지 가설을 확인하다가 아무도 몰랐던 그렇지만 알고 나서는 놀라지 않을 수 없는 신비로운 패턴을 발견하는 것인데 대부분의 정말 놀라운 패턴들은 아쉽게도 데이터 정합성 부터 잘못된 경우도 적지 않았던 것 같네요 여러모로 쉬운 일은 아니지만 지루함들 속에서 찾을 수 있는 가치가 있다고 오늘도 믿어봅니다 Taking boring flat data and bringing it to life through visualization John Tukey Reference 1 book 데이터 과학을 위한 통계2판 OReilly 2 데이비드 도노호 50 Years of Data Science 2015 httpscoursescsailmitedu183372015docs50YearsDataSciencepdf 3 공공데이터포털 공휴일 가져오기 httpstempdevtistorycom34 4 Banning exploration in my class httpsmediumcomeytanadarbanningexplorationinmyinfovisclass9578676a4705,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164870&boardType=techBlog&searchData=&page=&subIndex=
실전에서 효과보는 Jira툴 기반 요구사항 관리,개요 프로젝트 수행 성공을 위해 도구의 할용은 중요한 요소가 된 지 오래되었습니다 소프트웨어 공학에서는 소프트웨어 위기 극복을 위한 하나의 방안으로 표준화된 방식을 지향하고 있습니다 프로젝트를 수행하면서 실전에서 요구사항 관리를 효과적으로 수행할 수 있었던 방법을 소개하고자 합니다 요구사항 관리의 방식 일반적으로 프로젝트 수행의 성공과 실패 요인의 많은 비중을 차지하는 요구사항 관리가 중요함에도 불구하고 실무에서는 정교한 방식을 추구하고자 하는 노력이 생각보단 관심이 적었던 것으로 경험이 남아 있습니다 제가 개발 프로젝트를 처음 진행할 당시만 하더라도 대부분 엑셀 파일에 요구사항을 listup 해서 초기에 확보한 후에 추가변경삭제 등 관리는 어렵게 진행이 되었습니다 그러다 보니 요구사항을 만족시키기 위한 노력은 더욱 힘들게 진행될 수 밖에 없었던 것 같습니다 발주자와 수주자 입장이 서로 차이가 나기 시작하면서 프로젝트가 간극은 점점 더 커질 수 밖에 없었습니다 도구 활용의 노력만 좀 더 있었더라면 프로젝트 품질은 더욱 향상될 수 있었을 텐데 말입니다 도구 활용에 대한 이해 1년전에 새로 프로젝트를 맡으면서 그 동안 생각해 오던 소프트웨어 공학 실천을 실현할 수 있는 계기가 있었습니다 해당 조직에서는 Jira Confluence 등 도구 사용에 거부감이 없었고 개발사와도 도구 활용 중심의 프로젝트 관리가 진행되고 있어 활용 이해도에 대해서는 걱정을 할 필요가 없었습니다 하지만 프로세스적인 연결 흐름을 만들어 줘야 한다는 인사이트를 얻었고 도구 중심의 프로젝트 관리 정책에 대한 생각을 공유하였습니다 요구사항 중심의 도구 활용 개발 프로젝트 수행에 있어 이미 강조드린 것처럼 요구사항 관리가 제일 먼저 앞서서 관리가 되어야 합니다 프로젝트의 가장 중요한 시점라고 할 수 있습니다 해당 프로젝트 실무 참여자의 이해 수준을 기반으로 Jira도구를 공식화하는 작업에 공을 들여야 했습니다 프로세스의 확립을 위한 발주자와 수주자간의 적응기간이라고 생각하였습니다 기존에는 Jira를 사용하면서도 슬랙채널허들텔레그램메일 등 다양한 소통내용이 모두 요구사항으로 회자들 되어 있었습니다 하지만 Jira에 등록되지 않은 요구사항은 관리되지 않는 요구사항으로 소통을 했으며 적극적으로 Jira를 활용한 요구사항 등록과 관리가 꾸준이 시행이 될 수 있도록 노력을 아끼지 않았습니다 로젝트 참여자들의 요구사항을 바라보는 관점의 변화가 생겨났고 그 동안 요구사항에 대해 관리가 어렵다는 인식도 상호간에 조금씩 사라졌습니다 지금은 자발적으로 백로그 기반으로 요구사항을 등록하고 그 백로그 기반으로 자연스럽게 설계구현테스트배포 활동으로 연결되는 흐름을 잘 이어가고 있습니다 이로써 요구사항 관리에 대한 불확실한 느낌은 사라진 것으로 보입니다 실전에서 사용하는 도구 중심의 요구사항 관리 사례 프로젝트 참여자들의 도구에 대한 이해와 요구사항 관리에 대한 자신감으로 이어진 실전에서 사용했던 요구사항 관리 방법을 Jira도구 중심으로 사례를 아래와 같이 소개합니다 애자일 프로젝트 관리 기능 방식 Jira에서 제공하는 애자일 프로젝트 구성요소를 활용하는 방식으로 프로젝트 요구사항 관리를 시스템적으로 운영될 수 있도록 수행한 사례입니다 1 보드 만들기 단계 보드는 백로그 구성요소를 생성하기 위한 사전 단계로 보드 만들기 보드 만들기에서 애자일 보드 만들기 스크럼 보드 만들기 선택 보드의 이름 정하기 2 백로그로 요구사항 관리 가능 단계 지정한 보드명으로 생성이 완료되면 백로그 가 생겨서 요구사항 등록부터 시작할 수 있는 요구사항 관리가 가능함 3 Epic 만들기 단계 요구사항을 등록하기 위한 준비과정으로 상위 단위인 Epic을 생성 Epic은 Jira의 이슈 기능에서 이슈 유형을 Epic으로 선택하여 생성 Epic은 요구사항인 스토리를 묶음으로 큰 작업 단위 여러 번의 스프린트를 포함하는 큰 업무 단위 4 요구사항 작성 단계 백로그가 생겨서 이제 요구사항 작성 가능 하단의 백로그 패널에서 이슈생성 을 클릭하여 요구사항 등록 백로그에 요구사항 등록이 되면 이제부터 스프린트에 배정할 Task가 준비된 상황 5 스프린트 만들기 단계 스프린트 이름과 기간을 설정하여 만들기 프로젝트에서 스프린트는 일반적으로 2주이며 사용자 정의가 가능 6 스프린트 계획 단계 요구사항 중에 스프린트에 배정할 대상이라면 드래그하여 스프린트 계획에 등록 스프린트 계획에 반영 완료 실전에서 시도해 보기 지금까지 요구사항 관리의 중요성을 강조하며 도구 활용의 실무 경험 사례를 소개하였습니다 애자일 개발방법론의 용어들이 나왔지만 사례 내용에서 충분히 실무적 이해가 되실 것으로 보이구요 요구사항 관리를 위해 도구 하나쯤은 잘 사용해 볼 것을 권장합니다 생각보다 개발 생산성을 높일 수 있고 프로젝트 수행의 성공 경험을 한 단계 업그레이드 할 수 있는 계기가 될 것으로 기대합니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164579&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,요즘 난리난 TypeScript 퇴출 논란 ㄷㄷ 노드와 파이썬까지?!,https://img.youtube.com/vi/BBGE5NARkjc/0.jpg,https://www.youtube.com/watch?v=BBGE5NARkjc
클라우드 담당자가 태블로를 사용하는 이유,안녕하세요 저는 Cloud SA 로서 다양한 분야의 고객을 만나고 있습니다 그렇다고 데이터 분석 쪽의 전담 SA는 아닙니다 그런데 제가 왜 태블로를 사용하게 되었을까요 이유는 데이터가 많기 때문입니다 클라우드에는 데이터가 많습니다 클라우드의 많은 서비스에서 발생되는 로그 정보들 비용 데이터 등이 있습니다 데이터의 양은 큰 편인데요 AWS 기준으로 비용 데이터는 월 사용량 150K 정도의 고객의 데이터는 매월 10GB 의 데이터가 발생됩니다 환경에 따라 조금 다를 수 있습니다 이 데이터에서 생각하지 못한 좋은 정보를 찾는 방법은 시각화가 좋은 방법이라고 생각합니다 그리고 시각화한 데이터를 고객에게 제공하면 좋은 인사이트를 제공할 수 있겠다고 생각하고 태블로를 사용하게 되었습니다 태블로 말고도 다양한 선택지가 있을 수 있는데요 제가 태블로를 선택한 이유가 있습니다 저는 정기적으로 만나는 고객도 있지만 고객을 매번 다르게 만나는 경우도 있습니다 따라서 다양한 형태의 데이터를 연결해서 분석해야 되는데 태블로는 많은 데이터 형식을 지원합니다 또한 별도로 환경 구축할 필요 없이 desktop 설치형으로 간편하게 사용할 수 있습니다 공유가 필요하면 server와 cloud 버전이 있는데 이 역시 cloud 버전으로 간편하게 사용할 수 있습니다 무엇보다 태블로를 선택한 가장 큰 이유는 사용하기가 쉽기 때문입니다 물론 더 의미있는 시각화를 만들기 위해선 좀 더 고급 스킬이 필요합니다 저는 태블로 교육을 받아본 적 없습니다 그래도 고객 데이터를 분석하기 위한 용도와 고객에게 제공할 화면을 만드는 정도는 어렵지 않게 잘 사용하고 있습니다 의미 있는 데이터는 아래와 같이 금방 찾을 수 있습니다 예를 들면 AWS 에서 RDS 요금이 고민이라면 그래프를 아래와 같이 그려보면 StorageIOUsage 를 확인해봐야겠구나 라는 걸 금방 찾을 수 있습니다 아래는 고객에게 보여드리는 대시보드 화면입니다 태블로는 반응형 툴로 각 정보에 필터를 적용해두면 데이터의 특정 정보를 선택하면 그에 맞도록 데이터가 새로 바뀌어 분석하기 정말 편합니다 예를 들면 일별 그래프 대시보드에서 12월 가장 피크 친 12일 날짜의 정보가 궁금하다면빨간 동그라미 선택하면 아래와 같이 해당 일자에 맞는 데이터로 그래프를 다시 그려줍니다 그 외에도 지도에 데이터를 표현하는 것도 기본 제공해주는 map으로도 아래와 같은 그림을 만들 수 있습니다 이외에도 추천 드릴 기능이 많지만 기능 소개가 아니니 여기서 마치겠습니다 다양한 시각화 화면은 아래 사이트에서 참고해보세요 Tableau Public 업무에 데이터가 많다면 시각화 해보시길 추천드립니다 감사합니다 참고 이 글은 DEVOCEAN 지원으로 태블로를 사용해 본 후기 입니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164566&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,인공지능 탑재한 최신 Meta 스마트 안경 리뷰겸 브이로그 (ft. YouTube Media Summit),https://img.youtube.com/vi/fy4Vc7qqwDU/0.jpg,https://www.youtube.com/watch?v=fy4Vc7qqwDU
@ITSUB,윈도우가 깔린 닌텐도 스위치 등장!? 신형 게임기 레노버 리전고 언빡싱,https://img.youtube.com/vi/Jw5aMVsM5yE/0.jpg,https://www.youtube.com/watch?v=Jw5aMVsM5yE
MARP : Markdown 으로 프레젠테이션(ppt) 만들기,개발자에게 문서화는 귀찮으면서도 뗄레야 뗄수없는 커뮤니케이션의 하나의 수단이기도 합니다 그런 개발자에게 익숙한 포맷은 아무래도 마크다운Markdown일겁니다 github의 README를 포함하여 노션 슬랙 등에서도 지원하는 포맷이기도 하죠 하지만 발표자료를 만들어야할 때 가장 먼저 떠오르는건 역시 파워포인트ppt나 pdf를 떠올리게 될텐데요 부득이하게 Markdown으로 만든 자료를 구글 독스나 MS 파워포인트 등의 툴을 이용하여 재작성하는 경우가 많습니다 혹시나 Markdown 을 활발하게 작성하는 개발자라면 이걸로 어떻게 발표자료 못 만드나 싶은 생각을 한번쯤 해보셨을텐데 MARP 가 그에 대한 하나의 대안이 될 수 있습니다 Installation VSCode Extentsion으로도 제공하는 MARP는 npm 으로 설치할 수 있는 Marp CLI 와 Visual Studio CodeVS Code Extension을 제공합니다 VSCode Extension에서 쉽게 검색해서 설치할 수 있습니다 설치가 끝났으면 VSCode 에서 markdown 문법으로 간단한 프리젠테이션을 만들어봅시다 Getting Started 다음은 MARP 홈페이지에 올라온 프리젠테이션 예시입니다 MARP 에서 Markdown을 변환 가능한 포맷인지 인지하기 위해 Markdown의 맨 위는 항상 다음과 같이 시작해야 합니다 marp true 여기서 전체적인 프리젠테이션의 테마와 같은 여러 세팅을 설정해서 쓸 수 있습니다 위에 보이는 그림은 MARP 홈페이지에 나온 예제 프리젠테이션입니다 하기와 같은 Markdown 포맷으로 문서를 만들고 VSCode 에서 Export Slide deck 을 통해 다양한 포맷으로 변환할 수 있습니다 문서는 다음과 같이 Markdown 문법으로 작성합니다 theme gaia class lead paginate true backgroundColor fff backgroundImage urlhttpsmarpappassetsherobackgroundsvg bg left40 80httpsmarpappassetsmarpsvg Marp Markdown Presentation Ecosystem httpsmarpapp How to write slides Split pages by horizontal ruler Its very simple satisfied markdown Slide 1 foobar Slide 2 foobar Review MARP 의 한계점은 아직 테마가 전문 툴만큼 많지 않고 세부적인 요소들의 사이즈 조절 및 위치 조정 등이 지원되지 않거나 불편한 점이 있습니다만 전문적인 자료 작성이 아닌 팀 내부 세미나 용도나 개인적인 문서 작성 용도로는 Markdown 문서를 재활용하여 빠르게 프리젠테이션 준비를 할 수 있다는게 장점으로 보입니다 Reference MARP Official website MARP Github MARP 마크다운md 문서로 PPT PDF를 만들어 보자 Marp Markdown으로 프레젠테이션 만들기 Marp for VS Code,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165264&boardType=techBlog&searchData=&page=&subIndex=
라마2에 적용된 추론 속도 향상 기술인 GQA(Grouped Query Attention)에 대해,우선 라마Llama 2에 대해서는 데보션에 올라온 Meta LLM 패권 전쟁 격화시키다 LlaMA2 상업적 이용가능 오픈소스로 공개 포스팅을 봐주세요 라마2와 1의 기술적인 차이가 많지는 않은데요 그 중에서도 모델 구조상 차이는 한 가지 뿐입니다 바로 라마2에는 GQA Grouped Query Attention가 적용되었다는 점입니다 이 기술은 추론 속도를 빠르게 하려고 도입되었는데요 메타에서 개발한 것은 아니고 구글에서 발표하였습니다 논문 링크 Grouped Query란 GQA를 설명하려면 우선 MQA MultiQuery Attention을 알아야 합니다 그리고 MQA를 알려면 먼저 MHA MultiHead Attention을 알아야 하는데요 MHA는 트랜스포머 논문에서 발표되었습니다 이 글의 독자는 트랜스포머와 MHA는 잘 아신다고 가정하겠습니다 그렇다면 MQA란 MHA와 어떤 차이가 있는 것일까요 QKV Query Key Value head가 각각 H개씩 있는 MHA와 달리 MQA에서는 QKV 중에서 key와 value의 헤드가 하나씩만 있습니다 Key와 value는 공유하는데 query는 여전히 여러개라서 multi query라고 부릅니다 아래 그림의 왼쪽과 오른쪽을 보면 더 쉽게 이해할 수 있습니다 MHA에 비해 MQA가 간결해 보입니다 그림에서 짐작할 수 있듯 MQA는 메모리 대역폭을 절약할 수 있다는 장점이 있습니다 그런데 MQA는 MHA에 비해 성능이 저하될 위험이 있고 학습이 불안정하다는 문제가 있습니다 이 문제를 해결하려고 GQA가 제안되었습니다 위 그림의 가운데를 보면 GQA는 MHA와 MQA의 중간입니다 H개 있던 KV 헤드를 1개로 줄이는 대신 적절한 G개의 그룹으로 줄이는 것입니다 G가 1이 되면 MQA가 되고 H가 되면 MHA가 되니까 GQA는 MHA와 MQA를 포함한 일반화라고 볼 수 있습니다 GQA의 성능 얼핏 보면 GQA는 MHA와 MQA의 중간에 있는 기법이니 성능도 어중간하지 않을까 걱정할 수 있는데요 저자들이 T5 모델로 실험을 하였습니다 아래 그림을 보면 GQA가 MQA와 비슷하게 빠르고 MHA와 비슷하게 성능이 좋은 훌륭한 방법임을 보여주고 있습니다 보통 이렇게 두 가지 측면에서 모두 잘하기는 힘든데 역시 구글입니다 GQA에서는 그룹 크기를 적절히 정해야 할 텐데 어떻게 정해줄까요 대략 원래 있던 H의 제곱근 정도로 하면 됩니다 H가 64일 때 G가 8이면 적절한 성능이 나오는 것을 보여주는 그래프입니다 GQA는 학습 속도를 향상시키는 기술은 아니고요 미미한 향상 효과는 있습니다 위 그래프에 나오듯 추론 속도를 몇 배 향상시킵니다 이미 만들어놓은 모델에도 GQA를 활용할 수 있을까 LLM 학습에는 오랜 시간과 많은 자원이 투입되므로 모델을 다 만들어놓았는데 GQA를 새로 적용해서 다시 학습시키는 것은 큰 부담입니다 다행히 GQA는 사후에 적용이 가능합니다 아래는 pretrain된 모델에 대해서 추가로 만큼 GQA로 학습시킬 때의 성능을 보여주는 그래프입니다 가 높으면 좋지만 아예 0일 때에도 GQA는 성능저하가 심하지 않습니다 이 결과에 따르면 비록 라마2에서 7B 13B는 GQA를 적용하지 않았고 70B만 GQA로 학습시켰지만 7B 13B에도 사용 가능하겠습니다 물론 라마뿐만 아니라 다른 곳에서 만든 모델에도 마찬가지로 적용할 수 있습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165192&boardType=techBlog&searchData=&page=&subIndex=
"엔비디아, 가정용 AI 반도체 3종 공개",,,https://n.news.naver.com/mnews/article/016/0002249435?sid=105
@nomadcoders,2023년 나온 따끈따끈 신상 CSS 알려드림.,https://img.youtube.com/vi/vc4DGouf62E/0.jpg,https://www.youtube.com/watch?v=vc4DGouf62E
,,,https://n.news.naver.com/mnews/article/015/0004934238?sid=105
"애플 MR 헤드셋 ‘비전 프로’, 내달 2일 미국 출시 확정",,,https://n.news.naver.com/mnews/article/469/0000779312?sid=105
삼성홍대 500m 거리에 애플스토어…경쟁사 안방서 견제구,,,https://n.news.naver.com/mnews/article/014/0005125215?sid=105
Mojo🔥: AI를 위한 새로운 프로그래밍 언어,세계에서 가장 많이 사용되는 프로그래밍 언어 중 하나인 Python은 그 사용자 친화적인 특성 덕분에 다양한 분야에서 활용되고 있습니다 웹 개발자부터 데이터 과학자 마케터 심지어 AI 전문가 등 여러 전문가들이 Python을 선택하는 주된 이유는 초보자에게도 친숙하며 읽고 쓰기 쉬운 언어라는 점입니다 그러나 C이나 Rust에 비해 Python의 실행 속도는 훨씬 느리고 배포 과정이 복잡하다는 단점이 있습니다 Python 프로그램을 완성한 후 이를 사용자에게 배포하려면 Python 자체를 설치해야 하고 해당 프로그램을 실행하는 데 필요한 모든 패키지를 추가로 설치해야 합니다 C은 속도가 빠르지만 배우고 사용하기 어렵다는 단점이 있었습니다 이러한 문제점들을 해결하기 위해 등장한 것이 바로 Mojo입니다 Mojo의 특징 단인 스레드인 Python에 비해 Mojo는 병렬 처리를 위한 최적화된 컴파일러 인프라인 MLIR를 사용하기 때문에 Mojo를 사용하여 AI 모델을 작성하면 병렬 처리를 통해 성능을 향상할 수 있습니다 그리고 Mojo는 Python보다 약 35000배 빠른 속도로 동작합니다 Mojo와 Python의 구문은 몇 가지 새로운 기능을 제외하면 거의 동일하며 Mojo는 Python과 완벽하게 호환되는 것을 목표로 하고 있습니다 Python의 모든 기능과 구문을 그대로 사용할 수 있고 이에 더해 훨씬 강력한 새로운 기능을 추가되었습니다 따라서 이미 존재하는 모든 Python 패키지와 라이브러리는 기본적으로 Mojo에서도 완벽하게 동작합니다 Mojo는 AI 하드웨어를 프로그래밍하는 데 사용할 수 있습니다 이는 기존의 프로그래밍 언어로는 불가능했던 AI 하드웨어의 복잡한 구조를 구현할 수 있게 합니다 C이나 CUDA가 필요하지 않습니다 C이나 CUDA는 AI 하드웨어를 프로그래밍하는 데 사용되는 프로그래밍 언어이지만 배우고 사용하기 어렵다는 단점이 있습니다 Mojo는 이러한 단점을 보완하여 누구나 쉽게 AI 하드웨어를 프로그래밍할 수 있도록 합니다 또한 사전 처리와 사후 처리 작업을 통해 모델을 쉽게 확장하거나 커스텀 연산으로 작업 대체가 가능합니다 커널 퓨전 그래프 재작성 형상 함수 등을 활용하여 더욱 효과적인 작업이 가능합니다 Mojo는 컴파일러 언어이기 때문에 프로그램이 완성되면 단일 파일로 패키징하여 배포할 수 있습니다 더불어 Rust와 유사하게 메모리 안전을 보장하기 위한 소유권 및 차용 검사 기능도 함께 제공됩니다 Mojo 사용하기 Mojo는 아직 초기 개발 단계이며 신청자에 한해 Playground 액세스를 허용하고 있습니다 아래 사이트로 이동합니다 Mojo Programming language for all of AI Get started with Mojo 버튼을 클릭합니다 기본 정보를 입력 후 Next 를 클릭합니다 아래와 같이 나온 다면 신청이 완료된 것입니다 이후 Playground 초대 메일이 올 때까지 기다려야 합니다 저와 같은 경우는 신청 후 약 10분 뒤에 메일이 왔습니다 Access Mojo Playgroud 를 클릭하면 Sign in 할 수 있는 페이지로 이동합니다 Sign in 을 클릭하고 계속 진행합니다 Loading 과정을 거쳐 Playground 세션이 준비됩니다 현재 많은 사람이 Playground에 액세스 하고 있고 효율적인 리소스를 사용할 수 있도록 Playground의 세션 길이를 2시간으로 제한하고 있습니다 만약 Playground를 계속 사용하려면 언제든지 세션을 다시 시작하면 됩니다 Playground모습은 Jupyter Lab과 같습니다 아래 코드는 Mojo에서도 문제없이 실행됩니다 def addarg1 arg2 c arg1 arg2 printc 하지만 Mojo는 def 대신 fn을 사용하여 언제든지 개발자가 더 빠른 모드를 선택할 수 있게 합니다 이 모드에서는 모든 변수의 유형을 명시적으로 선언해야 합니다 결과적으로 Mojo는 최적화된 기계어 코드를 생성하여 함수를 구현할 수 있습니다 fn addarg1 Int arg2 Int let c Int arg1 arg2 printc 또한 class 대신 struct를 사용하면 속성들이 메모리에 밀접하게 압축되어 데이터 구조를 포인터 추적 없이 사용할 수 있습니다 이러한 기능은 C와 같은 언어의 빠른 속도를 가능하게 합니다 class MyClass A simple example class def fself x selfx x return selfx struct MyClass A simple example struct var x Int fn finout self x Int selfx x return selfx Mojo는 불변의 값과 가변의 값을 위해 let과 var 선언을 지원합니다 let은 불변이며 var는 가변입니다 fn comparearg Int let l Int 10 var v Int v arg for i in rangel printv v v 1 Python module을 import 하는 방법은 아래와 같습니다 from PythonInterface import Python This is equivalent to Pythons import numpy as np let np Pythonimportmodulenumpy Now use numpy as if writing in Python array nparray1 2 3 printarray 정리 및 의견 Mojo 프로젝트는 이제 초기단계이지만 그 가능성과 전망은 무척이나 환상적인 것 같습니다 Mojo가 제공하는 풍부한 기능과 강력한 성능 향상은 그 자체로 큰 매력이며 이것이 어떻게 인공지능 머신러닝 분야를 진화시킬지 기대하게 됩니다 이제 막 시작한 여정이지만 Mojo가 우리에게 가져다 줄 변화와 가능성을 기대해 봅니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165125&boardType=techBlog&searchData=&page=&subIndex=
[다시보기] 2월 우아한테크세미나: 우아한형제들의 RDS Aurora Graviton2 성능 이슈 해결 사례와 RDS를 모니터링하는 방법,itemname subname 2월 우아한테크세미나 다시보기 우아한형제들의 RDS Aurora Graviton2 성능 이슈 해결 사례와 RDS를 모니터링하는 방법 2월 세미나에서는 우아한형제들에서 Aurora에 Gravtion2 적용 시 write performance 이슈를 어떻게 해결했는지 알려드리고 우아한형제들에서는 어떻게 RDS를 모니터링하는지 설명해 드립니다 2월 28일화 저녁 7시 우아한테크 YouTube 채널에서 만나요 일정 2023 2 28화 저녁 7시 약 2시간 우아한테크 YouTube 채널 실시간 스트리밍 httpsyoutubecomwoowatech 신청 기간 2023 2 28화까지 신청하신 분께는 세미나가 끝난 뒤 발표 자료pdf를 보내 드립니다 주요 내용 1부 Amazon Aurora 아키텍처 소개 Amazon Intel 및 Graviton 아키텍처 소개 우아한형제들에서 Aurora에 Graviton2 적용 시 write performance 이슈 해결 사례 2부 우아한형제들에서 데이터베이스를 모니터링하는 방법 우아한형제들의 모니터링에 대한 생각 QA 추천 대상 완전 관리형 데이터베이스와 다른 데이터베이스의 차이점이 궁금한 분 이슈 해결 과정에 대한 인사이트를 얻고 싶은 분 Amazon Aurora가 궁금하신 분 데이터베이스MySQL PostgreSQL Aurora 모니터링이 궁금한 분 모니터링에 대한 인사이트를 얻고 싶은 분 강연자 AWS 이덕현 2005년 게임 시스템관리자로 시작해 2008년부터 2021년까지 DB 엔지니어로 살다가 2021년부터 AWS에서 Database Solutions Architect로 살아가고 있습니다 우아한형제들 오윤택 2009년 게임 프로그래머로 시작하여 2011년부터 DB 엔지니어로 살아가고 있습니다 우아한형제들 DevRel 공통 관심사를 가진 개발자들이 기술 교류 활동을 통해 함께 성장할 수 있기를 희망합니다 기술블로그 우아한테크세미나 우아한스터디 우아콘에서 자주 만나요 facebookcomwoowahanTech,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/10645/
"카카오 준신위 2차 회의…‘준법 시스템’, ‘신뢰∙상생’ 소위원회 신설",,,https://n.news.naver.com/mnews/article/366/0000960710?sid=105
"과기부, ‘CES K-메타버스’ 공동관 참가 기업 지원",,,https://n.news.naver.com/mnews/article/056/0011637718?sid=105
Mac에서 HAProxy 실행하기,수행과정 HAProxy를 MacOS에 설정하기 위해서 다음과정을 수행한다 brew를 이용하여 haproxy 설치하기 config 파일 작성하기 haproxy 실행하기 haproxy 모니터링 화면 확인하기 HAProxy 설치 macos에서 brew를 이용하여 haproxy를 설치하자 brew 설치 brew가 설치되어 있지 않다면 다음 페이지에서 brew를 우선 설치한다 HAProxy 설치 brew가 설치 되었다면 이제는 haproxy를 다음 커맨드로 설치한다 brew install haproxy Running brew update autoupdate Autoupdated Homebrew Updated 2 taps homebrewcore and homebrewcask 생략 To start haproxy now and restart at login brew services start haproxy Or if you dont wantneed a background service you can just run opthomebrewopthaproxybinhaproxy f opthomebrewetchaproxycfg Summary opthomebrewCellarhaproxy282 9 files 36MB 설치를 하고 나면 실행 방법이 간단하게 로그로 나타난다 brew servcies start haproxy 위 명령을 실행하면 백그라운드로 haproxy가 실행된다고 나온다 만약 설정파일을 함께 지정하고 직접 실행하고자 한다면 다음 커맨드를 이용하라고 설명하고 있다 즉 설정파일의 위치를 직접 지정하기 위해서 f 옵션을 사용하는 것을 확인하자 opthomebrewopthaproxybinhaproxy f opthomebrewetchaproxycfg 처음 설치하는 경우 위 경로에 haproxycfg 파일이 없다는 것을 알 수 있다 configcfg 파일 작성하기 이제 configcfg 파일을 작성해서 proxy 대상을 지정해보자 global daemon maxconn 40 defaults mode http timeout connect 5000ms timeout client 50000ms timeout server 50000ms frontend localhost bind 80 defaultbackend servers backend servers server server1 1270017121 server server2 1270017122 listen stats bind 9000 stats enable stats realm Haproxy Statistics stats uri haproxystats stats auth UsernamePassword 기본구조 global global settings here defaults defaults here frontend a frontend that accepts requests from clients backend servers that fulfill the requests global 은 전역 세팅을 수행하는 영역이다 defaults는 기본 프록시를 위한 설정 정보를 지정한다 frontend는 클라이언트가 접속하는 프런트엔드 포트와 각 설정을 수행할 수 있다 backend는 프런트엔드가 서버로 프록시하게 되며 클라이언트 요청을 처리하는 역할을 하는 서버 목록을 기술하게 된다 Global 영역 maxconn HAProxy로 접근할 클라이언트의 최대 연결 개수를 지정한다 이를 통해서 로드밸런서가 메모리 부족으로 다운되는 것을 방지할 수 있다 메모리 요구사항을 수용하기 위한 적절한 커넥션 수는 sizing guide를 참조하자 Defaults mode http 모드는 haproxy가 사용할 프로토콜을 지정한다 http를 지정하면 연결을 위해서 tcp연결을 수행하게 된다 timeout connect 백엔드 서버로 연결을 설정할 때 타임아웃을 지정한다 timeout client 클라이언트가 요청을 보낼 것으로 예상되는 시간이다 TCP세그먼트를 보내는 기간동안 활동이 없음을 측정한다 timeout server 백엔드 서버로 응답을 받기위해 대기하는 시간이다 frontend bind 주어진 ipport 로 클라이언트 요청을 listen한다는 것을 알려준다 ip주소를 로 두어 어떠한 값이 들어오든지 신경쓰지 않을 수 있다 defaultbackend defaultbackend는 거의 모든 프런트엔드에 있으며 usebackend 규칙이 트래픽을 다른곳으로 먼저 보내지 않으면 기본 백엔드로 프록시한다는 의미이다 usebackend나 defaultbackend 값이 설정되어 있지 않으면 503 Service Unavailable에러가 나게 된다 backend backend servers backend 의 이름을 servers로 지정했다 server server1 1270017121 server의 이름을 server1로 지정한다 server의 ipport를 지정하게 된다 이렇게 server를 작성하면 클라이언트 요청을 백엔드 서버 중 하나로 프록시 하게 된다 상세한 설정 정보를 확인하고자 하면 다음 메뉴얼을 참조하자 HAProxy 실행하기 이제 cfg파일을 작성했으니 haproxy를 직접 실행해보자 haproxy f cfg 파일경로 위 커맨드로 실행이 가능하다 실행시 이슈 해결 만약 실행했는데 Missing LF on last line file might have been truncated at position 3 과 같은 에러가 난다면 다음 커맨드를 수행해보자 echo haproxycfg port 80이 이미 떠있어서 충돌나는 경우 haproxycfg 파일 내부 바인드 포트를 바꿔주자 frontend localhost bind 81 포트를 81로 바꿔주었다 이제 httplocalhost81 로 접속하면 서버로 프록시 되는 것을 확인할 수 있다 HAProxy 모니터링 위 cfg 파일에서 haproxy를 위한 모니터링 설정을 했었다 listen stats bind 9000 stats enable stats realm Haproxy Statistics stats uri haproxystats stats auth UsernamePassword 바인드 포트는 9000번이다 통계정보를 활성화 하기 위해서 stats enable 라고 지정했다 통계 접속 uri는 haproxystats 이다 접속시 idpwd를 지정하기 위해서 auth 속성을 지정했다 통계 페이지에 대해서 상세정보는 다음을 참조하자 접속을 위해서 httplocalhost9000haproxystats 로 접근하고 IDPWD를 설정하면 다음 결과를 확인할 수 있다 WrapUp HAProxy를 설치하고 설정을 해 보았다 프런트엔드 포트를 81로 설정했으므로 httplocalhost81 로 접근하여 프록시를 수행할 수 있다 프록시 대상 서버는 7121 7122 포트이고 요청이 들어오면 해당 포트로 프록시 해주게 된다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165256&boardType=techBlog&searchData=&page=&subIndex=
"실제 서비스에 Canary 배포 적용 (feat ArgoCD, Istio, Kustomize)",들어가며 MSA 서비스를 운영하면 아무래도 신규 개발 및 배포가 자주 발생하게 됩니다 이때 전자문서 국민비서등 정부행정안전부의 서비스와 연동이 많은 우리 서비스 특성상 개발스테이징에서 QA가 불가능한 경우가 많다 정부의 개인보호 정책 때문에 개발스테이징에서는 테스트 불가한 제약이 많기 때문이다 그래서 운영환경에서 일부 테스트가 진행되어야 하는데 동시에 배포되는 Web 서비스의 특성상 운영 테스트가 힘든 경우가 많았다 이를 해결하기 위해 특정 IP테스터그룹만 접근 가능한 Canary 배포를 적용하였고 공유하고자 합니다 기본적으로 배포 전략은 지식이 있다는 가정으로 글을 작성하였습니다 필요하신 분은 아래 블로그 참고하세요 httpsreferencem1tistorycom211 이글은 Istio를 기반으로 구현하였기 때문에 관련글은 미리 참고하시면 좋습니다 Kustomize에 대한 기본 개념이 있어야 합니다 아래 이전글 참고 httpsdevoceanskcomsearchtechBoardDetaildoID163544 Canary 배포 구현 목표 최대한 자동으로 진행되고 Manual 작업은 없어야 한다 특정 IP만 Canary 배포 서비스Pod에 접근할 수 있어야 한다 위 두개를 목표로 수립하고 진행하였습니다 사실 Canary배포의 전략은 많이 있습니다 가장 흔히 사용되는건 Traffic에 weight를 주어서 신규 배포 서버로 조금씩 유입시키는 전략입니다 하지만 저희 서비스 특성상 신규 기능을 먼저 테스트 그룹이 QA를 진행하고 나머지 서버에 Rolling 업데이트 하는 방법을 이번에 설명 드릴 예정입니다 Test를 위한 환경 준비 시나리오는 위 그림과 같습니다 20111 이라는 IP를 가진 그룹만 Version2Green Pod로 network traffic이 흘러가고 기존 고객들을 Version1Blue Pod로 가는 것입니다 즉 Istio gateway에서 IP를 filtering 하여 pod version을 보고 routing 하는 방식입니다 1 Pod 2개 설치 아래와 같이 httpbin기존에 서비스 되고 있는 version1 Blue Pod를 가정과 httpbincanary신규 서비스 진행 예정인 version2 Green Pod를 가정 httpbin v1 Service Deployment apiVersion v1 kind ServiceAccount metadata name httpbin apiVersion v1 kind Service metadata name httpbin labels app httpbin service httpbin spec ports name http port 8000 targetPort 80 selector app httpbin apiVersion appsv1 kind Deployment metadata name httpbinmain labels app httpbin version v1 spec replicas 1 selector matchLabels app httpbin version v1 template metadata labels app httpbin version v1 spec serviceAccountName httpbin containers image dockeriokonghttpbin imagePullPolicy IfNotPresent name httpbinv1 ports containerPort 80 httpbin canary Deployment apiVersion appsv1 kind Deployment metadata name httpbincanary labels app httpbin version canarytagname spec replicas 1 selector label selector for the Pods matchLabels app httpbin version canarytagname template Pod template metadata labels app httpbin version canarytagname spec serviceAccountName httpbin containers image dockeriokonghttpbin010 imagePullPolicy IfNotPresent name httpbincanary ports containerPort 80 2 Virtual Service 생성 및 DestinationRule 생성 일단 Istio에서 지원하는 Virtual Service이하 VS 생성하겠습니다 이름은 testvs 입니다 gateway는 기존 저희 것을 그대로 사용하였습니다 그리고 VS에 match 를 이용하여 route rule 을 생성하였습니다 아래 sample은 httpsdomaincomhttpbin 으로 들어오는 traffic 중 XForwardedFor 에 특정 IP가 포함되어 있으면 특정 pod로 routing 되게 하였습니다 참고로 IP를 filtering 하는 규칙에 CIDR을 지원하지 않습니다 그래서 특정 대역폭 예를 들어 2032360016 을 사용하길 원하는 아래와 같이 prefix 203236 으로 사용하시면 됩니다 istio에서 사용 가능한 규칙은 공식 문서 httpsistioiolatestdocsreferenceconfignetworkingvirtualserviceHTTPMatchRequest 를 참고하세요 header를 포함하여 다양한 방법 적용이 가능합니다 Pod로 유입되는 Request의 Header 정보는 제 이전글을 참고하시면 됩니다 MWP TEST virtual service apiVersion networkingistioiov1beta1 kind VirtualService metadata name testvs spec hosts gateways mwpgateway http match uri prefix httpbin headers XForwardedFor prefix 203236 uri prefix httpbin headers XForwardedFor prefix 20111 rewrite uri route destination host httpbin port number 8000 subset canary match uri prefix httpbin uri prefix httpbin rewrite uri route destination host httpbin port number 8000 subset main 아래와 같이 DestinationRule을 설정합니다 기본적으로 VS에서 routedestinationhost에서 지정한 pod로 route 되는데 현재 deployment에서 같은 host name인 httpbin 을 사용하기 때문에 version 구분을 위해서 아래와 같이 subsets를 지정하여 관리합니다 즉 VS에서 routedestinationsubset이 존재하면 아래 version으로 Routing 됩니다 httpbin DestinationRule apiVersion networkingistioiov1beta1 kind DestinationRule metadata name httpbin spec host httpbin subsets name main labels version v1 name baseline labels version v1 name canary labels version canarytagname 3 최종 형상 만약 모든 배포가 제대로 되었다면 위 그림과 같이 testvsVirtual Service 아래에 httpbinService가 있고 그 아래 httpbin기존 code httpbincanary신규 code가 존재합니다 4 Test Curl 보내기 지난시간 Istio 방화벽 구축하기에서 배웠던 httpbin 전용 curl을 실행하고 Client IP에 따라 내가 원하는 Pod에 Log가 제대로 찍히는지 확인해 봅니다 curl location httpsdomincomhttpbingetshowenvtrue 실제 운영 서비스에 Canary 배포 적용 이번 구현의 최종 목표는 아래와 같습니다 최대한 자동으로 진행되고 Manual 작업은 없어야 한다 특정 IP만 Canary 배포 서비스에 접근할 수 있어야 한다 지금까지 설명한 부분은 사실 2번에 대한 내용이었습니다 1번 Canary 배포를 최대한 자동으로 배포할 수 있을지에 대한 내용은 각 서비스들의 운영환경마다 다르기에 지금 부터 소개할 방법은 저희 환경에 최적화된 내용입니다 막간을 이용한 저희 서비스 소개 PASS앱은 본인인증 외 많은 서비스가 있습니다 그중 전자문서가 저희가 운영중인 지금 설명하고 있는 모바일지갑 서비스입니다 해당 버튼을 누르면 이니셜 자격증명 행정안전부 전자문서지갑 행정안전부 국민비서 서비스등을 사용할 수 있습니다 위 서비스는 PASS앱에서만 접근이 가능하고 테스트 가능합니다 그렇기 때문에 실제 테스트도 PASS앱 위에서 진행합니다 특히 행안부 전자문서는 상용망에서 PASS인증서 기반으로 동작합니다 그렇기 때문에 많은 테스트가 상용망에서 이루어져야 합니다 위와 같이 mainfront 2개의 Pod는 실제 서비스중인 code이고 mainfrontcanary1개 pod는 앞서 설명 드린 방법을 이용하여 Deploy한 신규 배포할 code 입니다 1 Deployment Sample Code 지난번 소개 드렸지만 저희는 kustomize를 사용합니다 기본 배포 code는 Base folder에서 작성하고 각 환경별로 overlay folder를 이용하여 patch를 적용합니다 저희는 local develop stging prdmain 총 4개의 patch 환경을 가지고 있습니다 아래는 Base에 적용된 frontend manifest 입니다 Main Front apiVersion v1 kind Service metadata name mainfront spec ports port 8080 targetPort 80 protocol TCP name http selector app mainfront tier frontend apiVersion appsv1 kind Deployment metadata name mainfront labels app mainfront tier frontend spec selector matchLabels app mainfront template metadata labels app mainfront tier frontend version mainfront spec containers name mainfront image mwpmainfrontlatest env name ENVPROFILE valueFrom configMapKeyRef name platformconfig key envprofile ports containerPort 80 Main Front Canary Deployment Test apiVersion appsv1 kind Deployment metadata name mainfrontcanary labels app mainfrontcanary tier frontend spec selector matchLabels app mainfront version mainfrontcanary template metadata labels app mainfront tier frontend version mainfrontcanary spec containers name mainfront image mwpmainfrontlatest env name ENVPROFILE valueFrom configMapKeyRef name platformconfig key envprofile ports containerPort 80 overlaystaging에 적용된 patch code 입니다 pod 스케쥴링을 위한 affinity 및 container image를 가져오기 위한 AWS ECR 실제 주소가 포함되어 있습니다 Main Front apiVersion appsv1 kind Deployment metadata name mainfront spec replicas 2 template metadata annotations injectistioiotemplates sidecarcustom spec affinity podAntiAffinity preferredDuringSchedulingIgnoredDuringExecution weight 100 podAffinityTerm labelSelector matchExpressions key app operator In values mainfront topologyKey kubernetesiohostname containers name mainfront image xxxxxxxxxxdkrecrapnortheast2amazonawscommwpmainfrontv1 readinessProbe httpGet path health port 80 initialDelaySeconds 5 periodSeconds 20 livenessProbe httpGet path health port 80 initialDelaySeconds 10 periodSeconds 20 Main Front Canary Deployment Test apiVersion appsv1 kind Deployment metadata name mainfrontcanary spec replicas 1 template metadata annotations injectistioiotemplates sidecarcustom spec affinity podAntiAffinity preferredDuringSchedulingIgnoredDuringExecution weight 100 podAffinityTerm labelSelector matchExpressions key app operator In values mainfront topologyKey kubernetesiohostname containers name mainfront image xxxxxxxxxxdkrecrapnortheast2amazonawscommwpmainfrontv1 readinessProbe httpGet path health port 80 initialDelaySeconds 5 periodSeconds 20 livenessProbe httpGet path health port 80 initialDelaySeconds 10 periodSeconds 20 즉 아래 image 설정을 참고하여 ECR에서 image를 가져오는데 image xxxxxxxxxxdkrecrapnortheast2amazonawscommwpmainfrontv1 이때 image의 versiontag은 kustomize에서 관리하는 file에서 실제로 정보를 가져와서 배포합니다 images name xxxxxxxxxxdkrecrapnortheast2amazonawscommwpmainfront newTag 7330de851cb66dcf5633e98065c841c884c92a23 아래 ECR에 나와 있는 것 처럼 kustomizationyaml에는 항상 최신 Image tag가 기록되어 있습니다 2 Canary Pod 배포하기 모든 준비가 끝났습니다 실제 서비스를 담당하는 Deployment와 Canary Deployment 둘다 최신 code가 포함된 image를 바라보고 있습니다 하지만 아직 배포는 되지 않았습니다 상용 배포만큼은 운영자에 의해서 수동 배포 방식으로 정책을 정했기 때문입니다 위 그림과 같이 ArgoCD GUI에 들어가면 mainfront와 mainfrontcanary가 outofsync 상태입니다 자 우린 먼저 카나리새를 전장에 투입해야 합니다canary를 먼저 sync를 하면 Test들만 접근할 수 있는 pod가 생성되고 모든 역량을 다해 카나리에 TC를 쏟아 붓습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164747&boardType=techBlog&searchData=&page=&subIndex=
@codingapple,Bun 서버가 4배 빠르다는건 진짜인가 (vs Node.js),https://img.youtube.com/vi/a8uPDppckQk/0.jpg,https://www.youtube.com/watch?v=a8uPDppckQk
[딥러닝/AI 소식] 트위터 추천 알고리즘 오픈소스화 / Github 공개,트위터는 트윗 추천에 쓰이는 알고리즘 소스코드를 깃허브GitHub에 공개했다 트위터 추천 알고리즘 공개는 일론 머스크의 공약 사항이다 머스크는 지난해 트위터 인수 과정에서 모든 이용자가 수긍할 수 있고 이용자 스스로 추천 로직을 직접 활용할 수 있도록 소스 코드를 공개할 것이라고 밝힌 적이 있다 머스크는 트위터 추천 알고리즘 공개에 관해 초기 버전이 다소 부끄러울 수 있지만 우리는 실수를 빠르게 수정할 것이고 사용자 의견을 받아 개선할 수 있도록 희망한다고 밝혔다 다만 공개된 소스 코드는 트위터가 개인 피드에서 트윗을 어떻게 표시하는지에 대한 것 이며 서비스 전체에 적용되는 검색 알고리즘의 기본 코드나 트위터의 다른 부분에서 콘텐츠가 어떻게 표시되는지에 대한 기본 코드를 공개하지 않았다 아래 내용은 트위터 블로그의 내용을 일부 번역한 내용이다 How Do They Choose Tweets 하루 약 5억 개의 트윗을 추천 알고리즘을 통해 상위 트윗 몇 개를 선별해서 For You timeline 에 보이도록 구성되어 있다 트위터 추천 시스템은 트윗 사용자 및 참여 데이터engagement data에서 잠재 정보를 추출하는 핵심 모델과 기능 집합을 기반 으로 한다 이러한 모델들은 미래에 다른 사용자와 상호 작용할 확률은 얼마나 되는가 또는 트위터 상의 커뮤니티는 무엇이고 그 안에서 인기 있는 트윗은 무엇인가 와 같은 중요한 질문에 대답하는 것이 목적이다 이런 질문에 정확하게 대답한다면 트위터는 더 관련성 있는 추천을 제공할 수 있다고 볼 수 있겠다 추천 시스템은 세 가지 주요 단계로 파이프라인이 구성 되어 있고 Candidate Sourcing Rank Filtering 순으로 진행된다 candidate sourcing 각각 다른 추천 소스에서 최고의 트윗을 가져오는 후보 소싱 Rank 머신 러닝 모델을 통해 각 트윗 순위 매기기 Hueristics Filters 적용 차단한 사용자의 트윗 선정적인 콘텐츠 이미 본 트윗 등 필터링 전반적인 플로우 는 아래 다이어그램과 같다 Candidate Sourcing 후보 소싱 여러 후보 소스들로부터 최신의 관계성이 높은 트윗을 추출해서 베스트 1500 개의 트윗을 추출 한다 팔로우 하는 사람들InNetwork과 팔로우 하지 않는 사람들OutNetwork 로부터 후보를 찾는데 사용자 마다 조금씩 차이가 있을 수 있지만 평균적으로 추천 알고리즘은 InNetwork 50 OutNetwork 50 으로 구성되어 있다 InNetwork 소스 가장 큰 후보 소스인 InNetwork 소스는 팔로우 하는 사용자들의 가장 관련성이 높은 최신 트윗을 전달하려 한다 로지스틱 회귀 모델을 사용해서 팔로우 하는 사람들의 트윗을 관련성에 따라 효율적으로 순위를 매긴다 InNetwork 의 가장 중요한 구성 요소는 Real Graph 인데 Real Graph 는 두 사용자 간의 참여율을 예측하는 모델 이다 당신과 트윗의 저자 간 Real Graph 스코어가 높을수록 추천 알고리즘이 둘의 트윗을 더 많이 포함시킬 것 이다 OutNetwork 소스 사용자의 외부 네트워크에서 관련성 있는 트윗을 찾는 것은 더 까다로운 문제이다 Social Graph 첫 번째 접근법은 팔로우하는 사람들이나 비슷한 관심사를 가진 사람들의 참여를 분석함으로써 관련성 있는 것으로 생각할 만한 것들을 추정하는 방법이다 내가 팔로우하는 사람들이 최근 어떤 트윗들에 참여했는지 나와 비슷한 트윗들을 좋아하는 사람들은 누구이며 그들이 최근에 좋아했던 다른 것들은 무엇인지 위와 같은 질문에 대한 답변을 바탕으로 후보 트윗들을 생성하고 로지스틱 회귀 모델을 사용하여 결과 트윗들의 순위를 매긴다 이러한 유형의 그래프 순회는 외부 네트워크 추천에 필수적이며 사용자와 트윗 간의 실시간 상호 작용 그래프를 유지하는 그래프 처리 엔진인 GraphJet을 개발해 사용한다 Embedding Spaces 임베딩 공간 접근법은 내 관심사와 비슷한 트윗과 사용자가 무엇인지에 대한 일반적인 질문에 대한 답을 얻으려 한다 트위터의 가장 유용한 임베딩 공간 중 하나인 SimClusters 는 custom matrix factorization algorithm 을 사용해서 영향력 있는 사용자들의 클러스터에 기반한 커뮤니티를 찾아낸다 145k 개의 커뮤니티가 있으며 3주마다 업데이트 된다 사용자와 트윗은 커뮤니티 공간에서 표현되며 여러 커뮤니티에 속할 수 있다 아래 그림은 큰 커뮤니티 예시들이다 이러한 커뮤니티에 트윗을 포함시키는 방법은 각 커뮤니티에서 트윗의 현재 인기도를 살펴보는 건데 커뮤니티의 사용자들이 트윗을 좋아할수록 해당 트윗은 그 커뮤니티와 관련이 더 많이 된다 커뮤니티를 찾는 solution이 Sparse Binary Factorization 이라고 하는데 우선 실행 속도가 매우 빨라 500000 개의 커뮤니티를 발견하는 데 적합하다고 한다 Sparse Binary 라고 하면 0 or 1 로만 구성되어 있는 Matrix 라고 보면 되나 sparse 의 장점이 속도가 빠르다는 점이기 때문에 결국 위 이야기가 연결되는 걸까 Ranking 후보 소싱을 마친 이 시점의 파이프라인에는 약 1500 정도의 후보자가 있을 텐데 이 중 관련성을 직접 예측한다 랭킹은 약 4800만 개의 파라미터 신경망으로 트윗 상호관계를 학습한 후 긍정적 참여좋아요 리트윗 등 을 위한 최적화 라고 보면 된다 이 랭킹 메커니즘은 수천 개의 피처를 고려해 10개의 라벨을 output을 주어서 각 트윗에 점수를 부여한다 각 라벨은 참여 확률probabilty of engagement 을 나타내며 이 스코어를 가지고 순위를 매긴다 Hueristics Filters 랭킹 단계가 끝난 후 휴리스틱과 필터링을 적용해서 균형 있고 다양한 피드가 생성될 수 있게 한다 예를 몇 개 보면 쉽게 이해될 것 같다 차단한 사용자의 트윗은 보지 않게 하는 Visibility 필터링 같은 사람이 쓴 연속적인 트윗은 피하도록 하는 Author Diversity 기능 InNetwork 와 OutNetwork 트윗들이 균형있게 전달되는 지에 대한 콘텐츠 Balance Mixing 프로세스의 마지막 단계에서 시스템은 트윗과 non트윗광고 팔로우 추천 등 콘텐츠를 함께 섞어서 디바이스에 나타나도록 한다 이 파이프라인은 하루 약 50억 번 실행되며 평균 15초 이하로 완료된다 단일 파이프라인 실행에는 CPU 시간으로 220 초 정도 걸린다고 한다 Reference httpsblogtwittercomengineeringenustopicsopensource2023twitterrecommendationalgorithm httpswwwdigitaltodaycokrnewsarticleViewhtmlidxno473090 httpsgithubcomtwitterthealgorithm 원문 httpssooeun67githubiodata20sciencetwitteropensourced,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164696&boardType=techBlog&searchData=&page=&subIndex=
@codingapple,수학포기자들은 코딩하면 큰일남,https://img.youtube.com/vi/ZibkMbbDT5c/0.jpg,https://www.youtube.com/watch?v=ZibkMbbDT5c
[다시 보기] 10월 우아한테크세미나: 글 쓰는 우아한 개발자,itemname subname 10월 우아한테크세미나 글 쓰는 우아한 개발자 10월 우아한테크세미나 1부 다시보기 10월 우아한테크세미나 2부 다시보기 지식과 경험을 명확하게 전달하는 글쓰기 능력은 직무 역량을 키우고 또 가늠하는 핵심 요소가 되는 만큼 중요합니다 우아한형제들은 단순히 글을 잘 써라 지식을 유통해 선한 영향력을 확대해라의 구호에 그치지 않고 실제 글을 잘 쓰게 하기 위해 구성원들의 역량과 영향력 존재 가치를 끌어올리기 위한 문화를 꾸준히 만들고 있습니다 10월 우아한테크세미나에서는 글로 소통하는 문화를 가꿔 온 우아한형제들이 펴낸 두 권의 책을 소개하고 집필에 참여한 구성원과 함께 책 준비 과정 글쓰기의 어려움과 나만의 극복 방법 등을 들려 드립니다 일정 2023 10 25수 저녁 7시 약 90분간 우아한테크 YouTube 채널 실시간 스트리밍 httpsyoutubecomwoowatech 주요 내용 1부 요즘 우아한 개발 우아한형제들의 글쓰기 문화 책 목차 훑어보기 기술블로그에 꾸준히 경험과 지식을 쌓은 과정 글 쓰면서 성장하기 2부 우아한 타입스크립트 With 리액트 신입 웹프론트엔드 개발자가 책을 쓰기까지 책 목차 훑어보기 20명이 넘는 저자와 함께 책을 쓴다는 것 책을 쓰면서 발생한 에피소드 글 쓰면서 성장하기 참고 도서 우아한형제들 저 요즘 우아한 개발 골든래빗2023 추천 대상 기술 책을 써 보고 싶은 누구나 글로 명확하게 소통하는 것에 관심 있는 분 기술블로그에 꾸준히 글을 발행하고 싶은 분1부 타입스크립트 리액트에 관심이 있는 주니어 개발자2부 강연자 우아한형제들 김민태 100세 코딩을 실천하고 있는 중고 개발자 이제 51 달성 중 우아한형제들 최현준 코딩과 함께라면 어디든 갈 수 있는 집돌이 개발자 우아한형제들 송지은 우아한형제들에서 하고 싶은 거 다 해 보는 중인 주니어 개발자 우아한형제들 황윤서 코드의 가치와 생산성에 대한 고민을 하며 성장하는 주니어 개발자입니다 우아한형제들 박주희 문제를 해결하기 위해 항상 고민하는 TPM입니다 우아한형제들 이찬호 가치를 좇는 낭만 개발자입니다 우아한형제들 유영경 테크니컬 라이팅 코치로서 기술블로그를 관리하고 글을 검토하고 부담 없이 누구나 경험을 공유할 수 있는 문화를 만들어 가고 있습니다 우아한형제들 DevRel 공통 관심사를 가진 개발자들이 기술 교류 활동을 통해 함께 성장할 수 있기를 희망합니다 기술블로그 우아한테크세미나 우아한스터디 우아콘에서 자주 만나요 facebookcomwoowahanTech,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/14513/
OpenAI 의 음성인식 Whisper #1,ChatGPT로 떠들석한 가운데 작년 9월에 음성인식ASR Automatic Speech Recognition 모델인 Whisper를 공개 했습니다 httpsopenaicomblogwhisper httpsgithubcomopenaiwhisper 학습데이터를 무려 680000시간을 사용하여 학습한 모델을 공개하였습니다 사용된 데이터의 양을 보면 아래 그림과 같이 다양한 언어를 학습을 시켰는데요 그중에 특히 한국어가 약 8000시간 영어 제외 언어로는 7번째 많은 양의 데이터를 학습을 시켰습니다 아마 AIHubhttpsaihuborkr의 데이터를 사용하지 않았을까 추측되는데요 기존의 오픈 된 ASR 모델들은 영어 또는 영어권 위주의 데이터로 학습이 되어 있어서 국내에서 사용하기엔 다시 한국어로 모델을 학습해서 사용했었어야 하는데요 그런데 친절히도 OpenAI의 Whisper는 한국어 데이터셋을 사용하여 학습하여 한국어의 인식에도 뛰어난 성능을 보여줍니다 Whisper 논문의 저자를 보니 익숙한 한국인의 이름이 있는데요 공동 저자이신 김종욱님께서 Whisper개발에 기여를 해주셔서 한국어 데이터가 학습에 이용된 것 같습니다 김종욱님 감사합니다 Network 네트워크 모델자체는 Transformer를 사용해서 특별히 큰 차이점은 없어보이는데요 아래 그림과 같이 Multitasking Training으로 Transcription 과 Translation Task가 있습니다 잠깐 Translation도 된다구요 인식 뿐만 아니라 바로 번역까지 되고 Time Alignment도 됩니다 우와 Facebook wav2vec AWS Transcribe Google STT와 비교해도 인식률이 월들히 좋은 것을 확인 할 수 있습니다 특히 일상생활 대화부터 감정이 섞이 거나 소리지르듯이 발화하거나 모델명 처럼 속삭이는 소리로 잘 인식이 됩니다 특히 노래도 인식이 잘 되는 점이 타 모델들데이터셋의 차이과의 큰 차이점이 아닐까 싶습니다 다음 번에는 성능 분석 예제 코드인 실시간 인식 부터 번역 예제 파인튜닝까지 분석해보도록 하겠습니다 Whisper를 사용하면 실시간 인식 번역 가사동기 등 다양한 분야에 사용할 수 있을 것 같습니다 이제 우리아들 영어 유치원 안보내도 되겠어요,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164545&boardType=techBlog&searchData=&page=&subIndex=
@codingapple,html은 프로그래밍 언어 맞네요 역시,https://img.youtube.com/vi/vZ_oT0p113I/0.jpg,https://www.youtube.com/watch?v=vZ_oT0p113I
EdrawMind 이용한 테스트 케이스 설계,소프트웨어 개발에는 많은 단계가 필요합니다 기획자가 요구사항을 작성하고 디자이너가 UI GUI 를 작성하여 설계를 확정합니다 이렇게 확정된 스펙을 기반으로 개발자가 프로그래밍을 하는 동안 QA 에서는 테스트 케이스 설계 과정을 진행합니다 테스트 케이스 작성은 주로 아래 프로세스를 거칩니다 이 때 테스트 케이스 설계에 사용 되는 기법이 우리가 많이 아는 명세 기반 테스트 케이스 설계입니다 명세 기반 기법에는 여러 방법이 있습니다 등가분할Equivalence Partitioning 분류 트리 기법Classification Tree Method 경계값 분석Boundary Value Analysis 상태 전이 테스팅State Transition Testing 결정 테이블 테스팅Decision Table Testing 원인결과 분석CauseEffect Graphing 조합 테스트 기법Combinatorial Test Techniques 시나리오 테스팅Scenario Testing 오류 추정Error Guessing 이 중 분류 트리 기법에 사용 되는 마인드맵을 알아보려고 합니다 분류 트리 기법은 말 그대로 소프트웨어 스펙을 트리 나무처럼 구조화해서 가지치기 처럼 작성하는 기법입니다 분류 트리 기법 분류 트리 기법은 아래처럼 입력을 기준으로 분리하는 방법이 있습니다 입력과 관련된 분류를 만들고 입력을 조합해서 출력에 영향을 미치는 클래스를 결정합니다 그림에서 입력은 input A B C 이고 출력은 A1 23 B12 C123 입니다 이 조합은 18가지의 테스트 케이스 조건이 됩니다 테스트 케이스 작성할때 하나의 기법을 이용하여 전체 테스트 케이스에 적용하기 보다는 기능에 맞는 기법을 조합하여 사용합니다 예를 들면 분류 트리 기법으로 입출력 조건을 만들고 이 때 사용되는 테스트 데이터를 선택할때는 등가분할이나 경계값 분석을 이용합니다 시나리오 테스트을 할때도 분류트리 기법이나 상태 전이 테스팅에서 도출된 테스트 케이스에서 사용자가 행동 패턴이 빈번하게 발생되는 케이스를 추출하여 사용합니다 마인드맵 EdrawMind 마인드맵은 일반적으로 아이디어를 도출할 때 사용됩니다 테스트 케이스 설계할 때는 스펙을 한눈에 정리하고 테스트 케이스 도출에 누락이 없도록 설계하기 위해 사용됩니다 여러 툴이 있지만 EdrawMind 의 사용방법을 알아보겠습니다 입력 방법은 엔터키와 텝키 스페이스 키만 입력하면 가지를 만들 수 있습니다 참고로 무료 설치본을 사용하면 100개 가지까지 사용할 수 있습니다 테스트 케이스 설계 팁 설계 팁 1 중분류 사용 중분류를 이용하면 큰 카테고리에서 누락된 부분이 없는지 확인할 수 있습니다 중분류는 테스트 대상 소프트웨어마다 다르겠지만 UI가 있는 경우 저는 테스트 케이스 작성할 때 UI 기능과 동작을 나눠서 작성합니다 마인드 맵에서 왼쪽와 오른쪽으로 분류하여 UI 기능 검증과 실행 검증으로 나눠서 설계합니다 zephyr를 이용하여 테스트 케이스 작성할때는 중분류를 story 유형 이슈로 만들어서 테스트 케이스를 링크로 연결합니다 이렇게 구분하면 중분류는 큰 기능이기 때문에 관련된 테스트 케이스를 한번에 확인할 수 있습니다 story 제목을 예를 들면 위에 마인드맵에서 11 OS 관련 기능 가 제목이 됩니다 추후에 관련된 기능이 늘어나면 동일한 스토리에 테스트 케이스를 추가하거나 새로운 스토리를 추가하여 관리합니다 설계 팁 2 번호를 붙임 번호를 이용하여 마인드맵을 작성하면 설계하는 사람과 구현하는 사람이 다를 경우 설계를 이해하는데 더 도움이 됩니다 테스트 케이스 실행할 때나 결과 확인할 때도 번호를 통해 어떤 이슈가 있는지 바로 알 수 있는 장점이 있습니다 위 예시 처럼 초록색 가운데 박스가 소프트웨어 대 기능이면 2번째 분류 번로 적힌 1114 는 중분류 기능입니다 11 12 는 ui 기능에 대한 분류이고 13 14는 실행에 대한 분류입니다 111이 테스트 케이스 하나가 됩니다 각 테스트 케이스 다음 가지는 테스트 케이스 스텝으로 작성합니다 zephyr를 이용하여 테스트 케이스 작성할때는 제목 부분에 번호를 함께 작성합니다 예를 들면 11 OS 관련 기능 스토리 하위에 111 spec1 테스트 케이스가 작성됩니다 참고 Zephyr를 이용한 테스트 관리 httpsdevoceanskcomblogtechBoardDetaildoID164332 출처 httpswwwosskrinfotestshowafda9e9d3be7471a9c559e2c3ac58221 httpjournalksaeorgxml1805018050pdf httpswwwedrawsoftcomkradedrawmindbrandhtmlgclidCjwKCAjwgqejBhBAEiwAuWHioHeXegU1GekdZTbjpVdgiDL2mKgCX1LbXMXQcg009ejuAgHNILAP7hoCz4QAvDBwE,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164935&boardType=techBlog&searchData=&page=&subIndex=
"과기정통부, CES에 'K-메타버스 공동관' 운영…국내기업 수출지원",,,https://n.news.naver.com/mnews/article/008/0004983247?sid=105
"오픈소스 웹 테스트, 자동화 라이브러리 Playwright 소개",Playwright는 오픈소스 웹 테스트 자동화 라이브러리로 MS에서 제작했습니다 Playwright 장점 하나의 API로 Chromium Firefox WebKit까지 테스트 할 수 있습니다 관련 툴 제공 Codegen 작업을 기록하여 테스트를 생성 어떤 언어로든 저장 httpsplaywrightdevdocscodegen Playwright inspector 페이지를 검사하고 선택기를 생성하고 테스트 실행을 단계별로 진행하고 클릭 지점을 확인하고 실행 로그를 탐색합니다 Trace Viewer 테스트 실패를 조사하기 위해 모든 정보를 캡처합니다 극작가 추적에는 테스트 실행 스크린캐스트 라이브 DOM 스냅샷 액션 탐색기 테스트 소스 등이 포함됩니다 Playwright Test for VSCode VSCode 에 plugin 을 설치하면 실행과 디버깅이 유리합니다 1설치 터미널에서 npm init playwrightlatest 으로 설치합니다 TypeScript 옵션으로 설치한 예시입니다 playwright npm init playwrightlatest next Getting started with writing endtoend tests with Playwright Initializing project in Do you want to use TypeScript or JavaScript TypeScript Where to put your endtoend tests tests Add a GitHub Actions workflow yN true Install Playwright browsers can be done manually via npx playwright install Yn true Initializing NPM project npm init y Wrote to UsersDocumentsplaywrightpackagejson name playwright version 100 description main indexjs directories test test scripts test echo Error no test specified exit 1 keywords author license ISC Installing Playwright Test npm install savedev playwrighttestnext added 4 packages and audited 5 packages in 500ms found 0 vulnerabilities Downloading browsers npx playwright install Writing playwrightconfigts Writing githubworkflowsplaywrightyml Writing testsexamplespects Writing testsexamplesdemotodoappspects Writing packagejson Success Created a Playwright Test project at Users1112469Documentsplaywright Inside that directory you can run several commands npx playwright test Runs the endtoend tests npx playwright test ui Starts the interactive UI mode npx playwright test projectchromium Runs the tests only on Desktop Chrome npx playwright test example Runs the tests in a specific file npx playwright test debug Runs the tests in debug mode npx playwright codegen Auto generate tests with Codegen We suggest that you begin by typing npx playwright test And check out the following files testsexamplespects Example endtoend test testsexamplesdemotodoappspects Demo Todo App endtoend tests playwrightconfigts Playwright Test configuration Visit httpsplaywrightdevdocsintro for more information Happy hacking playwright 설명에는 실행하는 방법 등이 나옵니다 VSCode로 설치 폴더를 열었습니다 playwrightconfigts packagejson packagelockjson tests examplespects testsexamples demotodoappspects 자동으로 생성된 테스트 코드는 아래와 같습니다 tests examplespects 타이틀이 있는지 보는 케이스와 start link를 클릭하는 2개 테스트 케이스입니다 import test expect from playwrighttest testhas title async page await pagegotohttpsplaywrightdev Expect a title to contain a substring await expectpagetoHaveTitlePlaywright testget started link async page await pagegotohttpsplaywrightdev Click the get started link await pagegetByRolelink name Get started click Expects the URL to contain intro await expectpagetoHaveURLintro 2 실행 ui를 이용한 실행 아래 명령어를 실행하면 실행UI 가 뜹니다 원하는 테스트 케이스를 실행하면 결과를 화면으로 볼 수 있습니다 npx playwright test ui 실행하고 브라우저가 열리는 시간까지 확인할 수 있습니다 터미널에서 실행 playwright npx playwright test Running 6 tests using 5 workers 6 passed 47s To open last HTML report run npx playwright showreport playwright npx playwright showreport Serving HTML report at httplocalhost9323 Press CtrlC to quit 기본적으로 테스트는 3개의 작업자를 사용하여 3개의 브라우저 크롬 파이어폭스 및 웹킷에서 모두 실행됩니다 이것은 playwrightconfig 파일에서 구성할 수 있습니다 테스트는 헤드리스 모드에서 실행되므로 테스트를 실행할 때 브라우저가 열리지 않습니다 테스트 결과 및 테스트 로그가 터미널에 표시됩니다 터미널에서 npx playwright showreport 명령어를 수행하면 아래와 같이 브라우저마다 실행한 결과가 웹페이지로 나옵니다 3 VSCode 실행옵션 테스트 케이스를 에이닷 웹 페이지로 바꿔서 검증을 해 보겠습니다 타이틀에 에이닷이 있는지 검증하는 케이스입니다 testhas title async page await pagegotohttpsasktelecomcom Expect a title to contain a substring await expectpagetoHaveTitle내 손안의 AI 친구 에이닷 VSCode 에서는 브라우저에서 실행하는 옵션과 트레이스 뷰어에서 실행하는 옵션을 선택할 수 있습니다 브라우저 선택 그리고 브라우저를 선택적으로 실행할수도 있습니다 pick locator 데모에서 웹페이지만 에이닷으로 변경한 테스트 케이스입니다 당연히 에러가 발생합니다 이 때 pick locator를 이용하여 엘리먼트 값을 알아보겠습니다 testget started link async page await pagegotohttpsasktelecomcom Click the get started link await pagegetByRolelink name Get started click Expects the URL to contain intro await expectpagetoHaveURLintro 테스트 케이스를 실행합니다 에러가 난 상황에서 pick locator를 선택하고 브라우저를 클릭합니다 확인하고자 하는 웹 페이지 엘리멘트를 클락하여 정보를 확인합니다 link name이 구글 플레이에서 다운로드 라는 것을 알 수 있습니다 코드의 name 을 변경하고 첫번째 데모에서 확인했던 타이틀 확인하는 코드를 사용하여 다시 실행해 봅니다 testget started link async page await pagegotohttpsasktelecomcom Click the get started link await pagegetByRolelink name 구글 플레이에서 다운로드 click Expects the name to contain intro await expectpagetoHaveTitleA에이닷 AI 대화 Open Beta Google Play 앱 실행하면서 에러가 발생할 경우 기대 결과와 실제 결과를 보여주는 기능이 유용합니다 한장으로 보는 playwright 참고해보세요 참고 httpsplaywrightdev httpsplaywrightdevdocsintro,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165090&boardType=techBlog&searchData=&page=&subIndex=
구름과 카카오가 함께한 다섯 번째 구름톤 후기 (1편),카카오는 자체 기술 행사뿐 아니라 기술 생태계에서 일어나는 다양한 행사를 지원하고 후원하고 있습니다 그중에서도 구름goorm과 카카오가 함께하는 구름톤9oormthon의 이야기를 들려드리려 합니다 구름톤은 카카오 클라우드 플랫폼의 이름인 9rum과 구름의 영문명 goorm Hackathon의 합성어로 두 구름이 만나 열리는 해커톤입니다 2022년부터 시작하여 이번 3월에는 5회 차를 맞이했어요 구름톤 행사에 대한 자세한 정보는 구름톤 홈페이지에서 확인하실 수 있습니다 https9oormthongoormio카카오는 구름톤에 카카오의 클라우드 인프라 현업 개발자들의 다양한 참여 장소와 기념품 제공 등으로 함께하고 있습니다 참가자분들의 즐거움과 성장 그리고 좀 더 나아가서는 기술 생태계의 발전에도 보탬이 되면 좋겠습니다제주도에서 3박 4일간 진행된 구름톤은 일상에서 벗어나 해커톤의 과정에 온전히 몰입하고 성장할 수 있도록 강의 멘토링 네트워킹 심사 등으로 알차게 구성되어 있어요 그 곳곳에는 다양한 모습으로 함께한 카카오 크루들이 있었습니다 전체 여정 중에서 이 글에서는 2023년 3월 구름톤의 첫날을 강의로 함께 열었던 카카오의 크루들을 만난 이야기를 공유해보려 합니다 크루들은 다음과 같은 강의 세션을 준비했어요카카오 클라우드 소개 dennis쿠버네티스 이론 및 실습 교육 dennis eloy오픈소스API로 신나는 해커톤 hunter중요한 건 꺾이지 않는 소통 robin 카카오 클라우드 소개와 쿠버네티스 이론 및 실습 교육 세션은 카카오 클라우드를 만들고 운영하시는 데니스와 엘로이가 준비해 주셨어요 카카오의 클라우드 컨테이너 플랫폼을 개발하는 크루들이 매번 이 강의를 맡아주셨는데요 이번에는 리더 데니스와 막내 엘로이가 함께 왔습니다데니스는 클라우드 네이티브 애플리케이션 개발은 무엇이고 왜 필요한지 소개하고 이를 위한 필수 도구인 도커와 쿠버네티스의 동작 원리 사용 방법도 알려주셨어요 엘로이는 클라우드 해커톤인 구름톤에서 참가자분들이 앱을 개발하고 실제로 클러스터에 올릴 수 있도록 가이드를 만들어 실습 세션을 진행했어요두 분은 멘토로도 활동하며 실제 개발 기간 동안 참가자분들의 궁금증을 해소해 드리고 카카오의 개발문화를 간접적으로 보여주시기도 했습니다 데니스와 엘로이에게 몇 가지 질문을 드렸어요강의를 통해 참가자분들이 무엇을 배워갔으면 하셨나요데니스모던 애플리케이션 개발환경에서 필수적인 클라우드 네이티브 애플리케이션 개발의 경험을 제공해드리고자 했어요 이 경험을 통해서 나중에 현업에 가셨을 때 컨테이너 기반의 개발을 도입해 보자고 제안하고 개발 방식에 변화를 이끌 수 있는 분들이 되셨으면 좋겠다는 바람입니다 기술적으로 깊게 다루기엔 짧은 시간이지만 핸즈온을 통해서 실행을 할 수 있는 수준까지는 도와드리려고 합니다엘로이저는 실습 중심의 강의를 했기 때문에 참가자분들이 이론적인 내용을 다 이해하진 못했더라도 전체 과정을 따라가 보면서 어떤 흐름인지 감을 잡을 수 있으셨으면 했어요 이론은 조각모음을 하듯이 조금씩만 이해를 하더라도 나중에 관련된 공부를 하게 되실 때 조금 더 와닿지 않을까 했습니다 카카오의 현업 개발자와 함께 클라우드를 사용해 보는 경험은 어떤 점이 좋은가요데니스클라우드 기술이 등장한 지 꽤 되었음에도 불구하고 막상 교육을 받을 수 있는 기회는 흔치 않은 것 같아요 하지만 현업에서는 클라우드를 기본적인 인프라로 사용하고 있어요저희는 카카오클라우드 스쿨 카카오테크캠퍼스와 같은 프로그램을 통해서도 클라우드 교육 활동을 하고 있는데요 수강생분들이 쿠버네티스나 도커 등에 대한 지식과 경험을 바탕으로 취업을 잘하시는 사례가 있었어요 구름톤도 마찬가지로 클라우드 기반으로 애플리케이션을 배포하고 운영해 볼 수 있는 기회가 흔치 않은 만큼 현직자들과의 만남을 통해 실무에서 활용되는 방식을 경험을 해보실 수 있다는 점이 의미가 있다고 생각해요 이번 경험에 이어서 스스로 조금 더 학습하시면 취업활동에도 큰 도움이 되지 않을까 생각합니다 멘토로서 알려주고 싶었던 것이 있었나요 실제로 멘티들은 어떤 부분에서 도움을 받았나요엘로이참가자 분들 중 취업에 관심 있는 분들이 많았기 때문에 자신이 원하는 회사를 선택하는 방법이나 저의 경우 목표하는 회사에 가기 위해 준비했던 과정들을 알려주고 싶었는데요 멘티분들도 공감해 주시고 여러 생각을 나눴어요 그리고 대부분의 참가자분들이 실제로 쿠버네티스를 사용해 본 경험이 없으셔서 조금이나마 저의 경험을 알려드리고 싶었고 멘티분들도 해커톤을 진행하면서 애플리케이션을 배포해보고자 하는 의지가 많이 있었습니다 헌터는 오픈소스API로 신나는 해커톤 세션에서 구름톤을 어떤 자세로 임하면 좋을지 다년간의 경험을 바탕으로 노하우를 전해주셨습니다 헌터는 구름톤 1기부터 지금까지 강의자와 심사위원으로 항상 함께하며 남다른 애정을 가지고 있습니다헌터는 짧은 구름톤 기간 동안에 프로젝트를 성공적으로 구현하기 위해서 최소 기능 제품MVP 관점에서 핵심 기능에 집중하고 거인의 어깨 위에서 세상을 보자는 모토로 오픈소스 오픈 API 다양한 도구를 적극적으로 사용하길 추천했어요 그리고 팀 내에서의 협업뿐 아니라 다른 팀의 모든 참가자들과도 협업하고 좋은 추억을 남기는 것이 중요하다는 메시지와 함께 협동과 성장의 마인드셋을 알려주셨습니다 마지막으로 이번 해커톤이 3박 4일에서 끝나지 않고 이후에도 좋은 프로덕트와 좋은 인적 네트워크로 발전할 수 있으면 좋겠다는 이야기도 전했어요 헌터도 잠시 만나보아요구름톤의 어떤 부분이 성장을 위한 좋은 경험이 될까요헌터회사에서는 주니어의 연차에서 할 수 있는 일들이 어느 정도 한정되어 있어요 하지만 이 분들이 구름톤에 모여서 프로젝트의 처음과 끝까지 경험하며 만들어내는 결과물을 보면 역량은 연차와 관계없고 역량을 발휘할 수 있는 환경이 정말 중요하다는 생각을 하게 되어요해커톤은 이렇게 첫날의 교육 프로그램도 중요한 한 부분이지만 이 이후로 팀 빌딩을 하고 아이디어를 구체화시키는 과정이 기다리고 있어요 단순히 코딩만 하는 것이 아니고 서로 적절하게 역할을 나누어 집중하고 소통하면서 또 하나의 프로젝트로 통합하는 일련의 과정 자체를 경험하는 것이 중요하다고 생각해요좋은 서비스나 제품을 만든다는 것은 다양한 관점에서 함께 고민하고 풀어가야 하는 문제인데 큰 조직에서 일을 하다 보면 자신에게 부여된 일에 집중하느라 나의 업무의 배경 목표나 전체적인 그림을 놓칠 수 있어요 구름톤은 그 전체적인 과정을 바라볼 수 있는 좋은 경험이 될 거예요 함께 좋은 추억을 만들라는 격려를 많이 해주신 것 같아요헌터구름톤은 단지 아이디어를 내고 결과물을 만드는 해커톤의 과정뿐 아니라 이렇게 강의 프로그램도 있고 멘토링도 진행하면서 참가자들이 함께 지식적으로 배울 수 있는 시간이 마련되어 있고요 비어파티와 같이 네트워킹을 위한 프로그램도 잘 구성되어서 자신의 팀원이 아닌 다른 참가자들끼리도 알아갈 수 있어요 또 제주에서 진행되는 만큼 간단히 주변으로 산책을 함께 다녀오는 것도 좋은 리프레시와 추억이 될 수 있을 것 같아요 헌터의 메시지가 잘 전달되었는지 구름톤 일정을 모두 마치고 엘로이가 이런 얘기를 했어요엘로이어쨌든 구름톤도 해커톤이라 서로 경쟁하는 자리인데 다른 팀의 멤버들끼리도 기술을 공유하면서 서로 협동하며 즐기는 모습들이 인상적이었어요 심사위원으로서 헌터의 이야기는 후기 2편에서 더 들려드릴게요 로빈은 중요한 건 꺾이지 않는 소통이라는 제목으로 1부에서는 기획자디자이너개발자 간의 소통을 2부에서는 사람과 기계간의 소통 즉 AI와의 소통과 활용에 대한 이야기를 해주셨습니다 서로의 역할과 언어가 다름을 이해하고 이를 바탕으로 소통할 수 있기를 바라는 로빈의 마음이 있었습니다 강의의 끝에 참가자 한 분이 로빈에게 이런 질문을 했어요우리가 AI의 도움을 받지 않아야 한다고 생각하시는 곳이 있나요이에 대한 로빈의 답변입니다로빈한 교사가 실험을 한 것을 본 적이 있어요 학생들이 과제를 ChatGPT로 대신할 것을 걱정한 교사였는데요 Chat GPT가 작성한 글과 학생들이 쓴 글을 비교하고 점수를 매기는 실험을 했어요 이때 가장 다른 부분이 감정이었다고 해요 앞으로 AI를 활용한 도구가 더 보편화되면 더 쉽게 AI의 도움을 받을 수 있겠지만 우리는 여전히 감정이 담긴 편지를 쓰고 인사를 전하고 진심을 담은 말과 글로 소통해야 할 텐데 그런 연습을 할 기회를 AI에게 뺏기는 것이 아닐까 했어요 자신의 능력을 먼저 키워야 하는 데에는 AI가 사용되면 안 된다고 생각합니다코딩테스트도 마찬가지예요 최근에는 AI가 코딩테스트를 대신 풀어줄 수 있을 만큼 발전했지만 개발자는 여전히 코딩테스트가 요구하는 역량을 준비해야 해요 기업에서 필요한 사고방식과 문제 해결 역량을 보는 테스트이기 때문에 나중에는 다른 방식으로 그 역량을 확인하게 될 거예요 그래서 자신의 역량을 키우는 일에는 AI를 활용만 하되 의존하면 안 된다고 생각합니다 그리고 로빈은 아직 참가자들이 익숙하지 않은 클라우드 환경에서 짧은 시간 진행하는 구름톤은 Chat GPT에 코드를 의존하면 잘못된 응답hallucination 등을 받았을 때 알아차리기 어려워서 오히려 어려움이 생길 수도 있기 때문에 적절한 활용이 중요하다고 짚어주셨어요저도 로빈에게 질문을 드렸어요서로 다른 직무 간의 소통에 대한 강의는 어떻게 준비하게 되셨나요로빈저도 개발자로서 협업과 소통에 대해 여러 고민들이 있어서 그런 부분을 녹여내고 싶었어요 기획자 디자이너 개발자 세 직군이 일상적으로 사용하는 언어에 차이가 있음을 인지하는 것이 서로에 대한 이해에서 가장 중요하다고 생각해요 같은 용어라도 다른 뜻으로 통용되기도 하기 때문에 각 직군별로 사용하는 여러 용어들을 설명해드리려 했어요 예를 들어 기획에서 자주 쓰이는 페르소나persona 같은 경우 저는 예전에 사내에서 해커톤을 하면서 처음 알게 됐거든요한 편으로는 각자가 서로의 역할에 대해 오해가 있다고 생각했어요 개발자는 개발만 잘하고 디자이너는 예쁜 것만 만들려고 생각한다거나 그런 오해요 함께 좋은 서비스를 만들기 위해 다양한 시각에서 고민할 수 있도록 직군 간의 선입견을 여러 예시들로 풀어드리고 공감대를 형성하고 싶었습니다 구름톤 5기의 첫째 날은 이렇게 간단히 소개드려봅니다 카카오 현업 개발자들의 지식과 경험을 흡수한 참가자분들 앞에는 이제 본격적으로 아이디어 피칭 팀빌딩 멘토링 그리고 밤샘을 거쳐 최종 결과물이 나오는 과정이 기다리고 있는데요 이미 구름톤에 심사위원으로 참여한 경험이 있는 헌터와 로빈은 대회 전부터 특별히 구름톤 참가자들이 만들어내는 결실에 대해 칭찬을 아끼지 않으셨어요 이번에도 고퀄리티의 결과물을 마주한 5기 심사위원들의 이야기 다음 후기에서 헌터가 직접 들려드릴게요 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/04/14/9oormthon-with-kakao-1/
@ITSUB,20만원대 OLED 탑재로 난리난 태블릿 용팡이? '이것' 하나 때문에 비추하는 이유..,https://img.youtube.com/vi/TpWq9XqSm5g/0.jpg,https://www.youtube.com/watch?v=TpWq9XqSm5g
경영쇄신 나선 엔씨…김택헌·윤송이 C레벨 뗀다,,,https://n.news.naver.com/mnews/article/003/0012307107?sid=105
GraphQL에 대하여 (Resolver ＆ DataLoader),GraphQL에 대하여 Interface Union Type에 이어서 Resolver와 DataLoader에 대해 알아보고자 한다 Resolver GraphQL에서 Resolver는 쿼리의 각 필드에 대한 데이터를 제공하는 함수이다 Resolver는 GraphQL 서버에서 데이터를 가져오는 데 사용되며 클라이언트가 요청한 필드의 값을 반환한다 Resolver는 GraphQL Schema의 field와 연결된다 각 field에는 해당 필드의 값을 검색하고 반환하는 Resolver 함수가 있어야 한다 기본 함수는 받은 값을 그대로 return하는 함수이다 Resolver 함수는 데이터베이스 외부 API 호출 메모리 캐시 또는 다른 소스로부터 데이터를 가져올 수 있다 GraphQL 스키마는 타입과 필드의 구조를 정의하는데 이를 기반으로 Resolver는 어떤 데이터를 반환해야 하는지 결정한다 Resolver는 필드의 값을 동적으로 계산하거나 적절한 데이터 소스로부터 데이터를 검색하여 반환할 수 있다 글로만 작성하면 이해가 어려울 수 있으니 간단히 예를 들어 보겠다 지난 글에서 사용한 SellerUser Schema를 사용하도록 하겠다 우선 아래와 같이 allusers를 호출한다고 가정해보자 query allusers name on SellerUser stuff id name total bizInfo biznum bizimg field로는 name stuff stuffid stuffname stufftotal bizInfo bizInfobiznum bizInfobizimg 총 8개가 있고 8번의 resolver가 호출된다 따로 함수를 작성하지 않으면 그냥 field의 값을 그대로 return한다 그럼 이 resolver는 왜 사용해야 할까 type SellerUser implements UserInterface uid Int email String name String phone String gender gender birthdate Time createdat Time updatedat Time bizInfo BizInfo stuff Stuff 만약 RDB를 사용했다고 가정하면 여기서 UserInterface에서 가져오는 field들은 하나의 table에 저장되어 있을 가능성이 높다 즉 아래의 field들은 data join 없이도 하나의 table에서 가져올 수 있다 uid Int email String name String phone String gender gender birthdate Time createdat Time updatedat Time 하지만 bizInfo와 stuff는 다른 table에 저장되어 있을 가능성이 높다 이를 가져오기 위해서는 table을 join해서 가져와야 한다 SellerUser를 조회할 때마다 bizInfo stuff 정보를 join해서 가져오면 맨 처음에 말했던 graphQL의 장점인 overfetching을 하지 않는다를 지킬 수 없게 된다 이 때 resolver를 통해 사용자가 field를 요청할 때만 해당 data를 조회해 오도록 할 수 있다 만약 그렇게 변경한다면 위의 sellerUser에서는 field마다 resolver가 아래와 같이 동작할 것이다 func r sellerUserResolver uidctx contextContext obj modelSellerUser string error return objUid error bizInfo stuff를 제외하고는 생략되어 있으며 resolver에서 그냥 자신의 값을 return한다 func r sellerUserResolver BizInfoctx contextContext obj modelSellerUser modelBizInfo error bizInfo table 조회 func r sellerUserResolver Stuffctx contextContext obj modelSellerUser modelStuff error Stuff table 조회 이렇게 변경하면 overfetching은 막을 수 있지만 user조회 시 DB를 총 3번 조회해야 하는 문제가 생길 수 있다 따라서 전략적으로 잘 활용해야 한다 DataLoader Resolver를 사용할 때 N1이라는 문제가 발생하게 된다 N1 문제란 요청은 한번만 했지만 Resolver에 의해 N번의 트랜젝션이 발생할 수 있는 것이다 예를 들면 user bizInfo stuff 이 3개의 table이 있다고 가정해보자 이 상황에서 alluser query를 호출하면 User정보를 위해 DB 조회 1회 user가 N명 있다고 가정하면 user 별 bizInfo조회를 위한 DB 조회 N회 user별 Stuff조회를 N회 해야 한다 아래와 같이 1회의 query요청으로 1NN회의 조회를 해야 한다 SellerUser data uid 1 name a bizInfo resolver 실행 1회 stuff resolver 실행 1회 uid 2 name b bizInfo resolver 실행 1회 stuff resolver 실행 1회 uid N name c bizInfo resolver 실행 1회 stuff resolver 실행 1회 이는 N이 커지면 커질수록 DB에 과부화를 준다 이 때문에 이를 해결하기 위해 DataLoader가 개발되었다 위의 예시에서 dataLoader는 user의 uid를 key 값으로 사용하고 이 uid를 배열로 모아서 각각 1번의 트랜잭션 Or 연산으로 bizInfo와 stuff를 찾을 것이다 DataLoader의 동작 방식은 아주 간단하고 아래와 같다 dataloader github Batch func을 선언한다 middleware에서 context에 해당 batch func을 삽입한다 필요한 resolver에서 dataLoader의 값을 Load해온다 이 때 주의할 점은 dataLoader에 요청한 키 값의 길이와 데이터를 조회한 결과 값의 길이는 반드시 같아야 한다 즉 위에 예시를 보면 조회 시 keys는 uid의 배열 12n으로 길이가 n인 배열일 것이다 실제로 db에 조회를 하면 SELECT FROM bizInfos WHERE userid ANY12n 과 같이 조회를 할 것이다 이 때 bizInfo 값이 존재하지 않아 null 이더라도 bizInfo1 null bizInfoN 과 같이 null로 배열을 채워 길이를 맞춰야 한다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164933&boardType=techBlog&searchData=&page=&subIndex=
"티빙, 네이버·스포티비 제치고 프로야구 중계권 따냈다",,,https://n.news.naver.com/mnews/article/417/0000974143?sid=105
"티빙, 2026년까지 프로야구 유무선 중계권 확보",,,https://n.news.naver.com/mnews/article/003/0012307280?sid=105
MySQL에서 JSON 데이터 다루기,서론 ifland 서비스에는 다양한 게시글이 올라옵니다 인스타그램과 유사한데 여러 장의 사진이나 동영상이 대부분이고 간혹 텍스트로만 이루어진 게시글도 있습니다 일반적인 SNS의 포스팅 서비스 혹은 게시글 서비스에는 MySQL에서 전혀 문제가 없습니다 일반적인 방식으로 테이블을 설계하면 되죠 비정형데이터 문제는 영상이나 사진과 함께 올라오는 비정형 데이터에서 특정 정보를 해시태그로 뽑아내고 그것으로 검색을 하는 요건에서 발생을 했습니다 예를들면 ifland 앱 안에서 노래방을 즐길 수 있고 노래 영상을 바로 포스팅 할 수도 있는데요 이 영상에는 해당 노래의 가수나 노래 영상에 사용된 특수 효과 등이 부가 정보로 같이 들어옵니다 이 비정형 데이터에서 걸그룹 IVE아이브의 정보를 저장하고 조회한다면 유저가 IVE 혹은 아이브와 같은식으로 아이브의 노래를 부른 노래방 영상을 모두 찾을 수 있는 것이죠 어 그럼 DB Table에 노래제목 가수 등의 컬럼을 만들거나 별도 테이블에서 관리하고 그것을 join하면 되지 않나요 다양한 비정형 데이터가 존재 노래방 영상만 있다고 하면 노래제목 가수 등의 컬럼을 만들수도 있긴 합니다 하지만 영상의 종류는 제각각이며 그것들이 보내주는 데이터도 그만큼 제각각입니다 예를들면 ifland 안에 낚시게임이 있는데 여기는 내가 잡은 물고기의 종류나 물고기의 사이즈 잡기까지 소요된 시간 등을 보내줍니다 MySQL에서 JSON을 관리해보자 보통 json을 형태 그대로 저장하고 조회하고 집계한다면 mongoDB를 떠올리기 쉽습니다 실제로 mongoDB를 검토하기도 했습니다만 현재까지의 포스팅 기능은 모두 MySQL만을 활용해서 이미 개발이 되어있었고 부가정보만 추가로 저장하면 되므로 JSON 컬럼을 사용하기로 하였습니다 Json의 특정 값 조회 JSONEXTRACT를 통해 매우 쉽게 조회가 가능합니다 예를들어 낚시게임에서 올라오는 부가정보가 아래와 같은 식이고 한 컬럼에 저장하고 있다고 해보겠습니다 name 참치 size 81 여기서 name이 가물치이면서 size가 5m 보다 큰 물고기를 찾으려면 아래와 같이 하면 됩니다 SELECT FROM FISH WHERE JSONEXTRACTjsoncolumn name 가물치 AND JSONEXTRACTjsoncolumn size 5 JSON의 특정값을 selet 결과로 빼오기 예를들어 size5 이상으로 잡힌 적이 있는 물고기를 가져온다고 해보겠습니다 where에 들어가는 조건은 이렇게 하면 될 것 같은데요 JSONEXTRACTjsoncolumn size 5 문제는 이렇게 가져온 값의 jsoncolumn은 JSON값이라는 것입니다 이것에서 name만 가져오고 싶은 경우인데 이때는 JSONUNQUOTE값을 사용합니다 예시를 보겠습니다 중복된 name은 한번만 보여주면 되므로 JSONUNIQUE를 DISTINCT로 한번 더 감싸주었습니다 SELECT DISTINCT JSONUNQUOTEJSONEXTRACTjsoncolumn name AS names FROM FISH WHERE JSONEXTRACTjsoncolumn size 5 DEVOCEAN inside 게시글의 질문답변 요약 Q JSON 의 내용 검색도 된다니 정말 좋네요 많은 데이터가 있을때 성능상 이슈는 없는지 궁금하네요 jsoncolumn이 인덱스 없이 조회를 처리해야한다면 아무래도 Full Table Search 가 일어날꺼 같은 느낌이 들긴 합니다 혹시 성능상의 이슈나 이를 해결할 수 있는 방안등도 있을까요 A 테이블 설계할 때 해당 컬럼의 타입을 VARCHAR로 할 수 있고 JSON으로 할 수도 있습니다 VARCHAR로 해도 위의 JSON 함수가 동작합니다 JSON으로 하면 내부적으로 parsing을 하기 때문에 insert 비용이 조금 올라가기는 한데 select에서는 더 효율적이고 json 형식이 맞는지 validation 체크도 됩니다 또한 JSON으로 하는 경우는 parsing 처리를 하기 때문에 json 안의 특정 값을 index 설정할 수도 있습니다 JSON 전체를 인덱싱하는 것은 불가 다만 그렇게까지 할 정도라면 별도의 컬럼으로 빼는 것이 보통은 나은 선택일 것 같습니다 JSON 함수를 써서 select 하는 것은 유저id 및 날짜 등 다른 조건으로 일단 어느정도 걸러내고 이후에 JSON 조건을 거는 방식으로 활용하는 것이 좋습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165191&boardType=techBlog&searchData=&page=&subIndex=
,,,https://n.news.naver.com/mnews/article/023/0003809528?sid=105
@ITSUB,곧 출시될 아이폰 15 보험 뭘로 들까? 애플케어+ vs 통신사 비교해보니.. 어라?,https://img.youtube.com/vi/CNieoqGtKdI/0.jpg,https://www.youtube.com/watch?v=CNieoqGtKdI
"업스테이지-콴다-KT 공동 개발 '매스GPT', 챗GPT 제치고 '세계 1위'",,,https://n.news.naver.com/mnews/article/031/0000802819?sid=105
"""신형 갤럭시에 밀릴수도""…애플에 경고 나오는 이유",,,https://n.news.naver.com/mnews/article/015/0004933894?sid=105
AKS usecase : 신속한 MySQL 개발환경,일반적으로 클라우드 리소스에는 시일이 소요된다 개발과정에서 DB를 리소스로 편하게 사용하고자 하는 요구사항이 있다 mysql 가 필요한 경우에 대한 과정으로 살펴 본다 mysql 구성에 대해 요청은 접수portal을 통해서 MSP 작업 담당자에게 전달되어져야 한다 접수 이후에 MSP 작업 담당자가 할당된다 벌써 몇 십분은 흘러간다 작업 담당자는 요구사항을 한 번 더 점검한다 당일에 해소되기 어려운 절차라 빨라도 다음 날에야 클라우드 리소스를 제공 받는다 Agile한 방법은 있다 이전 같으면 Azure의 서비스를 구성하는 것이 자연스러웠다 지금은 aks로 시스템을 론칭하여 활용하고 있으니 mysql서비스로 aks가 바로 생각났다 aks를 활용하여 개발 환경을 신속히 제공하는 방법에 대해 생각하던 차라 반가웠다 AKS에 몇 분만에 MySQL을 설치하다 이미 argocd 환경은 구성되어 있다 repo를 우선 구성해야 한다 Artifact Hub 가 repo를 구성하는 허브로 알려져 있어 접속해서 확인해 보았다 bitnami의 mysql을 선택하여 INSTALL을 클릭하면 repo 주소를 알 수 있다 bitnami가 제공하는 repo URL은 httpschartsbitnamicombitnami 이다 MySQL 설치가 편리한 argocd 툴을 사용하자 argocd에 해당 bitnami repo 주소를 이용해서 helm 방식으로 타겟 클러스터에 배포를 한다 몇 분만에 mysql 설치를 완료할 수 있다 기존의 소통하는 시간 정도면 완성이 되었다 ArgoCD로 설치 과정에서 시행착오를 줄이자 HELM으로 표시된 섹션의 mysql 구성 정보와 관련된 파라미터를 변경하면 사용자가 원하는 방식으로 mysql을 구성할 수 있다 authrootPassword mysql클라이언트가 접속시 root유저에 대한 패스워드 설정 port mysql 데몬이 구동할 때 사용하는 포트 번호일반적으로 3306이나 보안을 고려해서 포트번호를 변경해서도 사용 service type ClusterIP가 default 이번 요구사항은 개발자가 자신의 로컬PC에서 접속 가능해야 하는 상황으로 외부로 서비스 노출이 가능하도록 LoadBalancer로 설정 primary secondary 모두 LoadBalancer로 변경 MySQL 접속 테스트는 K9s 도구를 활용하자 k9s를 통해 해당 mysql pod로 접속하여 아래와 같은 명령으로 설정했던 root패스워드로 접속 가능한지 확인 가능하다 mysql uroot p Enter password 설정한 루트 패스워드 설치한 MySQL에는 영구 스토리지도 있다 mysql로 생성한 리소스를 모두 확인해 보면 pod service가 보인다 pvc까지 할당된 것을 볼 수 있고 bitnami repo에서 제공하는 mysql은 스토리지 설정이 8GB 용량이다 테스트나 개발에 사용하기에 충분한 스토리지 용량이다 mysql로 생성한 리소스 모두 확인 kubectl get all n mysql NAME READY STATUS RESTARTS AGE podmysql0 11 Running 0 12m NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE servicemysql LoadBalancer 100228191 880830519TCP 12m servicemysqlheadless ClusterIP None none 8808TCP 12m kubectl get pvc n mysql NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE datamysql0 Bound pvcd60db5ddfc25434d8d4900aa1d734aa9 8Gi RWO default 16h 위의 EXTERNALIP는 보안을 위해 비식별화 표시 보안성에 유의하자 로컬PC에서 접속이 가능하여 편리하지만 보안성을 확보할 필요를 느낀다 LoadBalancer서비스 타입으로 pod를 생성을 하면 azure 에서 제공하는 MC리소스그룹에 자동으로 생성되는 nsgnetwork security group에다 IPport기반 접근제어를 설정한다 정리 개발자를 위한 mysql 개발 환경을 k8s 인프라로 설치하니 몇 분만에 제공할 수 있었다 개발자들는 지금부터 약 3개월간 사용할 예정이다 k8s클러스터의 유휴 자원을 활용할 수 있고 mysql 오픈소스로 설치하였다 일반적인 Azure database for MySQL서비스 발생 비용을 절감하여 개발이나 테스트에 리소스를 공급할 수 있었다 보안성을 위해 개발자 소스IP 한정으로 접속이 가능하도록 제한은 해야 한다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165182&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,얼굴로 움직이는 휠체어 ㄷㄷ |  2023 ICT 멘토링 엑스포 페스티벌,https://img.youtube.com/vi/TNFr3c0TzJA/0.jpg,https://www.youtube.com/watch?v=TNFr3c0TzJA
"카카오, 준신위 운영 위해 준법·신뢰 부문서 2개 소위 신설",,,https://n.news.naver.com/mnews/article/015/0004934138?sid=105
@nomadcoders,아직도 '랭체인'을 모른다고 해서 5분 설명해드림,https://img.youtube.com/vi/aDN8hm4pfPE/0.jpg,https://www.youtube.com/watch?v=aDN8hm4pfPE
"WHO “가자지구 중부, 의료 기능 마비” 미사일 포격에 의료진 대부분 대피",,,https://n.news.naver.com/mnews/article/366/0000960792?sid=105
Cilium 을 활용한 Kubernetes ClusterMesh 환경 구성,Cilium 을 활용한 Kubernetes ClusterMesh 환경 구성 cilium 의 여러가지 기능중에 clustermesh 를 활용하여 multi kubernetes cluster 를 구성하는 방법을 설명합니다 cilium 이 제공할 수 있는 것 cilium 의 기본 동작은 multi cluster 에 배포된 Application 의 k8s service 에 cilium service annotation 을 추가하는 것으로 동작합니다 k8s service object 에 annotaion 추가 apiVersion v1 kind Service metadata name echoserver namespace echoserver annotations serviceciliumioglobal true 진정한 multi cluster 구성이라면 아래 요건이 필요할텐데요 multi cluster routing application 배포시 pod 가 multi cluster 에 배포 cluster 가 추가되면 기존 pod 리벨런싱 cilium 의 기본 동작 원리가 k8s service 를 확장하는 개념이기 때문에 필요요건 중에 multi cluster 의 traffic 을 routing 해주는 기능을 제공해 줄 수 있습니다 cilium 은 multi cluster routing 기능에 초점 cilium 이 multi cluster 의 전체 요건을 충족한다고 볼 수는 없으나 multi cluster routing 기능을 활용하여 아래 cluster 운영 요건을 해결하는데 도움을 줄 수 있을 것이라는 개인적인 생각입니다 kubernetes migration kubernetes version upgrade kubernetes 이중화 구성 kubernetes 재해복구 구성 multi cluster routing 구성L4 단순 연결 아래 그림은 kubernetes cluster 2개를 를 단순히 L4 에 연결하여 ingress 로 접속되게 한 이중화 구성입니다 flowchart TB subgraph minikubeprd direction TB A1Ingress B1Service C10Pod B1Service C11Pod end subgraph minikubemig direction TB A2Ingress B2Service C21Pod B2Service C22Pod end LBL4 A1 LBL4 A2 이런 구성에서 아래 그림처럼 1개 cluster 내 pod 가 전체적으로 장애가 발생할 경우에 L4 에서는 인지를 할 수 없으니 traffic 을 계속해서 cluster 로 routing 을 하게 되서 일부 traffic 에 대한 장애가 발생하게 됩니다 flowchart TB subgraph minikubeprd direction TB A1Cilium Ingress B1Service B1Service X C11Pod X B1Service X C12Pod X end subgraph minikubemig direction TB A2Cilium Ingress B2Service B2Service C21Pod B2Service C22Pod end LBL4 traffic A1 LBL4 A2 multi cluster routing 구성cilium clusermesh 활용 아래 그림은 cilium clusermesh 기능을 활용하여 multi cluster routing 기능을 구성한 모습니다 flowchart TB subgraph minikubeprd direction TB A1Cilium Ingress B1Service B1Service C11Pod B1Service C12Pod end subgraph minikubemig direction TB A2Cilium Ingress B2Service B2Service C21Pod B2Service C22Pod end B1 C21 B1 C22 B2 C11 B2 C12 LBL4 A1 LBL4 A2 아래 그림에서 보듯이 1개 cluster 내에 pod 가 전체 장애가 발생하더라도 다른 cluster 의 pod 로 routing 을 해주게 됩니다 flowchart TB subgraph minikubeprd direction TB A1Cilium Ingress B1Service B1Service X C11Pod X B1Service X C12Pod X end subgraph minikubemig direction TB A2Cilium Ingress B2Service B2Service C21Pod B2Service C22Pod end B1 C21 B1 C22 B2 X C11 B2 X C12 LBL4 A1 LBL4 A2 Demo 구현 아래 Demo 절차는 직접 구현을 해볼 수 있도록 stepbystep 으로 모든 script 를 포함하고 있으니 순서대로 직접 구현을 해보셔도 좋습니다 참고로 아래 환경에서 실행하였습니다 다른 os 에서는 shell script 를 다르게 사용해야 합니다 mac os docker desktop minikube Demo 요약 minikube 로 2개의 cluster 를 생성하고 cilium clustermesh 설정을 한 후 각 cluster 에 application 을 배포하여 multi cluster 간 pod routing 이 되는지 확인하는 절차입니다 중요 체크사항으로 아래 minikube 설정에는 service cidr pod cidr 을 동일하게 구성했습니다 실제로 multi cluster routing 구성시에는 각 cluster 마다 다른 cidr 을 가지도록 설정을 해야 합니다 cilium cli 설치 아래 절차는 cilium cli 로 명령어를 실행하지만 실제로 cilium 은 CRDcustom resource definition을 생성하게 됩니다 생성되는 CRD 는 cluster 내에서 조회하거나 cilium document 를 참고하세요 os 환경에 맞게 script 를 사용해야 합니다 cilium document 를 참고하세요 local 에서 실행 CILIUMCLIVERSIONcurl s httpsrawgithubusercontentcomciliumciliumclimasterstabletxt CLIARCHamd64 if uname m arm64 then CLIARCHarm64 fi curl L fail remotenameall httpsgithubcomciliumciliumclireleasesdownloadCILIUMCLIVERSIONciliumdarwinCLIARCHtargzsha256sum shasum a 256 c ciliumdarwinCLIARCHtargzsha256sum sudo tar xzvfC ciliumdarwinCLIARCHtargz usrlocalbin rm ciliumdarwinCLIARCHtargzsha256sum cilium minikube 설치 minikube 가 설치나 addon 을 사용하는데 편리하여 단순 클러스터 구성시 자주 사용합니다 os 환경에 맞게 script 를 사용해야 합니다 minikube document 를 참고하세요 local 에서 실행 curl LO httpsstoragegoogleapiscomminikubereleaseslatestminikubedarwinamd64 sudo install minikubedarwinamd64 usrlocalbinminikube minikube version 설치된 minikube 확인 minikube profile list 삭제시 사용 minikube delete all 다운로드 한 docker image 까지 삭제할때 사용 minikube delete all purge mkprd 구성 Demo 에는 2개의 cluster 가 필요합니다 prd 클러스터를 먼저 생성합니다 각 단계는 이전 단계에서 생성되는 pod 가 정상적으로 실행이 되어야 다음 단계가 실행될 수 있으니 pod 실행 상태를 모니터링 하면서 진행하세요 local 에서 실행 minikube 설치 insecureregistry docker image pull 시 403 에러 발생해서 추가 staticip node ip 를 다르게 하려고 추가 minikube start p mkprd driverdocker insecureregistryquayio insecureregistryregistryk8sio networkminikube staticip1921680100 containerruntimecontainerd networkplugincni cnitrue metal lb 설치 minikube 에 addon 인 ingress 가 loadbalacer type 으로 실행되므로 추가 kubectl contextmkprd apply f httpsrawgithubusercontentcommetallbmetallbv01310configmanifestsmetallbnativeyaml metal lb IPAddressPool 설치 kubectl contextmkprd apply f EOF apiVersion metallbiov1beta1 kind IPAddressPool metadata name firstpool namespace metallbsystem spec addresses 192168100024 EOF minikube ingress enable minikube addons enable ingress p mkprd cilium install with ingressController enable cilium install context mkprd clusterid1 kubeproxyreplacementstrict helmset ingressControllerenabledtrue helmset ingressControllerloadbalancerModeshared cilium hubble enable cilium hubble enable context mkprd ui helmset hubblerelayservicetypeClusterIP cilium clustermesh enable cilium clustermesh enable context mkprd servicetypeNodePort cilium 상태 확인 cilium status context mkprd sample app 배포 kubectl contextmkprd apply f EOF apiVersion v1 kind Namespace metadata name echoserver apiVersion appsv1 kind Deployment metadata name echoserver namespace echoserver spec replicas 1 selector matchLabels app echoserver template metadata labels app echoserver spec containers image ealenechoserverlatest imagePullPolicy IfNotPresent name echoserver ports containerPort 80 env name PORT value 80 apiVersion v1 kind Service metadata name echoserver namespace echoserver annotations serviceciliumioglobal true spec ports port 80 targetPort 80 protocol TCP type ClusterIP selector app echoserver apiVersion networkingk8siov1 kind Ingress metadata name echoserver namespace echoserver spec ingressClassName nginx rules host echoserverlocalhost http paths path pathType Prefix backend service name echoserver port number 80 apiVersion networkingk8siov1 kind Ingress metadata name echoservercilium namespace echoserver spec ingressClassName cilium rules host echoserverciliumlocalhost http paths path pathType Prefix backend service name echoserver port number 80 EOF mkmig 구성 설치 절차는 prd cluster 와 동일합니다 local 에서 실행 minikube 설치 minikube start p mkmig driverdocker insecureregistryquayio insecureregistryregistryk8sio networkminikube staticip1921680200 containerruntimecontainerd networkplugincni cnitrue metal lb 설치 kubectl contextmkmig apply f httpsrawgithubusercontentcommetallbmetallbv01310configmanifestsmetallbnativeyaml IPAddressPool 설치 kubectl contextmkmig apply f EOF apiVersion metallbiov1beta1 kind IPAddressPool metadata name firstpool namespace metallbsystem spec addresses 192168200024 EOF minikube ingress enable minikube addons enable ingress p mkmig cilium install with ingressController enable cilium install context mkmig clusterid2 kubeproxyreplacementstrict helmset ingressControllerenabledtrue helmset ingressControllerloadbalancerModeshared cilium hubble enable cilium hubble enable context mkmig ui helmset hubblerelayservicetypeClusterIP cilium clustermesh enable cilium clustermesh enable context mkmig servicetypeNodePort cilium 상태 확인 cilium status context mkmig sample app 배포 kubectl contextmkmig apply f EOF apiVersion v1 kind Namespace metadata name echoserver apiVersion appsv1 kind Deployment metadata name echoserver namespace echoserver spec replicas 1 selector matchLabels app echoserver template metadata labels app echoserver spec containers image ealenechoserverlatest imagePullPolicy IfNotPresent name echoserver ports containerPort 80 env name PORT value 80 apiVersion v1 kind Service metadata name echoserver namespace echoserver annotations serviceciliumioglobal true spec ports port 80 targetPort 80 protocol TCP type ClusterIP selector app echoserver apiVersion networkingk8siov1 kind Ingress metadata name echoserver namespace echoserver spec ingressClassName nginx rules host echoserverlocalhost http paths path pathType Prefix backend service name echoserver port number 80 apiVersion networkingk8siov1 kind Ingress metadata name echoservercilium namespace echoserver spec ingressClassName cilium rules host echoserverciliumlocalhost http paths path pathType Prefix backend service name echoserver port number 80 EOF cilium clustermesh 연결 및 확인 cilium clustermesh 기능을 활성화해서 2개 cluster 간 routing 이 되도록 설정합니다 clustermesh connect 설정 cilium clustermesh connect context mkprd destinationcontext mkmig clustermesh 설정 상태 확인 완료될때까지 cilium clustermesh status context mkprd wait sample application service pod ip 확인 각 cluster 에 배포된 sample application echoserver 의 service pod ip 를 확인해봅니다 이때 ip 가 중복되지 않아야 합니다 위에서 cluster cidr 설정 필요에 대해 언급했습니다 kubectl contextmkprd get n echoserver svc o wide NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE SELECTOR echoserver ClusterIP 1011020029 none 80TCP 24m appechoserver kubectl contextmkmig get n echoserver svc o wide NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE SELECTOR echoserver ClusterIP 1010710189 none 80TCP 15m appechoserver kubectl contextmkprd get n echoserver pod o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES echoserver75b74d967z2k6f 11 Running 0 24m 1000124 mkprd none none kubectl contextmkmig get n echoserver pod o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES echoserver75b74d96758gmq 11 Running 0 16m 100076 mkmig none none cilium service routing 확인 cilium service routing 을 확인해보면 아래 결과처럼 echoserver service clusterIP 에 각 cluster 의 2개의 pod ip 가 할당되고 active 상태인 것을 확인할 수 있습니다 cluster 내 service clusterIP 로 들어오는 traffic 을 각 cluster 의 pod 로 routing 을 해준다는 의미입니다 cluster 가 3개 이상으로 늘어난다면 아래 숫자도 해당 cluster 수 만큼 늘어나게 됩니다 kubectl contextmkprd exec n kubesystem ti dscilium cilium service list clustermeshaffinity 30 101102002980 ClusterIP 1 100012480 active 2 10007680 active kubectl contextmkmig exec n kubesystem ti dscilium cilium service list clustermeshaffinity 30 101071018980 ClusterIP 1 10007680 active 2 100012480 active minikube ingress service 터널링 설정 이제 traffic 을 확인해보기 위해 minikube 의 ingress service 를 local 에서 호출할 수 있도록 minikube service 터널링을 실행합니다 실행상태로 유지되어야 하니 별도의 터미널에서 실행하세요 service 목록을 확인합니다 minikube service list p mkprd minikube service list p mkmig minikube ingress service 를 터널링 minikube service n ingressnginx ingressnginxcontroller p mkprd minikube service n ingressnginx ingressnginxcontroller p mkmig cilium ingress service 를 터널링 minikube service n kubesystem ciliumingress p mkprd minikube service n kubesystem ciliumingress p mkmig 각 실행 결과는 아래 형태로 표시됩니다 http80 에 해당하는 port 를 확인합니다 아래 실행결과에서는 http12700160011 를 사용해서 ciliumingress 를 호출하면 됩니다 각 실행결과를 참고하세요 NAMESPACE NAME TARGET PORT URL kubesystem ciliumingress http80 http192168010030880 https443 http192168010032224 Starting tunnel for service ciliumingress NAMESPACE NAME TARGET PORT URL kubesystem ciliumingress http12700160011 http12700160012 kubesystem ciliumingress http12700160011 http12700160012 sample application api 호출 확인 curl 명령어로 각 cluster 에 ingress 호출과 cilium ingress 를 각각 호출하여 routing 결과를 확인합니다 ingress 호출 각 클러스터에 있는 ingress 호출하면 해당 서버에 있는 pod 만 호출됨 각 클러스터에 있는 ingress 호출하면 해당 서버에 있는 pod 만 호출됨 port 를 위에 조회된 걸로 변경해야 함 for i in 110 do curl H Host echoserverlocalhost http12700163599echoenvbodyHOSTNAME sleep 1 echo done echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f for i in 110 do curl H Host echoserverlocalhost http12700163847echoenvbodyHOSTNAME sleep 1 echo done echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq cilium ingress 호출 각 클러스터에 있는 cilium ingress 호출하면 2개 서버에 있는 pod 에 호출됨 port 를 위에 조회된 걸로 변경해야 함 for i in 110 do curl H Host echoserverciliumlocalhost http12700163869echoenvbodyHOSTNAME sleep 1 echo done echoserver75b74d967z2k6f echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d967z2k6f echoserver75b74d967z2k6f for i in 110 do curl H Host echoserverciliumlocalhost http12700163987echoenvbodyHOSTNAME sleep 1 echo done echoserver75b74d967z2k6f echoserver75b74d96758gmq echoserver75b74d967z2k6f echoserver75b74d96758gmq echoserver75b74d96758gmq echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d967z2k6f echoserver75b74d96758gmq 이상으로 cilium clustermesh 를 활용하여 multi cluster routing 을 구성해 봤습니다 cilium 은 network 관련해서 종합선물셋트와 같은 open source 입니다 추가로 cilium 의 기능을 테스트하고 공유하도록 하겠습니다 End,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165280&boardType=techBlog&searchData=&page=&subIndex=
"애플, 약세 딛고 반등 성공...저가매수에 비전프로 기대감 더해져",,,https://n.news.naver.com/mnews/article/014/0005125254?sid=105
@codingapple,인생에 도움되는 맥북 고오급 터미널 명령어들 (일반인용),https://img.youtube.com/vi/EhiqpFtmjig/0.jpg,https://www.youtube.com/watch?v=EhiqpFtmjig
"(go-08) Go Type - Method, Interface",Golang 올해 마지막 연재글입니다 이번에는 Method Interface에 대한 개념을 정리해봅니다 Method method 는 struct 에 종속되어 실행할 수 있는 함수라고 생각하면 된다 struct 에 종속되었다고 표현하는 것을 receiver 라 하며 아래 func p Person String string 메소드를 보면 쉽게 이해가 된다 type Person struct FirstName string LastName string Age int func p Person String string return fmtSprintfs s d pFirstName pLastName pAge func main p Person FirstName Seungkyu LastName Ahn Age 52 fmtPrintlnpString Person 타입의 pString 으로 메소드를 호출할 수 있다 리시버는 포인터 리시버와 밸류 리시버로 나누어 진다 리시버의 필드 값을 바꾸고 싶다면 포인터 타입을 활용한다함수에서 포인터 아규먼트를 활용하는 것과 같다 또한 nil 인스턴스를 활용하고 싶다면 포인터 타입을 쓴다 리시버의 필드 값을 바꾸고 싶지 않다면 밸류 타입을 쓴다 func main var t Timerscheduled timeNow fmtPrintlntString tAddMinute10 fmtPrintlntString tAddMinute10 fmtPrintlntString type Timer struct scheduled timeTime func t Timer AddMinutem int tscheduled tscheduledAddtimeMinute timeDurationm func t Timer String string return fmtSprintfscheduled v tscheduled output scheduled 20220829 232323841625 0900 KST m0000193377 scheduled 20220829 233323841625 0900 KST scheduled 20220829 234323841625 0900 KST Timer 에 scheduled 시간의 값을 변경하기 위해서 포인터 리시버를 사용하여 필드 값을 변경했다 main 함수에서 Timer 변수를 포인터가 아닌 일반 변수로 선언했음에도 불구하고 tAddMinute10 를 호출한 후에 값이 변경되었음을 알 수 있다 이는 tAddMinute10 과 tAddMinute10 이 동일하기 때문이다 func main adder Adderstart 10 fmtPrintlnadderAdd10 func1 adderAdd method value fmtPrintlnfunc110 func2 AdderAdd method expression fmtPrintlnfunc2adder 10 type Adder struct start int func a Adder Addval int int return astart val output 20 20 20 method 는 function 과 비슷하다 method 를 변수에 할당할 수 있으며 이를 method value 라고 한다 타입 자체 method 로 부터 function 을 생성할 수 있는데 이를 method expression 이라고 하며 첫번째 파라미터로 리시버를 전달한다 type Animal int const dog Animal iota cat cow pig fmtPrintlndog cat cow pig output 0 1 2 3 const 와 iota 를 활용하면 enum 처럼 사용할 수 있다 type Logic interface Processs string string type Client struct L Logic func c Client Program p cLProcessMy Process fmtPrintlnp 나만의 Process 로직 만들기 type MyLogic struct func ml MyLogic Processs string string return fmtSprintfs is working s func main c Client L MyLogic cProgram output My Process is working Interface interface 를 잘 활용하면 다형성과 같은 개념을 사용할 수 있다 interface 선언 Logic interface struct 선언 필드로 interface 를 가짐 Client struct 리시버의 메소드 구현 Client struct 의 interface 호출 커스텀 struct 선언 MyLogic struct MyLogic 리시버의 Logic interface 구현 메소드 선언 커스텀 비즈니스 로직 구현 struct 의 필드로 interface 가진다는 의미는 해당 interface 를 구현한 어떠한 struct 도 넣을 수 있다는 의미이다 var s string fmtPrintlns nil var i interface fmtPrintlni nil i s fmtPrintlni nil output true true false interface 는 zero value 로 nil 을 갖는다 type MyInt int var i interface var mi MyInt 10 i mi fmtPrintlniMyInt 1 i2 ok iint if ok fmtPrintlnfmtErrorfunexpected type for v i return fmtPrintlni2 1 output 11 unexpected type for 10 interface 는 interfaceType 으로 형변환 가능하다 type MyInt int type MyRead struct func m MyRead Readp byte n int err error return 0 nil func checki interface switch j itype case nil fmtPrintfj Tn j case int fmtPrintfj Tn j case MyInt fmtPrintfj Tn j case ioReader fmtPrintfj Tn j case string fmtPrintfj Tn j case bool rune fmtPrintfj Tn j default fmtPrintfj Tn j func main var a interface checka var b int checkb var c MyInt checkc var d bool checkd var e rune checke var f MyRead checkf var g string checkg output j nil j int j mainMyInt j bool j int32 j mainMyRead j string interface 를 받아서 타입을 체크하는 로직을 넣을 수 있다 Interface 를 활용한 방법을 알아보자 먼저 DataStore 와 Logger interface 를 만든다 type DataStore interface UserNameForIDuserID string string bool type Logger interface Logmessage string LoggerAdapter 는 Log 메소드를 구현한 함수하기 때문에 Logger 타입이다 type LoggerAdapter funcmessage string func lg LoggerAdapter Logmessage string lgmessage func LogOutputmessage string fmtPrintlnmessage type 이 함수이면 다음과 같이 사용할 수 있다 즉 LogOutput 함수를 LoggerAdapter 함수로 타입을 변환한다 이렇게 되면 LogOutput 은 결론적으로 Logger 타입이라 할 수 있다 l LoggerAdapterLogOutput SimpleDataStore 는 UserNameForID 메소드를 구현했으므로 DataStore 타입이다 type SimpleDataStore struct userData mapstringstring func sds SimpleDataStore UserNameForIDuserID string string bool name ok sdsuserDatauserID return name ok func NewSimpleDataStore SimpleDataStore return SimpleDataStore userData mapstringstring 1 Fred 2 Mary 3 Pat SimpleLogic 은 Logger 아 DataStore 인터페이스를 가지는 struct 이다 또한 BusinessLogic 인터페이스를 구현하였기 때문에 BusinessLogic 타입 이기도 하다 business logic type BusinessLogic interface SayHellouserID string string error type SimpleLogic struct l Logger ds DataStore func sl SimpleLogic SayHellouserID string string error sllLogin SayHello for userID name ok sldsUserNameForIDuserID if ok return errorsNewunknown user return Hello name nil func sl SimpleLogic SayGoodbyeuserID string string error sllLogin SayGoodbye for userID name ok sldsUserNameForIDuserID if ok return errorsNewunknown user return Goodbye name nil func NewSimpleLogicl Logger ds DataStore SimpleLogic return SimpleLogic l l ds ds Controller 는 Logger 아 BusinessLogic 을 가진 struct 이다 type Controller struct l Logger logic BusinessLogic func c Controller Greetingw httpResponseWriter r httpRequest clLogIn SayHello userID rURLQueryGetuserid message err clogicSayHellouserID if err nil wWriteHeaderhttpStatusBadRequest wWritebyteerrError return wWritebytemessage func NewControllerl Logger bl BusinessLogic Controller return Controller l l logic bl func main l LoggerAdapterLogOutput ds NewSimpleDataStore logic NewSimpleLogicl ds c NewControllerl logic httpHandleFunchello cGreeting httpListenAndServe7777 nil curl X GET localhost7777hellouserid1 Hello Fred,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164189&boardType=techBlog&searchData=&page=&subIndex=
"티빙, '네이버·통신 연합군' 제쳤다…프로야구 중계권 우협대상자 선정",,,https://n.news.naver.com/mnews/article/138/0002164444?sid=105
@codingapple,코딩실력보다 훨씬 더 중요한거,https://img.youtube.com/vi/pkr48S22zH0/0.jpg,https://www.youtube.com/watch?v=pkr48S22zH0
"엔씨소프트, 조직개편…최고사업책임자 3인 중심으로",,,https://n.news.naver.com/mnews/article/018/0005651096?sid=105
@coohde,WEB1 - 19. 웹서버 운영하기 (2023년 수정판. VSCode Live server),https://img.youtube.com/vi/pyoTcvNrbmk/0.jpg,https://www.youtube.com/watch?v=pyoTcvNrbmk
"카카오, 올해 첫 준신위 회의…쇄신 속도 낸다",,,https://n.news.naver.com/mnews/article/029/0002847966?sid=105
누구나 할 수 있는 10배 더 빠른 배치 만들기,itemname subname 최근 셀러시스템팀에서는 하루 한 번 주기로 실행되는 배치를 최적화하는 과제를 진행했습니다 작업 결과 좋은 성과를 얻었고 최적화를 검토하는 의사 결정 과정 자체로도 의미 있는 사례라고 생각해 경험을 공유하고자 합니다 먼저 셀러시스템에서 관리하는 비운영 시간 데이터 설명부터 시작해 보겠습니다 비운영 시간 데이터 셀러시스템에서는 가게와 업주에 대한 다양한 데이터를 관리합니다 배달의민족에 입점한 사장님들이 가게의 요일별 휴무일 임시 휴무일 공휴일 휴무 여부 임시 운영 중지 등 다양한 휴무 설정할 수 있는데요 실제 가게가 노출되고 음식을 주문하는 과정에서는 가게가 운영하는지 안하는지만 중요하기 때문에 셀러시스템팀에서는 이러한 정보를 조합하여 계산된 결과만을 유관부서에 전달하기도 합니다 이것이 바로 비운영시간 데이터입니다 셀러시스템은 다양한 채널에서 입력되는 각종 운영과 휴무 데이터를 취합하고 비운영시간 데이터를 계산합니다 그 후 이러한 데이터가 클라이언트까지 잘 전달될 수 있도록 각 지면에 적절한 형태로 가공하여 제공하는 역할을 합니다 현재 연동 구조에서는 실시간으로 수정되는 정보를 반영하는 것뿐만 아니라 매일 새벽에 전체 데이터를 계산하고 그 결과를 미리 갱신해둔 후 유관부서에 전파하는 작업 또한 하고 있습니다 운영시간 데이터에 대한 좀 더 자세한 정보가 궁금하시다면 아래 글도 참고하시기 바랍니다 현실 타협은 후퇴다 안 되는 일을 되게 만드는 PM이란 문제 상황 새벽에 배치 작업을 할 때 배달의민족에 등록된 수많은 가게의 데이터를 매일 갱신하기 때문에 배치 수행 시간이 상당히 오래 걸립니다 이른 새벽에 배치가 실행되는 덕분에 일과 시간 이전에 배치가 모두 끝나고 DB 부하도 큰 수준은 아니어서 여태까지는 큰 문제없이 운영되고 있었습니다 하지만 최근 배포를 새벽에 진행할 일이 여러 번 있었는데요 배포를 할 때마다 배포 예정 시간과 배치 실행 시간이 겹치는 바람에 배치가 없었다면 매끄럽게 진행될 배포가 여러 번 복잡한 절차를 밟아 진행할 필요가 생겼습니다 팀에서는 이렇듯 새벽 시간까지 오래 실행되는 배치가 배포 및 운영에 영향을 끼치는 것은 잠재적인 리스크라고 판단하였습니다 셀러시스템팀에서는 이러한 리스크가 발견되었을 때 당장 조치가 필요한 것이 아니라면 우선 백로그개발을 기다리는 과제 목록에 등록합니다 배치 성능 개선 과제 또한 우선 백로그에 등록하였고 이후 정기적인 백로그 그루밍백로그 항목을 살펴보고 유지 관리하는 프로세스 회의에서 우선순위를 재평가하고 팀원 사이에서 의견을 취합하였습니다 그루밍 회의에서 배치 성능 개선이 필요하다는 것에 공감대가 있었고 우선순위를 높여 스프린트에서 과제를 진행하게 되었습니다 IO 최적화 배치 수행시간 개선을 위해 우선적으로 살펴본 부분은 IO 병목이었습니다 IO Input Output 병목이란 컴퓨팅에서 부하를 설명할 때에는 크게 CPU 부하와 IO 부하로 나뉩니다 데이터를 계산하고 처리하는 과정인 CPU 부하와 달리 IO 부하는 디스크에 파일을 읽고 쓰거나 DB 및 외부 컴포넌트와 통신하는 과정에서 발생합니다 IO 병목은 이러한 IO 부하가 시스템의 전체적인 효율성을 떨어뜨리는 부분을 말합니다 배치에서 사용하는 IO 부하 중 가장 핵심은 DB 쿼리였기 때문에 코드 및 로컬 환경에서 실제 호출하는 DB 쿼리를 살펴보면서 IO 병목 지점을 살펴보았는데요 JPA 지연 로딩으로 설정된 연관 관계 엔티티를 가져오는 과정에서 N1 문제가 발생하는 것을 확인하였습니다 위에서 언급하였듯 비운영시간 데이터 계산을 위해서는 다양한 종류의 데이터를 가져와야 하는데요 이러한 데이터가 모두 1 N 구조의 연관관계로 설정되어 있어서 관련 데이터를 가져오는 데에 오랜 시간이 걸리는 것을 확인할 수 있었습니다 N1 문제의 해결방식은 다양한데요 연관관계로 설정된 엔티티의 종류가 많고 실제 연관관계 데이터의 수정은 불필요하다는 점 등을 고려하여 각 엔티티 정보를 연관관계를 통해 가져오는 것이 아닌 별도 쿼리 호출을 통해 명시적으로 한 번에 읽어오게끔 수정했습니다 수정 전 public ListLiveShopClose generateLiveShopCloseShop shop LocalDate startDate LocalDate endDate final ListShopCalendar shopCalendars shopCalendarRepositoryfindAllByCalendarDateBetweenstartDate endDate final ListShopTemporaryClosed shopTemporaryCloses shopgetActiveShopTemporaryClosed final ListShopClosed shopCloses shopgetActiveShopClosed final ListShopOperationHour operationHours shopgetShopOperationHourIsTypeOperationHourTypeOPERATION return LiveShopClose 데이터 생성 수정 후 ListLiveShopClose generateLiveShopClosesListLong shopNos LocalDate startDate LocalDate endDate ListShopNo shopNoEntities shopNosstreammapShopNonewcollectCollectorstoList ListShopCalendar shopCalendars shopCalendarRepositoryfindAllByCalendarDateBetweenstartDate endDate MapLong ListShopTemporaryClosed activeShopTemporaryClosedMap shopTemporaryClosedRepositoryfindActiveByShopNosshopNoEntitiesstream collectgroupingByShopTemporaryClosedgetShopNo CollectorstoList MapLong ListShopClosed activeShopClosedMap shopClosedRepositoryfindActiveByShopNosshopNoEntitiesstream collectgroupingByShopClosedgetShopNo CollectorstoList MapLong ListShopOperationHour operationHoursMap shopOperationHoursRepositoryfindOperationHoursByShopNosshopNoEntitiesstream collectgroupingByShopOperationHourgetShopNo CollectorstoList return shopNosstream flatMapshopNo generateLiveShopClose shopNo shopCalendars ListUtilsemptyIfNullactiveShopTemporaryClosedMapgetshopNo ListUtilsemptyIfNullactiveShopClosedMapgetshopNo ListUtilsemptyIfNulloperationHoursMapgetshopNo stream collectCollectorstoList ListLiveShopClose generateLiveShopCloseLong shopNo ListShopCalendar shopCalendars ListShopTemporaryClosed activeShopTemporaryCloses ListShopClosed activeShopCloses ListShopOperationHour operationHours return LiveShopClose 데이터 생성 도메인 로직 및 기타 최적화 그 다음으로는 도메인 로직을 고려해 더 최적화할 수 있는 부분이 있을지 살펴보았습니다 현재 가게의 비운영시간 데이터가 업데이트될 경우 변경된 가게에 대한 이벤트를 발행하고 있는데요 기존 로직에서는 실제 데이터의 변경 여부와는 관계없이 D1D2 데이터를 무조건 재생성하기 때문에 실제로는 데이터가 변경되지 않을 테지만 다시 데이터가 생성되어 변경 이벤트가 전송되는 케이스가 있었습니다 이러한 케이스에 대응하여 데이터가 바뀌었는지 여부를 확인한 후 실제로 바뀐 경우에만 변경 사항을 적용하도록 개선하였습니다 이를 통해 불필요한 DB 부하를 줄여 배치 수행시간을 줄일 수 있을 뿐만 아니라 변경 이벤트로 인한 간접적인 부하 또한 개선할 수 있었습니다 최적화 검토 잠시 다른 얘기를 해보겠습니다 유명한 개발 서적인 Effective Java3rd ed Joshua Bloch 2017에서는 아래와 같은 격언을 소개하고 있습니다 성능 효율을 높이기 위해 컴퓨팅 업계에서는 많은 죄악이 저질러지는데심지어 효율적이지조차 않을 때도 있다 그 수는 그냥 멍청해서 저지르는 죄악보다 많다 William A Wulf 1972 우리는 세세한 성능 효율에 대해서는 무시할 필요가 있다 말하자면 97가 이 경우에 해당한다 섣부른 최적화는 만악의 근원이다 Donald E Knuth 1974 우리는 최적화에 대해서 다음 두가지 규칙을 따른다 첫째 하지 마라 둘째 전문가 한정 아직은 하지 마라 최적화되지 않은 상태로도 완벽하게 깔끔한 해결책을 찾는 것이 먼저다 M A Jackson 1975 Java1995년는 물론이고 SQL1978년과 C1980년이 등장하기도 전에 프로그래밍 대선배들은 위와 같은 발언들을 쏟아냈습니다 최적화 얘기를 한참 하다가 최적화가 죄악이란 격언을 가져오다니 뜬금이 없으실 텐데요 사실 이는 무작정 최적화하지 말란 것은 아니고 효율만 좇다가 득보다 실이 큰 경우를 경계하라는 뜻에 가깝다고 생각합니다 최적화를 하기 전에 항상 아래 두가지를 검토해보면 좋을 것 같습니다 최적화 이전에 먼저 좋은 코드를 작성하기 코드를 작성하는 데 있어서 성능을 염두에 두는 것은 물론 중요합니다 하지만 많은 경우 대부분의 코드는 성능상 영향이 크지 않고 실제로 병목이 되는 부분은 극히 일부분입니다 좋은 코드를 최적화하기는 쉽지만 섣부르게 최적화된 코드를 좋은 코드로 만드는 건 어렵습니다 빠른 코드보다는 좋은 코드를 짜는 데에 먼저 집중하고 최적화는 그 다음에 생각해야 합니다 정량적으로 성능을 측정하면서 병목을 파악하기 정량화된 지표를 통해 실제로 병목이 되는 부분을 파악해야 합니다 지엽적인 부분을 일일히 개선하는 마이크로 최적화는 많은 경우 100ms 를 99ms로 줄이는 것에 그칩니다 마이크로 최적화 보다는 거시적인 관점에서 중요한 병목을 찾고 이를 구조적으로 해결하는 것이 중요합니다 그리고 실질적으로 얼마나 빨라졌는지 정량적인 성과로 나타낼 수 있어야 합니다 이번에 진행한 최적화는 유의미한 최적화였을까요 우선 기존에 안정적으로 동작하며 비즈니스 로직이 명확한 코드가 있었습니다 하지만 이러한 안정적인 코드의 수행시간이 오래 걸려 운영 및 유지보수에 있어서 잠재적인 큰 리스크라는 공감대가 있었습니다 최적화를 위해 구조적인 병목을 찾았고 성능 테스트 결과 5배 이상 더 빨리 실행되는 것을 확인하였습니다 베타 환경 테스트 결과 특정 조건에서는 20배 이상 빨라지기도 하였습니다 이 정도의 성능 개선이라면 최초 문제가 되었던 상황을 깔끔하게 해결함과 동시에 다소 복잡해진 코드를 고려해도 유의미한 최적화라는 결론을 내렸습니다 빨라도 문제 근데 이거 빨라도 너무 빨라진 것 같습니다 개발 환경에서의 지표를 통해 운영 환경 소요시간을 유추해 보면 6시간 걸리던 것이 1시간 조금 넘게 걸리는 것으로 나오는데요 내가 뭘 놓친 게 있나 아니면 코드를 잘못 짰나 생각이 들었지만 설령 정상 동작하더라도 무작정 빠른 게 능사가 아니기 때문에 안정적인 서비스 제공이 가능한지 다시 검토해보았습니다 MSA 구조에서는 애플리케이션과 직접적으로 연동되는 DB와 로드밸런서 등 뿐만 아니라 많은 모듈 및 유관부서들이 유기적으로 연결되어 있기 때문에 영향 범위를 면밀히 검토해야 합니다 특히 위에서 한 번 간단하게 언급한 것처럼 현재는 다음과 같은 형태이기 때문에 이 부분에 문제가 없을지 주로 검토했습니다 데이터 변경이 발생하면 변경 사항이 큐를 통해서 유관부서에 전달되고 필요에 따라 유관부서가 추가적인 API 호출을 하는 구체적으로는 아래와 같은 사항들을 확인해보았습니다 개발 환경에서 테스트 당시 애플리케이션이 실행되는 서버의 CPU 및 IO 지표 개발 환경에서 테스트 당시 DB CPU 쿼리 지연 시간 등 지표 예상 트래픽을 산출 현재 운영 환경에서의 피크 트래픽과 비교하여 문제가 없을지 검토 변경 사항을 전달하는 큐에서 지연이 발생해도 문제가 없을지 검토 예상 트래픽 비교 및 도메인 로직 최적화 과정에서 변경이벤트 또한 상당히 많이 줄어든 점을 감안하여서 문제가 없을 것으로 확인하고 운영 환경에 배포하였습니다 배포 이후 문제가 없을 것으로 예상하였지만 일들이 항상 마음처럼 굴러가던가요 운영환경에서 추정했던 속도보다 더 빠르게 동작을 하는 바람에 유관 부서 트래픽 또한 예상 이상으로 인입되어 DB CPU가 다소 높아지는 문제가 있었습니다 그렇지만 너무 빨라서 발생하는 문제에 대해서 사전에 미리 검토를 해보았던 덕분에 당황하지 않고 빠르게 문제 원인을 좁히고 대응 방안을 도출할 수 있었습니다 근본적으로는 실행 속도가 너무 빨라진 것이 문제이기 때문에 모순적이지만 우선 단기적인 대응 방안으로 의도적으로 지연 시간을 설정해 천천히 실행하도록 수정하였습니다 BeanSTEPNAME JobScope public Step liveShopCloseCreateStep return stepBuilderFactorygetSTEPNAME Long LongchunkCHUNKSIZE readershopCloseScheduleReadernull writerliveShopCloseWriternull null null transactionManagerstoreTransactionManager listenernew AfterChunkSleepListener200 build Slf4j public class AfterChunkSleepListener implements ChunkListener private final long sleepMillis public AfterChunkSleepListenerlong sleepMillis thissleepMillis sleepMillis Override public void afterChunkChunkContext context try loginfoChunk 실행 후 sleep millis 현재 read Count sleepMillis contextgetStepContextgetStepExecutiongetReadCount TimeUnitMILLISECONDSsleepsleepMillis catch InterruptedException e logerrorThread sleep interrupted e Override public void afterChunkErrorChunkContext context 사용안함 Override public void beforeChunkChunkContext context 사용안함 그리고 위와 같이 지연 시간을 설정해도 기존 390분이 소요되던 배치가 30분 소요되는 결과 를 얻을 수 있었습니다 마지막으로 이러한 변경이벤트를 유관부서에 전달하고 유관부서가 다시 우리 API를 호출하는 방식에 대해서 좀 더 효율적인 해결책은 없을지 고민하게 되는 계기가 되었습니다 결론 이번 최적화 작업을 요약하면 다음과 같습니다 리스크를 확인하고 과제에 대한 우선순위를 조정하기 문제 상황을 분석하고 병목을 확인하기 IO의 경우 최대한 한번에 여러건을 읽고 쓰도록 하여 효율성 높이기 도메인 로직을 검토하여 개선할 수 있는 부분이 있는지 살피기 유의미한 최적화인가 정량적인 지표로 다시 검토하기 빨라도 문제일 수 있으니 최적화에 의한 영향 범위를 검토하고 운영 환경에서도 문제가 없을지 확인하기 대단한 알고리즘을 작성하지도 유행하는 신규 프레임워크를 사용한 것도 아니지만 생각보다 좋은 결과를 얻었고 그 과정도 좋은 사례라고 생각되어 공유드립니다 당연하게 생각되는 부분이라도 돌다리를 한 번 더 두드려보듯이 항상 한 번 더 고민해 본다면 누구라도 저보다 더 잘하실 수 있으리라고 생각합니다 장렬 우아한형제들 셀러시스템팀에서 백엔드 엔지니어를 맡고 있습니다,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/13569/
AKS로 쿠버네티스 시작하기 : 간단한 spring boot 앱 배포하기,AKS로 쿠버네티스 시작하기 argocd툴에 ingress 통해 접속 에 이어서 지난 블로그에서 argocd로 도메인명으로 쉽게 접속할 수 있게 되었습니다 CICD 환경이 어느 정도 구성이 되었으니 현재까지 구성한 클러스터을 기반으로 간단한 앱을 배포해 보도록 하겠습니다 지난 블로그 링크 AKS로 쿠버네티스 시작하기 argocd툴에 ingress 통해 접속 Spring Boot 애플리케이션 소스 준비 pomxml spring boot 소스 indexhtml 로 구성을 합니다 pomxml xml version10 encodingUTF8 project xmlnshttpmavenapacheorgPOM400 xmlnsxsihttpwwww3org2001XMLSchemainstance xsischemaLocationhttpmavenapacheorgPOM400 httpsmavenapacheorgxsdmaven400xsd modelVersion400modelVersion parent groupIdorgspringframeworkbootgroupId artifactIdspringbootstarterparentartifactId version310version relativePath lookup parent from repository parent groupIdcomexamplegroupId artifactIdhellospringartifactId version001SNAPSHOTversion namehellospringname descriptionhellospringdescription properties javaversion17javaversion properties dependencies dependency groupIdorgspringframeworkbootgroupId artifactIdspringbootstarterwebartifactId dependency dependency groupIdorgspringframeworkbootgroupId artifactIdspringbootstartertestartifactId scopetestscope dependency dependencies build plugins plugin groupIdorgspringframeworkbootgroupId artifactIdspringbootmavenpluginartifactId plugin plugins build project HellospringApplicationjava package comexamplehellospring import orgspringframeworkbootSpringApplication import orgspringframeworkbootautoconfigureSpringBootApplication SpringBootApplication public class HellospringApplication public static void mainString args SpringApplicationrunHellospringApplicationclass args indexhtml DOCTYPE html html langen head meta charsetUTF8 titlehellotitle head body h1hello springboot working on k8sh1 body html 서비스 동작 확인 아래와 같이 8080 포트에서 서비스를 제공하는 것을 확인할 수 있습니다 간단한 애플리케이션이 준비가 되었습니다 로컬 환경에서 잘 동작하는 것이 확인이 되었습니다 이번에는 CICD 자동화 이전에 Dockerfile을 활용하여 컨테이너 이미지를 만드는 과정을 포함해서 전체 세부 절차를 확인할 수 있는 내용으로 진행해 보겠습니다 단계별 절차를 이해하면 자동화 시에도 자신의 환경에 맞게 최적화 구성을 할 수 있겠습니다 로컬 환경에서 jar 기동 확인 Spring boot 의 특성상 내장형 톰캣 구조로 로컬 환경에서 아래의 java명령어로 jar로 생성된 애플리케이션을 동작시키는 것도 가능합니다 로컬에서 잘 작동하는 지 먼저 확인하고 컨테이너 이미지를 작성하는 것을 권장합니다 애플리케이션 빌드를 수행하면 target디렉토리에 jar 파일이 생성됩니다 해당 디렉토리에서 아래의 명령어를 수행하면 jar로 생성한 애플리케이션을 동작시킬 수 있습니다 java jar jar Dockerfile 만들기 애플리케이션 빌드환경과 일치하는 openjdk를 base로 해서 애플리케이션 패키징을 통해 생성한 jar파일을 appjar로 복사한 다음 8080포트로 기동시키는 Dockerfile입니다 EXPOSE 8080 기동하는 컨테이너는 8080포트를 통해서 서비스 제공을 합니다 ENTRYPOINT를 통해서 컨테이너가 기동하면 java jar appjar 가 동작할 수 있도록 구성을 할 수 있습니다 FROM openjdk17alpine314 WORKDIR ARG JARFILEjar COPY targetJARFILE appjar EXPOSE 8080 ENTRYPOINT java jarappjar 애플리케이션 이미지 만들고 태깅하기 Dockerfile을 만든 애플리케이션 프로젝트 루트에서 podman build 명령어로 타겟 클러스터 플랫폼리눅스 맞게 acrappazurecriohellospringlatest로 이미지를 만들고 태깅하여 생성합니다 podman build platform linuxamd64 tag acrappazurecriohellospringlatest STEP 16 FROM openjdk17alpine314 STEP 26 WORKDIR 241ccc727841 STEP 36 ARG JARFILEjar efe8fbff9872 STEP 46 COPY targetJARFILE appjar 5b38963da7df STEP 56 EXPOSE 8080 72ca8d7eaf0c STEP 66 ENTRYPOINT java jarappjar COMMIT acrappazurecriohellospringlatest ec212b7dca25 Successfully tagged acrappazurecriohellospringlatest ec212b7dca25e83f1b0e01a5bbd483888e0355abfbd98c67327103e30f57f0b8 ACR로 업로드하기 위에서 생성한 이미지태깅포함를 podman push 명령어로 ACR에 업로드 가능합니다 podman push acrappazurecriohellospringlatest ServiceDeployment yaml 파일 작성하기 ACR에 올려진 이미지를 이제 k8s클러스터에 배포할 차례입니다 k8s에서는 해당 이미지를 사용하여 컨테이너를 구동시켜 서비스를 제공할 수 있게 하기 위해 service오브젝트와 deployment오브젝트를 yaml파일로 작성해야 합니다 apiVersion v1 kind Service metadata name svchellospring namespace hellospring spec type LoadBalancer ports name http port 8080 targetPort 8080 selector app apphellospring apiVersion appsv1 kind Deployment metadata name deployhellospring namespace hellospring spec replicas 2 selector matchLabels app apphellospring template metadata labels app apphellospring spec nodeSelector kubernetesioos linux containers name containerhellospring image acrappazurecriohellospringlatest resources requests cpu 100m memory 256Mi limits cpu 250m memory 512Mi env name TZ value AsiaSeoul ports containerPort 8080 작성한 ServiceDeployment 을 k8s에 배포하기 k9s 툴을 띄워두고 kubectl apply f yaml 파일명 명령어를 통해 배포하면 pod상태를 바로 확인할 수 있습니다 혹시라도 pod 동작에 에러가 발생하면 k9s의 기능을 이용해서 디버깅도 가능합니다 소문자 l 키 logs d 키 describe 참고로 위의 오른쪽 패널의 k apply 명령은 kubectl apply 를 의미합니다kubectl을 k로 alias 해서 사용 배포한 서비스 확인 kubectl get svc n 으로 서비스를 확인할 수 있습니다 n은 namespace를 의미 여기서는 external IP가 2019622889 입니다 k get svc n hellospring NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE svchellospring LoadBalancer 10057154 2019622889 808030720TCP 10m 해당 external IP로 접속해 보면 서비스 동작을 확인할 수 있습니다 정리 소스작성 컴파일 jar생성 java jar로 확인 Dockerfile작성 컨테이너 이미지 생성과 태깅 ACR에 업로드 ServiceDeployment yaml 작성 k8s에 배포 순으로 전체 과정을 간단한 앱을 통해 진행해 보았습니다 해당 절차를 응용하면 컨테이너 동작과 관련한 문제 발생에 단계적으로 확인하여 대응할 수 있습니다 다음은 k8s 구성 최적화 측면과 k8s를 활용한 유용한 usecase 사례를 진행해 보도록 하겠습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164957&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,AI 클론싱어 정답 오류 정말 죄송합니다,https://img.youtube.com/vi/H7WnQ5m6xf8/0.jpg,https://www.youtube.com/watch?v=H7WnQ5m6xf8
로그 및 SQL 진입점 정보 추가 여정,itemname subname 안녕하세요 B마트서비스개발팀 박지우입니다 1월 신규 입사 후 맡게 된 가장 컸던 기술 개선 건으로 로그 및 SQL에 진입점 정보를 남기는 작업을 진행했는데요 이 작업이 왜 필요한지 그리고 이 과정에서 겪은 시행착오와 해결 방법은 무엇인지를 중심으로 이야기를 나누려고 합니다 앞으로 해당 작업을 진행하시는 분들이 같은 난관에 직면할 때 이 글을 통해 조금이나마 도움을 받을 수 있기를 바라며 글 시작하겠습니다 진입점이 뭐죠 진입점은 사용자 요청의 시작점을 의미합니다 애플리케이션 또는 시스템에서 사용자 요청이 최초 진입되는 지점이 바로 진입점 입니다 그 중에서도 SQL 진입점은 데이터베이스 시스템으로의 쿼리 전송 시 애플리케이션 또는 시스템에서 쿼리가 전송된 시작점을 의미합니다 다음은 앱 로그에서 확인되는 쿼리 정보 예시입니다 handler JiwooControllergetShop 이 부분이 눈에 띄지 않으시나요 기존에 모든 SQL 요청에 대해 다음 방식으로 진입점 정보를 SQL 로그 주석 handler 으로 남기는 작업을 진행했습니다 정확히는 컨트롤러메소드 이벤트 컨슈머메소드 배치 job 이름을 남기게 되었습니다 초기 세팅부터 추가적인 고려 사항들까지 포함한 전환 과정을 공유하고자 합니다 근데 왜 진입점 정보가 남아야 해요 모든 SQL 요청마다 진입점 정보가 남는다면 다음과 같은 큰 이점을 누릴 수 있습니다 1 성능 최적화 특정 진입점에서 발생하는 과도한 데이터베이스 액세스나 비효율적인 쿼리를 식별하는 데 도움이 됩니다 DBA분들이 제공한 slow query 목록이 있습니다 Grafana 운영 DB의 MySQL Overview에서 slow query 리스트를 확인할 수도 있습니다 하지만 각 쿼리가 어디서 실행됐는지는 직접 코드에서 확인이 필요합니다 SQL문 마다 진입점 위치를 알 수 있다면 개선 작업이 훨씬 빨라집니다 2 추적성 문제가 발생했을 때 어떤 진입점에서 해당 SQL 요청이 발생했는지 쉽게 파악할 수 있습니다 이로써 디버깅 시간을 줄일 수 있습니다 예를 들어 데드락 발생 시 최초 컨트롤러 위치를 안다면 더 빠른 해소가 가능할 겁니다 또한 어떤 컨트롤러나 이벤트 컨슈머 진입점에서 DB 리소스를 가장 많이 사용하는지 분석도 가능하고 어떤 진입점이 데이터베이스를 얼마나 자주 사용하는지 어떤 시간대에 가장 활발한지 등의 패턴을 파악하는 데에 도움이 됩니다 의심스러운 쿼리나 예상치 못한 접근이 있을 때 어느 모듈 어느 진입점에서 발생했는지 역시 확인할 수 있습니다 3 심미성 아름답죠 MDC를 아시나요 그렇다면 로그에 왜 진입점 정보가 제대로 남지 않았을까요 이번 작업을 통해 저 역시 MDC라는 개념을 처음 알게 되었습니다 이에 대한 사전 지식을 먼저 공유하고자 합니다 1 MDCMapped Diagnostic Context MDCMapped Diagnostic Context는 자바 로깅 프레임워크slf4j 등에서 지원하는 현재 실행중인 쓰레드 단위에 메타 정보를 넣고 관리하는 공간 입니다 MDC라는 이름처럼 Map 형태라 Key Value 형태로 값을 저장합니다 또한 각 스레드는 자체 MDC를 가지기 위해 threadLocal 로 구현되어 있습니다 동일한 logger를 사용하더라도 로깅한 메시지는 스레드마다 다른 contextMDC 정보를 가지게 되는 것이죠 사실 SQL 진입점 정보를 넣는 방법은 다양합니다 그 중 MDC를 SQL 진입점 정보를 넣기로 결정한 것은 모든 로그에 이 로그를 남기게된 호출의 진입점 정보를 남기기 위함입니다 MDC로 로그에 진입점 정보를 남기고 SQL에도 MDC 속 값을 꺼내 진입점 정보를 함께 남긴다면 로그부터 SQL까지 모두 진입점 정보를 쉽게 확인할 수 있는 구조로 확장이 가능합니다 이에 SQL 진입점 정보를 넣는 공간으로 MDC를 선택했습니다 MDC에 SQL 진입점 정보를 저장하기 위해선 로그와 SQL 진입점에 진입점 정보를 남기기 위한 MDC 설정이 이뤄져야 합니다 로그에 진입점 정보가 남기 위해서는 다음 두 작업이 필요합니다간략한 예시 1 진입점의 각 메소드 실행 시 진입점 정보를 추가한다 MDCputhandler controllerInfo 2 스레드 변경 시 진입점 정보를 복사한다 MapString String contextMap MDCgetCopyOfContextMap MDCsetContextMapcontextMap 기본 작업 즉 SQL에 진입점 정보를 포함하고 싶다면 다음 3가지 작업이 필요합니다 진입점에 들어올 때 MDCput으로 진입점 정보를 저장한다 스레드 전환 시 MDC의 정보를 복사한다 각 DB 접근 기술별로 SQL에 진입점 정보를 추가하도록 세팅한다 1 진입점 정보 저장 우선 팀 내에 이미 있었던 SQL 진입점 설정을 위한 기본 세팅에 대해 먼저 공유 드리겠습니다 우선 1의 진입점 정보를 저장하는 방법은 다음과 같습니다 Slf4j public class MdcLoggingInterceptor implements HandlerInterceptor public static final String REQUESTCONTROLLERMDCKEY handler Override public boolean preHandleHttpServletRequest request HttpServletResponse response Object handler throws Exception if handler instanceof HandlerMethod HandlerMethod handlerMethod HandlerMethod handler String handlerName handlerMethodgetBeanTypegetSimpleName String methodName handlerMethodgetMethodgetName String controllerInfo handlerName methodName MDCputREQUESTCONTROLLERMDCKEY controllerInfo return true MDC에 저장된 정보를 모두 초기화 한다 Override public void afterCompletionHttpServletRequest request HttpServletResponse response Object handler Exception ex throws Exception MDCclear 위의 MDC에 저장된 정보를 모두 초기화하는 MdcLoggingInterceptor 인터셉터를 WebMvcConfiguration에 추가한다 Configuration public class WebMvcConfiguration implements WebMvcConfigurer Override public void addInterceptorsInterceptorRegistry registry registryaddInterceptornew MdcLoggingInterceptor 위의 코드에서 HandlerInterceptor 를 통해 MVCController 호출 시마다 MDC에 진입점 정보가 저장됩니다 인터셉터 안에서는 핸들러 실행 전preHandle 핸들러와 핸들러 메소드 이름을 추출해 MDC handler 키의 값으로 저장합니다 예를 들어 testController 클래스의 search 메소드가 실행되었다면 MDC 내 진입점 정보는 handler testControllersearch로 저장됩니다 2 스레드 전환 시 진입점 정보 복제 다음으로 2 스레드 전환 시 MDC의 정보 복제가 필요합니다 스레드가 전환될 때 기본적으로 MDC 정보가 자동으로 복제되지 않기 때문입니다 이때 스레드풀 내에서는 taskDecorator를 사용할 수 있습니다 해당 taskDecorator는 Spring의 ThreadPoolTaskExecutor 에서 제공해주는 기능으로 스레드 풀에서 실행되는 각 태스크를 래핑하는 데 사용됩니다 이를 사용하여 실행되는 태스크에 대해 threadLocal 내 정보를 복제하는 등의 커스텀 로직을 적용할 수 있습니다 Bean public ThreadPoolTaskExecutor asyncThreadPoolTaskExecutor ThreadPoolTaskExecutor taskExecutor new ThreadPoolTaskExecutor taskExecutorset taskExecutorsetTaskDecoratornew MDCCopyTaskDecorator 이곳에서 복제가 이뤄집니다 return taskExecutor 위의 코드에서 컨텍스트 복제가 이뤄지는 부분은 MDCCopyTaskDecorator 입니다 해당 클래스는 다음과 같이 짜여 있습니다 public class MDCCopyTaskDecorator implements TaskDecorator Override public Runnable decorateRunnable runnable MapString String contextMap MDCgetCopyOfContextMap return try if contextMap null MDCsetContextMapcontextMap runnablerun finally MDCclear decorate 메소드는 말 그대로 데코레이터 패턴이 사용되었습니다 MDCgetCopyOfContextMap으로 이전 스레드의 MDC Map 정보를 가져오고 전환된 스레드에 해당 Map 정보를 그대로 세팅해주는 방식입니다 해당 taskDecorator는 Spring의 ThreadPoolTaskExecutor 내에서 다음과 같이 execute 실행 시점에 decorate로 적용하도록 구현되어 있습니다 public class ThreadPoolTaskExecutor Override protected ExecutorService initializeExecutor ThreadFactory threadFactory RejectedExecutionHandler rejectedExecutionHandler BlockingQueueRunnable queue createQueuethisqueueCapacity if thistaskDecorator null executor new ThreadPoolExecutor thiscorePoolSize thismaxPoolSize thiskeepAliveSeconds TimeUnitSECONDS queue threadFactory rejectedExecutionHandler Override public void executeRunnable command Runnable decorated taskDecoratordecoratecommand if decorated command decoratedTaskMapputdecorated command superexecutedecorated 3 DB 접근 시 SQL에 진입점 정보 추가 세팅 또한 DB 접근 시점에 SQL문에 진입점 정보를 추가하도록 세팅이 필요합니다 Spring Data JPA를 사용하고 있기 때문에 하이버네이트에 세팅하는 인터셉터인 EmptyInterceptor 인터페이스를 상속받아 진입점 정보를 추가하는 인터셉터를 만들었습니다 여기서 EmptyInterceptor는 인터셉트를 원하는 메소드만을 오버라이드할 수 있습니다 여기서 onPrepareStatement의 SQL 문자열을 가져와 진입점을 추가했습니다 public class MDCRequestHandlerCommentInterceptor extends EmptyInterceptor private final transient MDCHandlerCommentAppender mdcRequestHandlerCommentAppender public MDCRequestHandlerCommentInterceptor thisSetofhandler consumer batch public MDCRequestHandlerCommentInterceptorSetString handlerKeys PreconditionscheckArgumentCollectionUtilsisEmptyhandlerKeys handlerKeys는 null 혹은 empty 이면 안됩니다 thismdcRequestHandlerCommentAppender new MDCHandlerCommentAppenderhandlerKeys Override public String onPrepareStatementString sql return mdcRequestHandlerCommentAppenderappendHandlerNamesql 그리고 SQL문 문자열에 직접 진입점 정보를 추가해준 MDCHandlerCommentAppender 클래스는 아래와 같이 구현되어 있습니다 public class MDCHandlerCommentAppender public static final SetString HANDLERMDCKEYS Setofhandler consumer batch private final SetString handlerKeys Controller 혹은 Consumer 를 나타내는 handler 값을 저장할 MDC key 들 이 중에서 값이 존재하는 하나만 사용하게 됩니다 public MDCHandlerCommentAppender thisHANDLERMDCKEYS public MDCHandlerCommentAppenderSetString handlerKeys PreconditionscheckArgumentCollectionUtilsisEmptyhandlerKeys handlerKeys는 null 혹은 empty 이면 안됩니다 thishandlerKeys handlerKeys public String appendHandlerNameString sql final OptionalString handlerMdcValue readHandlerMdcValue if handlerMdcValueisEmpty return sql if StringUtilsisBlankhandlerMdcValueget return sql return buildSqlWithHandlerInfosql refinedHandlerMdcValue private OptionalString readHandlerMdcValue return handlerKeysstream mapMDCget filterObjectsnonNull filternotStringisBlank findFirst private String buildSqlWithHandlerInfoString sql String refinedHandlerMdcValue return handler refinedHandlerMdcValue sql 코드를 보면 알 수 있듯 handler 키의 값이 존재하는지 그리고 그 값이 빈 값이 아닌지를 체크한 후 SQL문에 진입점 정보를 추가해주는 방식입니다 해당 인터셉터는 DB 설정의 yml 파일 내에서 다음과 같이 직접 지정해야 합니다 springjpapropertiessessionfactoryinterceptor fqcn전체 패키지MDCRequestHandlerCommentInterceptor 이를 통해 JPA를 경유하는 SQL문의 경우 진입점 정보를 추가할 수 있습니다 추가 작업 위의 세 단계를 통해 진입점에서 하이버네이트로 들어오는 SQL에 진입점 정보를 포함해 로그로 남길 수 있습니다 하지만 위의 작업에는 추가적인 고려가 필요한 작업들이 빠져있습니다 기본 설정 이후에도 모든 SQL 로그에 진입점 정보가 남도록 추가 설정하는 법을 소개해드리겠습니다 1 진입점 정보가 저장되도록 추가 설정 우리는 위에서 진입점에 들어올 때 MDCput으로 진입점 정보를 저장하도록 세팅했습니다 다만 전수 조사한 결과 해당 설정이 적용되지 않는 케이스가 몇 군데 있었습니다 아래는 그러한 케이스들입니다 Spring Web MVC 모듈의 Interceptor 설정 추가 팀의 프로젝트는 멀티 모듈 구조로 되어 있으며 총 10개의 모듈로 구성되어 있습니다 그 중 일부 모듈에서는 MVCController를 사용하고 있지만 Interceptor 등록이 누락된 경우가 발견되었습니다 Configuration public class WebMvcConfiguration implements WebMvcConfigurer Override public void addInterceptorsInterceptorRegistry registry registryaddInterceptornew MdcLoggingInterceptor WebMvcConfiguration 이 모듈 별로 있다 보니 빼먹은 부분이 있었던 거죠 다음 코드와 같이 설정을 추가하니 진입점 정보가 남는 것이 확인되었습니다 또한 아예 WebMvcConfiguration 세팅 자체가 안 되어있는 부분도 있어 해당 모듈에선 새 WebMvcConfiguration을 등록했습니다 작업 시 모듈별 인터셉터 설정이 모두 되었는지 유의하시기 바랍니다 Event Consumer Batch 모듈에 Logging Aspect 설정 추가 이외에도 진입점 정보가 저장되지 않는 케이스로는 이벤트 컨슈머 배치 쪽이 있습니다 지금까지의 작업은 MVCController 즉 Spring Web MVC의 컨텍스트에서만 작동하기 때문입니다 즉 해당 작업으론 Event ConsumerSQS Kafka 등 스케쥴러나 Batch 등 Spring Web MVC의 범위 밖에서 기대한대로 동작하지 않습니다 둘의 프로세스에서의 실행 흐름과 생명주기는 MVCController와는 다르기 때문에 추가 설정이 필요한 것이죠 이에 AOP를 적용해 설정을 추가해줬습니다 1 Event Consumer 모듈 Aspect Component Slf4j public class ConsumerLoggingAspect MDC 키 값으로 consumer를 사용 public static final String CONSUMERMDCKEY consumer SqsListener 어노테이션이 붙은 메소드 실행 시 아래의 코드가 수행됩니다 BeforeannotationorgspringframeworkcloudawsmessaginglistenerannotationSqsListener Spring Cloud AWS 22x 이하 BeforeannotationioawspringcloudmessaginglistenerannotationSqsListener Spring Cloud AWS 230 이후 public void beforeConsumeSqsJoinPoint joinPoint putMDCjoinPoint KafkaListener 어노테이션이 붙은 메소드 실행 시 아래의 코드가 수행됩니다 BeforeannotationorgspringframeworkkafkaannotationKafkaListener public void beforeConsumeKafkaJoinPoint joinPoint putMDCjoinPoint Scheduled 어노테이션이 붙은 메소드 실행 시 아래의 코드가 수행됩니다 BeforeannotationorgspringframeworkschedulingannotationScheduled public void beforeScheduleJoinPoint joinPoint putMDCjoinPoint private void putMDCJoinPoint joinPoint Method method MethodSignature joinPointgetSignaturegetMethod String consumerInfo joinPointgetTargetgetClassgetSimpleName methodgetName MDCputCONSUMERMDCKEY consumerInfo SqsListener 어노테이션이 붙은 메소드 실행 후 아래의 코드가 수행됩니다 AfterannotationorgspringframeworkcloudawsmessaginglistenerannotationSqsListener Spring Cloud AWS 22x 이하 AfterannotationioawspringcloudmessaginglistenerannotationSqsListener Spring Cloud AWS 230 이후 public void afterConsumeSqsJoinPoint joinPoint clearMDC MDC 내 이벤트 컨슈머 진입점 정보가 초기화됩니다 private void clearMDC MDCclear KafkaListener 어노테이션이 붙은 메소드 실행 후 아래의 코드가 수행됩니다 AfterannotationorgspringframeworkkafkaannotationKafkaListener public void afterConsumeKafkaJoinPoint joinPoint clearMDC Scheduled 어노테이션이 붙은 메소드 실행 후 아래의 코드가 수행됩니다 AfterannotationorgspringframeworkschedulingannotationScheduled public void afterScheduleJoinPoint joinPoint clearMDC 팀 내에서는 Consumer 모듈에서 Amazon SQS Kafka Spring Scheduler 기술을 사용하고 있습니다 이에 따라 SQL 진입점 종류는 총 세 가지로 SqsListener KafkaListener 그리고 Scheduled 어노테이션을 통해 구분됩니다 위의 코드와 같이 세 어노테이션을 포함한 지점들을 진입점으로 판단해 해당 부분에 AOP 설정을 적용했습니다 2 Batch 모듈 Spring Batch의 경우 역시 각각의 Job에 따라 SQL 진입점이 달라집니다 이를 관리하기 위해 Batch 모듈에도 AOP를 통해 MDC에 진입점 정보를 추가했습니다 Aspect Component public class BatchLoggingAspect MDC 키 값으로 batch를 사용 public static final String BATCHMDCKEY batch 스프링 배치의 JobLauncherrun 메소드 실행 시 아래의 코드가 수행됩니다 Aroundexecution orgspringframeworkbatchcorelaunchJobLauncherrun public Object putMDCProceedingJoinPoint joinPoint throws Throwable final Object args joinPointgetArgs final Job job Job args0 메소드의 첫 번째 인자는 Job의 인스턴스입니다 MDCputBATCHMDCKEY jobgetName return joinPointproceed 위 코드는 스프링 배치에서 실행되는 각 Job마다 해당 Job의 이름을 로깅하는 설정입니다 BatchLoggingAspect라는 Aspect 클래스를 만들었습니다 또한 포인트컷 표현식을 통해 스프링 배치의 JobLauncherrun 메소드 실행 전후를 타겟으로 설정했는데요 모든 Job의 시작 지점이 해당 메소드이기 때문입니다 팀 내 배치는 배치 마다 새로운 프로세스로 실행하기 때문에 이벤트 컨슈머와는 다르게 별도로 MDCclear 작업을 추가하지 않았습니다 만약 하나의 프로세스에서 스레드를 공유하면서 사용하는 구조라면 MDC 초기화 작업이 필요합니다 Batch에 JobScope StepScope 설정 추가 Batch 작업 로깅 설정이 완료된 후 결과 로그를 Grafana에서 확인하였습니다 예상과 달리 절반 이상의 사용자 요청에 대한 진입점이 SQL에 기록되지 않는 현상을 발견했습니다 여기엔 여러 이유가 있었는데요 주요 원인 중 하나는 빈 스코프 설정의 누락 과 관련되어 있었습니다 Spring Batch 에서는 Job과 Step의 실행 동안 생성되는 빈의 수명주기를 관리하기 위한 JobScope와 StepScope 어노테이션이 제공됩니다 JobScope는 Job이 시작될 때 빈이 생성되고 Job이 끝날 때 빈이 소멸됩니다 StepScope는 마찬가지로 step의 생명주기를 따릅니다 Bean public ItemReaderA AReader IteratorA iterator ARepositorygetAiterator 생성 시점에 sql문 호출 return if iteratorhasNext return iteratornext return null 그런데 팀 내 배치 중 Reader의 빈 생성 시점에 sql문이 만들어지고 있으며 빈 스코프 지정이 안 된 경우가 있었습니다 이때 Reader 빈은 특별한 스코프 어노테이션이 없기 때문에 ApplicationContext가 시작될 때 생성됩니다 따라서 Job이 시작되기 전에 DB 접근이 이뤄집니다 JobLauncherrun 실행 이전에 빈이 만들어지고 위 코드에서는 빈이 만들어질 때 DB에 SQL문이 날아갈테니 당연히 MDC에 Batch job 이름이 들어갈 리 없습니다 따라서 현재 상황에서 JobLauncherrun 시점에 추가한 Job 이름 정보가 SQL에 추가되려면 DB에 접근하는 모든 시점의 빈 스코프가 최소 JobScope 이상이어야 합니다 그렇지 않으면 MDC 설정이 완료되기 전에 Bean이 생성되면서 SQL 쿼리가 실행될 수 있습니다 다만 reader writer processor bean 초기화 시 bean 생성 시점의 쿼리는 MDC 로그가 남지 않지만 구현체 안에서 실행되는 실제 객체service는 MDC 로그가 잘 남습니다 이는 구현체 내부의 로직이 MDC 설정 후 즉 JobLauncherrun 후 실행되기 때문입니다 따라서 구현체 내에서의 로직은 MDC 설정이 완료된 상태에서 SQL 쿼리가 실행되기 때문에 해당 로그에는 MDC 정보가 정상적으로 남게 됩니다 요약하자면 아래 Bean Component 등을 통한 빈 생성시 bean 객체 생성 시점에 남기는 SQL 로그를 보고 싶다면 bean 스코프 지정이 필요합니다 이에 Job 설정 안에서 DB 호출 지점이지만 bean 스코프가 정의되지 않은 경우 다음과 같이 스코프를 추가 정의했습니다 JobScope step StepScope reader writer processor tasklet 2 MDC 정보의 복제를 위한 추가 설정 사실 위에서 설정해줬던 taskDecorator 는 스레드풀 그중 Spring의 threadPoolTaskExecutor 에서만 적용 가능합니다 따라서 이외의 스레드 전환 상황에 대해서도 설정이 필요합니다 그렇다면 어떤 상황에서 스레드가 전환될까요 다음과 같은 케이스를 생각해볼 수 있습니다 스레드풀 java ExecutorService 등 비동기 작업 Executor ForkJoinPool 등 Java의 Future와 CompletableFuture 스케쥴러 작업 Reactive 프로그래밍 Reactor 등 요청마다 새로운 스레드나 재사용되는 스레드를 사용하는 서버 캐시 Caffeine 등 그리고 전수 조사한 결과 해당 설정이 적용될 수 없는 다양한 케이스를 마주하게 되었습니다 taskDecorator 미설정된 부분 추가 설정 우선 threadPoolTaskExecutor 사용처 중 taskDecorator이 설정되지 않은 경우가 있었습니다 threadPoolTaskExecutor를 검색해 추가 설정만 하는 간단한 작업이었습니다 이외에도 taskExecutor 중 threadPoolTaskExecutor로 변환이 쉽게 가능한 케이스는 전부 변경했습니다 taskExecutorsetTaskDecoratornew MDCCopyTaskDecorator 문제는 ExecuteService 였습니다 ExecuteService 에서 TaskExecutor로 다음은 Executor 구조 UML입니다 주요 메소드만 표시한 약식 UML이니 가볍게 봐주세요 방금 본 세팅은 말씀드렸듯 ThreadPoolTaskExecutor에서만 적용 가능합니다 스레드풀은 대체로 Executor 인터페이스를 기반으로 합니다 그리고 일반적으로 javautil의 ExecutorService나 Spring core의 TaskExecutor 중 하나를 기본 스레드풀로 선택하여 사용하는 경우가 많습니다 그리고 ThreadPoolTaskExecutor은 TaskExecutor를 상속받은 구현체입니다 따라서 ExecuteService를 사용하는 경우 MDC 정보를 복제하는 것이 기본적으로 불가능하다는 점을 알아두셔야 합니다 물론 방법은 있습니다 데코레이터 패턴을 이용해 우리만의 CustomExecutorService를 만드는 겁니다 public class CustomExecutorService implements ExecutorService Override public void executeNonNull Runnable command Runnable decorate taskDecoratordecoratecommand executorServiceexecutedecorate 이외 모든 ExecutorService 메소드들에 적용하면 됩니다 위의 CustomExecutorService는 ExecutorService의 모든 메소드를 오버라이드하여 TaskDecorator를 적용하게 됩니다 처음에는 이러한 방식으로 구현했습니다 하지만 팀 내 스레드풀 설정이 ExecutorService와 ThreadPoolTaskExecutor로 혼용되어 있는 것보단 기존의 ExecutorService를 ThreadPoolTaskExecutor로 대체하는 편이 좋지 않겠냐는 코드 리뷰를 받게 되었습니다 CustomExecutorService를 쓸 시 앞으로 코드 관리 포인트가 증가할 것도 이유였습니다 이에 기존의 ExecutorService를 모두 ThreadPoolTaskExecutor로 전환하는 작업을 진행하게 되었습니다 이쯤 되니 SQL 진입점이 문제가 아니고 스레드풀 전체를 건드려야 하는 일이 되었습니다 팀에선 다음과 같은 ExecutorService가 bean으로 등록되어 있었습니다 ExecutorService executorService ExecutorsnewFixedThreadPoolcorePoolSize ExecutorService executorService new DelegatingSecurityContextExecutorServiceExecutorsnewFixedThreadPoolcorePoolSize ExecutorService executorService ExecutorsnewScheduledThreadPooltaskPoolSize 이 부분은 Executors에서 제공하는 각 스레드풀 픽스쳐 설정을 비교해 동일하게 세팅했습니다 해당 세팅 시 적정 corePoolSize QueueCapacity MaxPoolSize 등을 함께 고민하고 스레드 덤프 분석을 통해 재세팅 진행했습니다 Beanname bmartTaskExecutor newFixedThreadPool public ThreadPoolTaskExecutor bmartTaskExecutor ThreadPoolTaskExecutor taskExecutor new ThreadPoolTaskExecutor taskExecutorsetCorePoolSizecorePoolSize 스레드풀 세팅 taskExecutorsetTaskDecoratornew MDCCopyTaskDecorator return taskExecutor DelegatingSecurityContextAsyncTaskExecutor는 Spring에서 지원하는 클래스입니다 멀티 스레드 환경에서 Spring Security를 사용할 수 있는 저수준 추상화를 제공하는 클래스입니다 스레드풀 세팅은 bmartTaskExecutor와 동일합니다 Bean DelegatingSecurityContextExecutorService public DelegatingSecurityContextAsyncTaskExecutor securityContextAsyncTaskExecutorThreadPoolTaskExecutor bmartTaskExecutor return new DelegatingSecurityContextAsyncTaskExecutorbmartTaskExecutor Beanname scheduled public TaskScheduler scheduled ThreadPoolTaskScheduler scheduler new ThreadPoolTaskScheduler 스레드풀 세팅 return new DelegatingMDCTaskSchedulerscheduler 여기서 DelegatingSecurityContextAsyncTaskExecutor는 Spring에서 지원하는 클래스로 기존의 DelegatingSecurityContextExecutorService와 동일하게 구동됩니다 반면 TaskScheduler는 taskDecorator 적용이 불가능합니다 위의 ExecutorService와 동일한 이유입니다 이에 DelegatingMDCTaskScheduler을 데코레이터 패턴으로 직접 구현했습니다 이때 기존 taskDecorator 구현 방식과 달리 AbstractDelegatingMDCSupport를 추가했습니다 구조는 다음과 같습니다 또한 코드는 다음과 같습니다 public class DelegatingMDCTaskScheduler extends AbstractDelegatingMDCSupport implements TaskScheduler private final TaskScheduler delegate public DelegatingMDCTaskSchedulerTaskScheduler delegate thisdelegate delegate Override public ScheduledFuture scheduleRunnable task Trigger trigger return delegateschedulewraptask trigger public abstract class AbstractDelegatingMDCSupport protected final Runnable wrapRunnable delegate MapString String contextMap MDCgetCopyOfContextMap return try if contextMap null MDCsetContextMapcontextMap delegaterun finally MDCclear protected final T CallableT wrapCallableT delegate MapString String contextMap MDCgetCopyOfContextMap return try if contextMap null MDCsetContextMapcontextMap return delegatecall finally MDCclear 여기서 AbstractDelegatingMDCSupport 클래스가 추가되었는데요 중간에 AbstractDelegatingMDCSupport 를 둔 구현 방향은 Spring Security의 AbstractDelegatingSecurityContextSupport 의 방식을 참고했습니다 Caffeine 캐시 사용처 추가 설정 이외에도 Caffeine 캐시 사용처 중 스레드풀 설정이 추가적으로 필요한 경우가 있었습니다 public class CacheConfiguration Bean Qualifiermemory public CacheManager inMemoryCacheManager CaffeineCacheManager caffeineCacheManager new CaffeineCacheManager CaffeineObject Object caffeine CaffeinenewBuilder executorcached해당 설정이 빠진 캐시 설정마다 executor를 추가했습니다 caffeineCacheManagersetCaffeinecaffeine return caffeineCacheManager Caffeine 캐시 사용처에 taskExecutor 세팅이 이뤄지지 않은 경우 taskDecorator 설정된 taskExecutor를 추가 설정했습니다 MonoFlux elastic 전환 Reactor의 MonoFlux에서 사용되는 Scheduler는 각각 elastic parallel이었고 당연히도 taskDecorator 세팅이 적용되지 않아 있었습니다 MonofromCallable getAAId publishOnSchedulerselastic FluxfromIterablerules publishOnSchedulerselastic parallel runOnSchedulersparallel 이에 elastic의 경우 사용처마다 SchedulersfromExecutorbmartTaskExecutor 를 받도록 전환했습니다 Flux에서 사용된 parallel은 CompletableFuture로 전환하거나 동기로 변환될 예정입니다 3 DB 접근 기술 별 추가 세팅 지금까지의 작업은 DB 접근이 하이버네이트를 통해 이뤄진다는 가정 하에 이뤄졌습니다 사실 위에서 얘기하지 않은 부분이 있습니다 바로 실제로 SQL에 진입점 정보를 추가해 쿼리를 날리는 부분입니다 그리고 팀 코드에서는 여러 이유로 JPA와 JdbcTemplate를 함께 사용하고 있었습니다 커스텀 JdbcTemplate 작업 이에 JdbcTemplate namedJdbcTemplate을 통한 쿼리에도 동일하게 진입점 정보를 넘기기 위한 CustomJdbcTemplate을 구현하는 작업을 진행했습니다 더불어 bean 등록되지 않고 클래스 내에서 new로 선언된 JdbcTemplate을 bean 등록된 CustomJdbcTemplate로 교체했습니다 public class CustomJdbcTemplate private final NamedParameterJdbcTemplate namedParameterJdbcTemplate private final JdbcTemplate jdbcTemplate private final MDCHandlerCommentAppender mdcHandlerCommentAppender public CustomJdbcTemplateNamedParameterJdbcTemplate namedParameterJdbcTemplate thisnamedParameterJdbcTemplate namedParameterJdbcTemplate thisjdbcTemplate namedParameterJdbcTemplategetJdbcTemplate thismdcHandlerCommentAppender new MDCHandlerCommentAppender NamedParameterJdbcTemplate public int updateString sql MapString paramMap final String refinedSql appendHandlerNamesql return namedParameterJdbcTemplateupdaterefinedSql paramMap jdbcTemplate public T ListT queryString sql Object args RowMapperT rowMapper final String refinedSql appendHandlerNamesql return jdbcTemplatequeryrefinedSql args rowMapper private String appendHandlerNameString sql final String refinedSql removeLastSemicolonsql return mdcHandlerCommentAppenderappendHandlerNamerefinedSql private String removeLastSemicolonString sql if sqlendsWith return sql return sqlsubstring0 sqllength 1 Batch 내 CustomPagingQueryProvider 작업 Batch 내 job 중 여러 이유로 MDC 진입점 정보 추가가 이뤄지지 않는 경우에도 처리가 필요합니다 JdbcPagingItemReaderBuilder JdbcBatchItemWriterBuilder를 사용하는 경우 등 입니다 MySqlPagingQueryProvider mySqlPagingQueryProvider new MySqlPagingQueryProvider mySqlPagingQueryProviderset PagingQueryProvider customPagingQueryProvider new CustomPagingQueryProvidermySqlPagingQueryProvider public class CustomPagingQueryProvider implements PagingQueryProvider private final PagingQueryProvider pagingQueryProvider private final MDCHandlerCommentAppender mdcHandlerCommentAppender new MDCHandlerCommentAppender public CustomPagingQueryProviderPagingQueryProvider pagingQueryProvider thispagingQueryProvider pagingQueryProvider Override public void initDataSource dataSource throws Exception pagingQueryProviderinitdataSource Override public String generateFirstPageQueryint pageSize String firstPageQuery pagingQueryProvidergenerateFirstPageQuerypageSize return mdcHandlerCommentAppenderappendHandlerNamefirstPageQuery JdbcPagingItemReaderBuilder 케이스의 경우 데코레이터 패턴의 CustomPagingQueryProvider를 구현한 후 queryProvider에 진입점 값을 추가하는 형태로 구현했습니다 마무리 지금까지 로그 및 SQL에 진입점 정보를 추가하는 여정이었습니다 단순히 쿼리 앞단에 진입점 정보를 추가하면 되는 간단한 작업이지만 과정에서 여러 시행착오를 겪었습니다 여기까지 작업하며 팀원들의 끊임없는 지원과 피드백을 받았는데요 특히 페어 작업하면서 많이 가르쳐주신 순규 님에게도 감사드립니다,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/13429/
네이버에서 이젠 프로야구 못 본다…돈 내고 본다?,,,https://n.news.naver.com/mnews/article/374/0000365837?sid=105
[SK뉴비이야기] 대화를 코드로 읽는 엔지니어와의 대화,안녕하세요 개발자들을 위한 영감의 바다 데보션입니다 사람의 말을 이해하고 또 사람의 말을 분석하기 위해서는 대화엔진을 활용한 참 많은 기술들이 필요한데요 그 중에서도 언어 모델을 고도화해나가고 NLP 성능을 개선시키기 위해 노력을 하고 있는 2년차 개발자 윤주성님을 만나보았습니다 입사후 1년여동안 적응과정과 또 입사하기까지의 다양한 경험에 관한 이야기 지금 만나볼까요 Q1 안녕하세요 먼저 간단한 소개 부탁드립니다 안녕하세요 작년에 SK 텔레콤으로 이직해 현재 ATech 대화 Foundation Model팀에서 근무하고 있는 윤주성입니다 작년 5월에 입사해 이제 막 1년이 되었구요 LLM 언어모델 개발 NLP task 개발 등의 업무를 담당하고 있습니다 Q2 입사전 경력이 궁금합니다 저는 자연어처리와 머신러닝을 전공했고요 컴투스 라는 회사에서 사회생활을 시작했습니다 그러다 전공을 살려 근무할 수 있는 기회가 있어 네이버로 이직을 하게되었습니다 네이버에서는 블로그 검색 모델링 어뷰징유저 탐지등의 업무를 진행하면서 실무에 대한 감각을 기를 수 있었습니다 사실 대화엔진을 연구하면서 중요한 것은 얼마나 많은 그리고 양질의 데이터를 처리해보고 모델링해본 경험을 통해 개발을 진행할수 있느냐가 핵심인데요 그런면에서 개발 주니어로써 정말 많은 것을 보고 배울 수 있었던 것 같습니다 Q3 SKT에서 개발자로 일한다는 것은 어떤 느낌일지 궁금합니다 좋은 동료들에게 좋은 영감을 받을수 있는것 같습니다 경력직의 경우 입사후 바로 실무에 투입되는데요 LLM 성능 고도화 등의 업무를 바로 진행함에 있어서 좋은 동료들이 든든한 버팀목이 되는 것 같습니다 좋은 동료와 인프라 덕분에 올해 한국어 LLM 성능 극대화한 더 큰 규모의 Foundation Model을 출시예정이고 국내 top 수준의 기술을 선보일 것으로 기대됩니다 Q4 SKT만의 강점이 있다면 어떤게 있을까요 아무래도 여러 거점 오피스를 활용 가능하다는 것이 제게는 강점으로 다가왔습니다 가끔 일하던 메인 오피스를 벗어나 기분전환도 하고 꼭 회의가 있지 않은날 집에서 가까운 오피스를 선택해서 가면 노트북 하나만 들고 가더라도 모든 것이 구비되어 있어서 업무를 하기 참 좋다는 생각을 합니다 Q5 개발 중니어로써 개발 주니어 혹은 중니어에게 해주고싶은 조언이 있다면 저는 개발 중니어로써 많은 사람들을 만나고 성장의 에너지를 얻기를 추천합니다 사실 요즘 여러 기술들이 정말 범람하고 있잖아요 이럴때 일수록 좋은 정보를 공유하고 또 공유 받으면서 함께 성장하는 것이 진심으로 중요하지 않나 싶어요 Q6 올해도 이제 반이 넘어가고 있는데요 윤주성님의 남은 올해의 반을 보낼 계획이 궁금합니다 개인적으로 올해는 저에게 무척 특별한 한해가 될것 같습니다 이직 후 본격적으로 굵직한 업무를 진행하게 된 것은 물론 데보션 전문가로 참여를 하게되었어요 사실 개발자로써 다른 개발자와의 소통은 필요한 것을 넘어 필수라고 생각하는데요 다행히 폭 넓게 참여할 수 있는 기회가 있어서 참여를 하게되었어요 이직을 한 입장에서는 적응에 대해서 시간이 걸리는데 이런 프로그램 덕분에 소프트 랜딩을 할수 있지 않나 싶어요 그리고 사실 바로 얼마전 한 아이의 아빠가 되어 하루하루 사뭇다른 책임감과 설렘으로 지내고 있어요 회사에서도 또 가정에서도 새로운 출발을 하고 있는 것 같아서 몹시 설레고 있고 또 시작한 일들을 잘 마무리하면서 예쁜 가정의 멋지고 든든한 아빠가 되는게 올해 남은기간의 목표랍니다 사람의 언어를 코드로 분석하고 또 다시 사람에게 필요한 서비스로 만들어내는 윤주성 매니저님 앞으로 SK텔레콤의 다양한 서비스에서 보다 정교하게 대화를 이해하고 맞춤형 정보를 제공해줄수 있길 기대할께요 주성님에 질문하거나 궁금한 내용이 있다면 아래 데보션 전문가 페이지를 참고하세요 이미지 누르시면 바로 이동됩니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164947&boardType=techBlog&searchData=&page=&subIndex=
2023 카카오 신입 공채 iOS 온보딩 회고,들어가며안녕하세요 오픈링크 iOS셀의 forest입니다 저는 지난 12월 2023 블라인드 신입공채로 카카오에 합류하게 되었습니다 공채 뉴크루들은 입사하여 약 1달간의 공통 온보딩 과정을 마친 후에 6주 동안 직무별 기술 온보딩 프로그램을 진행하게 됩니다 이 중 제가 경험한 iOS 직무 기술 온보딩 프로그램에 대해서 회고해보려 합니다 iOS 온보딩은 이렇게 진행됩니다온보딩은 Swift 프로그래밍 저자이자 iOS 부트캠프 야곰 아카데미의 리더인 야곰이 담당해 주셨습니다 Swift를 처음 접하는 크루도 있기에 Swift 언어의 기초부터 시작해 iOS의 프레임워크와 뷰 컴포넌트들을 학습하는 단계부터 시작했습니다온보딩 프로그램은 온라인으로 진행되었는데요 매주 2회의 라이브 세션과 활동학습 그리고 프로젝트로 이루어져 있습니다 라이브 세션라이브 세션은 야곰이 직접 진행해 주셨습니다 단순히 지식을 전달하는 방식이 아닌 학습 주제에 대한 자료를 뉴크루들이 먼저 예습하고 라이브 세션 중에 각자 토론 및 질문을 통해 애매하게 알고 있는 것을 확실히 내 것으로 만들 수 있는 시간이었습니다 활동학습매주 새로운 주제에 대해 페어로 미션을 같이 수행하면서 개념을 익히고 활용하는 시간이었습니다 미션 결과를 다른 페어 그룹과 공유하면서 이해가 어려운 부분에 대해서 질문을 하거나 참고하는 시간을 가졌습니다 프로젝트온보딩에서 4개의 프로젝트를 진행했습니다 라이브세션과 활동학습에서 배웠던 주제를 활용하고 추가적으로 필요한 지식을 자기 주도적으로 학습하여 앱을 만들게 됩니다프로젝트는 스텝 바이 스텝으로 진행됩니다 먼저 스텝의 요구사항이 공개되면 코드를 작성해서 리뷰어에게 PR을 보냅니다 리뷰어가 리뷰를 남기면 저희는 그 리뷰를 반영하여 코드를 보완하고 스텝을 마무리한 후 다음 스텝으로 넘어가게 됩니다마지막 미션 프로젝트를 제외하고는 2명씩 짝을 이루는 페어 프로그래밍으로 프로젝트를 진행했습니다 성적관리첫 프로젝트로는 연락처를 관리할 수 있는 콘솔 앱을 만들었습니다이 프로젝트를 통해 Git의 사용법과 Swift의 기본 문법에 익숙해질 수 있었고 단위 테스트Unit Test를 활용한 테스트 주도 개발TDD을 경험할 수 있었습니다 로그인프로필카카오톡의 로그인 화면과 프로필 화면을 간단한 앱으로 만들었습니다모바일 앱 아키텍처로써 가장 기본적인 개념인 MVC 아키텍처를 배우고 적용하였습니다또한 iOS 애플리케이션의 생명주기인 App Life Cycle과 뷰의 생명주기 View Life Cycle을 학습하고 각 Life Cycle에서 발생하는 이벤트를 활용하여 코드를 작성했습니다 아울러 뷰 컴포넌트를 사용할 때 Apple의 디자인 철학이 잘 담겨있는 HIGHuman interface guideline를 프로젝트에 적용하는 경험을 할 수 있었습니다 만국박람회만국박람회의 개최정보와 출품작 정보를 제공해 주는 앱을 제작했습니다아이폰 사용자라면 익숙하실 손쉬운 사용 설정과 연관이 있는 접근성Accessibility에 대해서 학습했고 글꼴에 Dynamic Type을 적용했습니다 이 과정에서 세로모드Portrait와 가로모드Landscape를 모두 지원하기 위해 화면의 모드가 변할 때 오토레이아웃Auto layout이 화면을 동적으로 변경하도록 구현하는 방식에 대해 학습했습니다 박스오피스REST API를 사용하여 박스오피스 정보를 보여주는 앱입니다네트워크 통신을 위한 네트워크 모델 객체 설계 HTTP Request와 JSON 데이터를 파싱을 위한 요청응답 타입 설계 통신으로 받아온 데이터를 다양한 뷰 컴포넌트를 사용하여 뷰를 표현하는 과정 등 정말 많은 경험을 할 수 있는 프로젝트였습니다 구현 과정에서 Diffable Data Source와 Compositional Layout을 활용한 Modern Collection View를 적극적으로 적용해 화면을 구현했습니다 기술적 고민6주간의 온보딩 동안 저와 iOS 뉴크루 동기들은 여러 기술적인 고민을 했는데요 그중 가상 인상 깊었던 주제를 소개드리려 합니다 네트워크와 무관한 테스트 작성하기위에서 소개드린 박스오피스 앱에서는 REST API를 사용하기 위해 앱 내부에서 네트워크 통신을 수행하고 결과로 받은 데이터를 가공하여 화면에 정보를 보여줍니다 그런데 테스트를 진행할 때마다 매번 실제 데이터를 가져오는 서버와 통신을 한다면 아래와 같이 몇 가지 문제가 발생하게 됩니다 테스트 과정의 속도가 느린 단점이 있습니다 네트워크 환경에 따라 통신이 실패할 수 있습니다 테스트 결과가 데이터를 가져오는 서버의 상태에 의존합니다 저희는 이러한 문제를 해결할 수 있는 방법을 찾고 싶었고 저희가 생각 중이던 테스트 방식은 실제로 서버와 통신을 하지 않으면서 원하는 데이터를 무결하게 받을 수 있는 방식이었습니다이러한 테스트 과정의 문제를 해결할 수 있도록 애플이 이미 구현해 놓은 URLProtocol이라는 추상클래스가 있었습니다 iOS에서의 URL을 사용한 네트워크 통신이 어떻게 이루어지는지 먼저 살펴보겠습니다 iOS의 Foundation 프레임워크에서는 URL통신을 위해 URLSession이라는 클래스를 제공합니다 이 URLSession에 URL request를 전달하면 URLSessionDataTask라는 객체가 만들어지고 이 객체를 사용해 데이터 통신이 진행됩니다 이러한 URLSession 데이터 통신은 기본적으로 우리가 익히 잘 알고 있는 HTTPS와 같은 통신 프로토콜을 준수합니다이때 위에서 언급한 URLProtocol 추상클래스를 상속받아 사용하면 저희가 필요했던 더미 데이터를 반환해 내는 저희만의 가짜 URLProtocol을 만들어 낼 수 있고 이를 테스트 과정에 적용하여 네트워크 통신을 하는 척 애플리케이션을 동작할 수 있게 됩니다 코드를 같이 살펴볼까요 먼저 URLprotocol 클래스를 상속한 MockURLProtocol을 만듭니다 이 클래스는 requestHandler 프로퍼티를 통해서 외부에서 더미 데이터를 주입받을 수 있고 URLprotocol 클래스에서 상속받은 startLoading메서드를 오버라이드해서 주입받은 데이터를 반환하도록 구현했습니다NetworkProvider는 네트워킹을 담당하는 모델의 구현 사항을 담은 프로토콜입니다 이 프로토콜을 채택한 구현체이자 저희가 테스트에 사용할 네트워크 객체인 MockNetworkingProvider의 session 변수에 위에서 만들어 놓은 MockURLProtocol을 설정하면 전체 코드가 완성됩니다이러한 방법으로 네트워크 동작 및 실제 서버의 데이터가 필요한 코드에서 기존에 사용하던 네트워크 모델 객체 대신 MockNetworkingProvider를 주입하여 사용하면 네트워크와 무관한 테스트를 할 수 있게 되는 것이죠 느낀 점자기주도 학습온보딩 기간 동안 교수자가 일방적으로 지식을 전달하는 시간은 거의 없었고 저희가 스스로 학습을 하는 시간이 대부분이었습니다 프로젝트를 수행하기 위해 알아야 할 지식에 대한 키워드만 제공받아 관련된 문서나 영상자료를 찾아보고 서로에게 물어보고 토론하는 과정을 통해 지식을 습득해 나갔습니다카카오의 문화 중 스스로 몰입하고 주도적으로 일합니다라는 가치가 있는데요 자기 주도적인 학습을 통해 프로젝트를 수행하는 과정을 경험하며 앞으로 현업에서도 주도적으로 공부하고 일하는 개발자가 될 수 있는 힘을 기를 수 있었다고 생각합니다페어 프로그래밍온보딩의 프로젝트는 페어 프로그래밍으로 진행되었습니다 페어 프로그래밍은 구현의 방향을 지시하는 내비게이터Navigator와 손으로 코드를 작성하는 드라이버Driver로 역할이 나뉘어 코드를 작성합니다처음에는 다른 사람과 같은 화면을 보면서 코드를 작성해 나가는 경험은 어색하고 불편하기도 했습니다 내비게이터가 말하는 방향을 드라이버가 이해하지 못하거나 내비게이터가 자신의 머리에 있는 생각을 드라이버에게 말로 전달을 잘하지 못하는 어려움도 있었습니다하지만 시간이 지나면서 페어 프로그래밍의 장점을 알게 되었습니다 작성한 코드에 대해서 페어 모두가 완벽히 이해를 하게 되었고 아는 것을 다른 사람에게 표현할 수 있는 방법을 배우는 데에 큰 도움이 되었습니다 그래서 페어 프로그래밍 과정이 저희가 빠르게 성장하는데에 아주 큰 도움이 되었다고 느낍니다 마치며6주간의 기술 온보딩을 마무리하면서 3월 10일 카카오 판교 아지트에서 온보딩 프로그램을 회고하는 랩업 미팅이 열렸었습니다 여기서 저희가 배운 것과 경험한 것을 summer의 멋진 프레젠테이션을 통해 다른 직무의 뉴크루들과 공유할 수 있었습니다 여기까지가 뉴크루 forest의 2023년 신입공채 iOS 온보딩 프로그램 회고였습니다좋은 온보딩 프로그램으로 저희가 카카오에 잘 적응할 수 있게 해 주신 신입온보딩 TF를 비롯한 카카오 관계자분들 그리고 기술 온보딩을 이끌어 주신 야곰 키오 수박에게 감사인사를 드리고 싶습니다 마지막으로 같이 온보딩을 기간을 함께하고 지금은 서로 다른 곳에서 iOS 개발자로서 열심히 일하고 계신 summer volga john 모두 좋은 개발자가 되기를 기원합니다 긴 글 읽어주셔서 감사합니다 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/03/28/2023-new-krew-onboarding-ios/
AKS로 쿠버네티스 시작하기 - Azure Kubernetes Service 부터,IT 분야에서의 트리거 요인 IT 분야에서 일하다 보면 새로운 기술을 접하게 됩니다 새로운 기술을 접하는 관점을 저는 실용적으로 활용할 수 있느냐에 관심을 가지고 있습니다 쿠버네티스 시작하기 작년말에 쿠버네티스를 시작하고자 하는 마음을 먹었습니다 일단 개념을 학습하고 IT 분야에서의 필자의 트리거 요인인 실용적으로 활용할 수 있는지에 대한 점검리스트를 만들고 확인해 보았습니다 일상의 업무가 있지만 새로운 것을 접하고 배우며 깨닫는 과정은 흥미로웠습니다 쿠버네티스를 접하게 된 것에 대한 감사를 느끼게 되었고 실무 적용이라는 목표를 세우게 되었습니다 쿠버네티스의 필요성 컨테이너는 물류 운송에 경제성 신속성 안전성을 최대로 충족시키는 표준화된 효율적인 운송도구로 알려져 있습니다 애플리케이션도 컨테이너로 표준화를 하면 물류 수송처럼 쿠버네티스를 통해 큰 변수 없이 손쉽게 배포할 수 있는 시대가 되었습니다 쿠버네티스는 컨테이너화된 애플리케이션을 배포 관리 확장할 때 요구되는 다수의 수동 프로세스를 자동화하는 오픈소스 컨테이너 오케스트레이션 플랫폼입니다 요즘은 쿠버네티스에서 동작하는 솔루션들이 많이 양산되는 상황으로 클라우드 환경에서 솔루션애플리케이션을 가동시키기 위한 표준 환경 또는 기반 환경으로 인식되고 있습니다 애플리케이션에 대한 배포 관리 확장을 자동화해 주기 때문에 개발 역할에서의 배포와 운영 역할에서의 관리확장 측면에서 많은 이점을 제공하는 DevOps 툴입니다 블로그 방향성 실무 프로젝트에 적용하는 관점으로 실제 동작하는 내용의 실용적인 과정을 중심으로 단계별로 지속적으로 작성을 진행하고자 합니다 create 단계의 상세 매뉴얼 형식보다는 생성 결과를 기준으로 설명하고 활용을 위한 이해 중심으로 기술합니다 참고로 리소스 create 단계에 대한 상세 절차는 Azure 링크로 제공하겠습니다 CSP 서비스로 시작하기 쿠버네티스를 조금 듣기 시작하던 몇 년 전부터 설치 과정에 많은 노력과 어려움이 있었습니다 클라우드 환경에서 쿠버네티스 서비스로 구성하는 CSP서비스를 활용하는 것이 훨씬 수월하고 편리한 시대가 되어 활용이 권장되고 있습니다 AKSAzure Kubernetes Service 클러스터 만들기 쿠버네티스는 여러 대의 서버로 구성한 클러스터라는 환경에 동작시키고자 하는 애플리케이션솔루션을 배포해 주는 도구입니다 이 글에서는 Azure Kubernetes Service를 이용해서 쿠버네티스 클러스터를 생성합니다 AKS 클러스터 구성 Azure에서는 AKS리소스를 생성하면 아래와 같이 Worker Node를 생성해 주며 사용자가 관리할 수 있습니다 AKS리소스 생성을 하면 MC로 시작하는 이름의 리소스 그룹내에 Azure가 관리해 주는 별도의 리소스가 자동으로 생성됩니다 k8s에서 master node로 알려진 영역으로 Azure에서는 Control Plane 이라고 부르고 있습니다 참고로 Azure에서는 Control Plane은 사용자로부터 추상화된 관리형 Azure 리소스로 무료로 제공됩니다 AKS 클러스터에 연결된 노드에 대해서만 비용을 지불합니다 출처 AKS의 Kubernetes 핵심 개념 고객관리의 리소스 그룹과 Azure 관리의 리소스 그룹 예시 AKS 클러스터를 RGAOPMGMT 리소스 그룹에 생성했더니 RGAOPMCMGMT 리소스 그룹이 자동 생성되고 Control plane의 구성요소가 구축되었습니다 Customermanaged Worker node 우선 aks 클러스터 생성 결과를 중심으로 구성요소를 확인할 수 있습니다 aks 클러스터의 리소스 구성을 통해 k8s의 구성요소에 대한 전체적인 윤곽은 보이실 것 같습니다 아래의 예시는 AKSAOPMGMT 라는 AKS를 생성한 결과 내용입니다 Essentials subscriptionlocationresource group 등 관리적인 내용을 제외하면 관심을 가져야 할 핵심적인 항목은 version api server address network type node pools 정도가 되겠습니다 Kubernetes resources k8s 리소스 측면에서는 Azure는 namespaces workloads services and ingresses configuration 등의 정보를 제공합니다 Properties 속성 정보에서는 Essentials 내용 이외로 보면 ConfigurationAuto Upgrade Type과 Networking 구성의 상세 내용을 제공합니다 aks 리소스 생성 결과를 통해서 k8s에서 중요한 정보들은 확인이 되었습니다 자세한 aks 클러스터 생성 방법 링크 AKS 클러스터 만들기 절차 Kubernetes resources 상세 Namespaces 필요성 여러 개의 팀이나 프로젝트에 걸쳐서 많은 사용자가 있는 환경에서 구분할 수 있도록 사용 정의 AKS 클러스터를 분할하고 리소스에 대한 액세스의 만들기 보기 및 관리를 제한하기 위해 논리적으로 구분하여 그룹핑하는 방법 네임스페이스 내에서 리소스 이름은 유일하지만 네임스페이스간에는 유일할 필요 없음 네임스페이스는 서로 중첩 불가 쿠버네티스 리소스는 하나의 네임스페이스에만 속함 동일한 네임스페이스에서 리소스를 구별하기 위해서는 레이블 사용 가능 예시 kubesystem 네임스페이스 엔지니어링 네임스페이스 Namespace 설명 default 기본으로 만들어지는 네임스페이스 소규모 환경에서 애플리케이션의 논리적 구분 불필요 시 사용 kubesystem 핵심 리소스와 네트워크 기능이 있는 네임스페이스 사용자 애플리케이션은 이 네임스페이스에 포함하지 않음 kubepublic 클러스터 전체에서 리소스를 표시하는 데 사용일반적으로는 사용하지 않음 모든 사용자가 볼 수 있음 Workloads Services and Ingresses Service들과 Ingress들의 내용을 볼 수 있습니다 서비스들 중에는 외부IP가 있는 것이 있고 Cluster IP만 있는 것이 있습니다 Configuration Config maps와 Secrets의 내용을 볼 수 있습니다 Azuremanaged Control plane RGAOPMGMT 리소스 그룹에 Worker node를 생성했기 때문에 Control plane은 RGAOPMCMGMT 리소스 그룹으로 자동 생성됩니다 자동으로 생성된 Control Plane 내용은 아래와 같습니다 Public IP address Load balancer Virtual machine scale set Network security group 등이 눈에 띕니다 Public IP address outbound용 IP 주소입니다 AKS의 Worker Node는 Cluster의 생성 업그레이드 패치 모니터링 등을 위한 kubeapiserver와의 통신 Node 생성을 위한 OS Image Cluster 구성을 위한 Container Image 등이 필요하므로 Azure Global Service 및 클러스터 생성에 필요한 Service로의 Outbound가 필요합니다 Load balancer kubernetes 라는 명칭으로 생성이 되어 있습니다 Service를 Load Balancer 타입으로 구성하면 생성 건별로 Public IP 하나씩 생기는 것을 볼 수 있게 됩니다 Virtual machine scale set Worker node 의 컴퓨팅 사양과 함께 Available zone이 123 에 걸쳐 있는 구성과 Scaling에 대한 정보를 볼 수 있습니다 정리 AKS라는 CSP서비스로 k8s클러스터를 만드는 과정은 단순하고 쉽게 가능합니다 만들고 나서 보니 k8s를 구성하는 많은 요소들을 CSP가 다 알아서 생성해 주니 편리한 세상이 된 것 같습니다 처음부터 k8s에 대한 세부 구성요소를 언급하기 보다는 k8s를 활용한다는 관점에서 AKS클러스터를 만드는 것부터 시작하였습니다 기억할 내용으로 구성요소는 크게 Control Plane과 Node로 나누어져 있다는 것과 Control Plane을 통해서는 관리자가 수행하고자 하는 기능이 동작되고 Node에서는 사용자가 만든 애플리케이션이 동작되는 것으로 이해해 보면 좋겠습니다 상세한 내용을 보고 싶으면 Azure Portal을 통해서 GUI로 클릭해가면서 세부 구성요소까지 쉽게 파악할 수 있습니다 파악해 볼만한 세부 구성요소는 k8s version api server url node pool node의 availability zone VMvCPU Memory namespace load balancer public IP backend pool service ingress scale set Upgrade policy 정도 되겠습니다 다음은 AKS클러스터를 생성하였으니 소스코드로 애플리케이션 컨테이너를 만들고 ACRAzure Container Registry에 컨테이너를 업로드합니다 ACR에 컨테이너의 업로드를 성공시키고 나면 드디어 AKS클러스터에 컨테이너화된 애플리케이션을 배포해 보도록 하겠습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164661&boardType=techBlog&searchData=&page=&subIndex=
KCC 2023 후기 (1) 학회 스케치와 참여 소감,안녕하세요 카카오 광고추천팀 eric신호석입니다 저는 현재 카카오 광고추천팀 내 메시지광고추천플랫폼셀에서 개발 및 운영을 담당하고 있습니다 저는 이번에 한국컴퓨터종합학술대회 Korea Computer Congress KCC 2023에 발표 및 세션 참석을 하게 되었습니다 KCC는 한국정보과학회가 주관하는 매년 열리는 학술 대회입니다 KCC는 한국을 대표하는 컴퓨터 과학 관련 학회로서 학문적인 연구와 지식 교류를 촉진하고 전문가들의 협력을 도모하기 위해 설립되었습니다 한국컴퓨터종합학술대회는 컴퓨터 과학 및 정보 기술 분야에서 활동하는 연구자 학자 기업 연구원 등이 모여 최신 연구 결과 및 기술 동향을 공유하고 이야기할 수 있는 자리를 제공합니다 이 대회는 주제별 세션 튜토리얼 초청 연설 포스터 세션 등 다양한 형태의 프로그램으로 구성되어 있습니다 많은 카카오 개발자들과 함께 이번 KCC 2023에 참여했습니다 KCC 2023은 제주도에 있는 라마다프라자제주호텔과 제주오리엔탈호텔에서 진행되었습니다 저는 KCC 2023을 처음 방문하였습니다 먼저 다양한 논문 발표와 워크숍들이 있는 것에 놀랐습니다 제주도에 많은 분들이 방문하여 여러 정보들과 의견들을 교류하고 있는 모습이 인상 깊었습니다두 번째는 생각에 닿지 않았던 다양한 주제들이 있다는 것을 알았습니다 범죄 수사와 AI 워크숍이나 사회성을 갖고 인간과 교감하는 멀티모달 인터랙션 인공지능 기술개발 과제 총괄 워크숍과 같은 주제들이 해당됩니다 첫 번째로 초거대 언어모델과 자연어처리 워크샵에 참여하였습니다 1시부터 6시까지 진행된 분량이기 때문에 전체 내용을 적을 수는 없지만 최근 트렌드가 되고 있는 ChatGPT에 대한 소개와 ChatGPT3가 가지고 있는 문제점 ChatGPT4에서는 어떻게 개선되었는지 ChatGPT가 못하는 것들에 대한 내용들을 들을 수 있었습니다 또한 기존 언어모델들이 가지고 있는 강점에 대해서도 소개했으며 이를 활용하는 방안에 대해서도 들을 수 있었습니다 그 이외에도 ChatGPT와 같은 언어모델이 가지고 있는 수학적 능력에 관한 연구동향 소개 또한 흥미로웠습니다 컴퓨터 그래픽스 및 상호작용은 교수님이 발표하는 내용이 아닌 대학원생분들이 연구하여 학술발표에서 발표한 내용을 다시 발표하는 내용이었습니다 개발적 관점으로부터 벗어난 HCI와 같은 주제를 포함해 다양한 관점을 볼 수 있었습니다 자폐 환자들을 위한 데일리 루틴 앱 사용자마다 최적화된 수면 계획 시스템이 포함됩니다 또한 뉴스의 정치적 성향을 분석하는 시도나 뉴스 추천 시스템 또한 재미있었습니다 발표자로서 참여한 카카오테크 워크숍은 1시부터 4시까지 진행되었습니다 저는 cookieshake와 함께 큐라스 메시지 광고 추천 플랫폼을 발표하였으며 이를 위해 hendopark jayleneshin liamkim 와 함께 발표를 준비하였습니다 업무를 진행하며 발표 준비를 하다 보니 시간은 절대적으로 부족했지만 틈틈이 스크립트도 준비하며 연습할 수 있었습니다 저희가 준비한 발표의 자세한 내용은 별도의 글로 소개드릴 수 있도록 해보겠습니다발표 당일 청자들은 대학원생분들이 많았습니다 많은 분들이 참석하셔서 발표 내용을 듣고 궁금한 점에 대해서는 구두나 오픈채팅방을 통해서 질문을 하였습니다 여러 세션들 중에서 카카오테크 워크숍에 참여하여 발표를 듣고 질문하는 모습에서 많은 감동을 받았습니다 처음 참여한 KCC 2023이었지만 많은 인상과 경험을 얻게 되어 매우 기쁩니다 이번 학회를 통해 다양한 분야의 논문 발표와 워크숍을 접할 수 있어서 매우 유익했습니다 KCC 2023에 참여하면서 컴퓨터 과학과 정보 기술 분야의 최신 동향을 알게 되었으며 다양한 분들의 의견 또한 볼 수 있었습니다 그런 관점에서 KCC 2023은 지식 교류를 촉진하는 훌륭한 플랫폼이었다고 생각합니다 다음 기회에도 KCC 2024에서 발표할 수 있었으면 좋겠습니다 한국정보과학회 2023 프로그램북 PBpdf kiiseorkr KCC 2023 후기 1 학회 스케치와 참여 소감 현재 글KCC 2023 후기 2 카카오 크루들과 함께 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/07/05/kcc2023-review-1/
카카오 준신위 2차 회의…6개 계열사 준법 현황 보고 마쳐,,,https://n.news.naver.com/mnews/article/003/0012307172?sid=105
"티빙, KBO 리그 유무선 중계권 우선 협상대상자 선정",,,https://n.news.naver.com/mnews/article/092/0002317379?sid=105
@codingapple,진짜로 개발자 대체품 나옴 (GPT4 코드 인터프리터),https://img.youtube.com/vi/1dwkGnH7f7M/0.jpg,https://www.youtube.com/watch?v=1dwkGnH7f7M
ELK 기반 SRE 환경 만들기 #2 | Kibana Visualize 및 시각화 대시보드 구현,들어가며 안녕하세요 오늘은 ELK 기반 SRE 환경 만들기 2탄 을 준비했습니다 1탄에서 Pipeline과 Repository 구축을 완료했기 때문에 이제 수집된 Data를 활용해서 시각화 작업을 진행해주면 됩니다 ELK Stack에서는 Kibana라는 Tool로 시각화를 진행합니다 먼저 Kibana Visualize 라고 하는 패널들을 생성해줘야 하는데요 다양한 타입의 패널들을 제공하고 있습니다 그리고 생성된 Visualize 들을 조합해 Dashboard Panel로 배치하여 시각화를 마무리할 수 있습니다 이번 포스팅에서는 다양한 Visualize 타입 중 빈번하게 사용되는 Vertical Chart Heat Map Data Table 을 준비해봤습니다 활용할 Metric 설정 및 X축 Y축 설정 등 Kibana UI 를 바탕으로 구현해보도록 하겠습니다 Visualize 에 사용된 필드들은 제가 실제로 업무에 활용하는 데이터들로 사내 보안 규정에 따라 대외비를 제거하여 일반적 IT 운영의 예시로 변형하였습니다 그리고 마지막에는 Dashboard Panel 들을 조합한 시각화 마무리까지 진행하도록 하겠습니다 1 Vertical Chart 세로 바 차트 먼저 Vertical Chart를 통해 서비스 별 Agent 들이 점유하는 Heap Memory 사용량을 모니터링 해보도록 하겠습니다 이를 위해 Agent 별 Max Heap 사용량MB과 Current Heap 사용량MB 필드를 대상으로 지정합니다 Max Heap 사용량MB은 단위 시간 동안의 최대값으로 지정하고 Current Heap 사용량MB은 단위 시간 동안의 평균값으로 지정했습니다 다음으로 Agent 별로 X축을 지정하기 위해 Buckets를 설정해줍니다 이 때 Aggregation 옵션을 Terms로 선택해주면 String Value를 설정할 수 있습니다 서비스의 Agent 명은 type 이라는 필드로 저장되고 있으며 현재 운영중인 시스템의 서비스 별 Agent 개수는 10 이므로 Order Size는 10으로 설정했습니다 마지막으로 Chart의 종류를 선택하기 위해 옵션 설정을 해보겠습니다 Agent 별 Heap Memory 사용량 모니터링은 Max Current 사용량의 값비교를 위함이니 Normal Mode를 선택했습니다 Kibana Vertical Chart에서는 누적 그래프Stacked도 제공하니 참고 부탁드립니다 Vertical Chart Example 2 Heat Map 히트맵 다음으로 Heat Map을 활용하여 서비스별 Agent에 연결된 Client Session을 모니터링 해보도록 하겠습니다 현재 운영중인 서비스의 Connection Pool은 약 400450개 정도로 관리중입니다 따라서 Current Session 필드만 사용하도록 Value 설정을 하였고 단위 시간 동안의 Max 값을 모니터링 하도록 설정했습니다 다음으로 X축 Y축 설정을 해주도록 하겠습니다 X축 설정은 서비스별 Agent 이므로 앞선 Vertical Chart와 동일하게 type으로 설정해주었습니다 그리고 Y축에 index를 지정하여 Daily Client Session 을 확인할 수 있도록 했습니다 이를 통해 일자별로 Client Session이 넘치는 Agent는 없는지 확인할 수 있게 되었습니다 Heat Map Example 3 Data Table 데이터 테이블 마지막으로 Data Table을 통해 Client의 Data 중복 적재 건수를 모니터링 해보도록 하겠습니다 현재 운영중인 서비스는 Client 서버들의 특정 Data 적재를 지원하고 있습니다 그런데 동일 Agent를 통해 서로 다른 Client들이 데이터를 적재할 경우 데이터 중복이 발생하여 누락이 발생합니다 이는 서비스 품질을 저해하는 Critical Issue로 사전 모니터링 및 사전 대응이 가능해야 합니다 따라서 이를 장애 사전 예방을 위해 해당 지표를 모니터링 항목에 추가하였습니다 Metric 설정은 중복 PID 발생의 Count 값으로 지정했습니다 다음으로 Data Table의 컬럼을 나누기 위해 Buckets 에서 Split Rows를 설정해줬습니다 여러 개의 Split Rows를 지정할 경우 SQL의 GROUP BY 절과 동일한 결과를 얻을 수 있습니다 Data Table Example 마무리 앞서 구현한 Kibana Visualize 들을 조합하여 시각화 대시보드를 구성했습니다 대시보드에 포함된 Single Value 와 Time Chart는 직관적으로 구현 가능한 Visualize 타입이기 때문에 별도 구현 내역을 작성하지는 않았습니다 위 대시보드를 통해 현재 운영중인 서비스의 실시간 지표 모니터링 환경을 구축하게 되었고 서비스 품질 향상에 많은 도움이 되었습니다 올해 상반기부터 담당 시스템의 서비스 품질 향상 및 업무 효율화를 위해 많은 고민을 했던 것 같습니다 그러던 중 다양한 오픈 소스와 신기술을 접하게 되었고 배울 수 있는 교육 기회까지 주어져 좋은 결과를 낼 수 있었던 것 같습니다 현재까지 구현된 내용은 초안으로 전체 서비스 적용 등 할일이 많지만 하반기에 차근차근 진행해볼 계획입니다 긴 글 읽어주셔서 감사드리며 다음은 ELK 환경 구축하며 겪었던 내용 바탕으로 공유하고 싶은 Tip 을 들고 돌아오도록 하겠습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165161&boardType=techBlog&searchData=&page=&subIndex=
