title,body,image,url
"Java의 미래, Virtual Thread",itemname subname 안녕하세요 회원프로덕트팀 서버개발자 김태헌입니다 이번 글에서는 팀에서 진행했던 Virtual Thread 스터디를 바탕으로 Java의 경량스레드 모델인 Virtual Thread를 소개하고자 합니다 Virtual Thread를 스터디 주제로 선정하게 된 일화가 있습니다 쇄국정책을 펼치시는 두 어르신 보시다시피 저희 팀엔 외세의 침략을 막는 흥선대원군 두 분이 계셔서 모든 프로젝트는 Java로 만들어져 있었습니다 Java만을 고집하던 중 2021년에 사용자 인증 게이트웨이 시스템 을 개발하게 되었는데 IO가 많이 발생하고 병목이 큰 장애로 전파될 수 있는 지점이어서 Kotlin의 coroutine을 활용하자는 의견이 나왔습니다 그 당시엔 Virtual Thread가 JDK 정식 feature가 아니어서 coroutine이 유일한 선택지였습니다 의견을 수렴하여 게이트웨이 시스템을 Kotlin으로 개발하게 되었지만 가끔 Kotlin 프로젝트를 보면 생소한 문법과 형식 때문에 낯설고 머리가 지끈지끈 아파왔습니다 얼마 지나지 않아 코루틴을 대체할 수 있는 Virtual ThreadProject Loom가 JDK19 얼리 액세스 feature로 포함되었다는 소식을 듣자마자 팀 스터디를 진행하게 되었습니다 경량 스레드 모델들 Go의 goroutine을 아신다면 경량 스레드 모델도 들어보셨을 겁니다 기존 언어의 스레드 모델보다 더 작은 단위로 실행 단위를 나눠 컨텍스트 스위칭 비용과 Blocking 타임을 낮추는 개념입니다 JVM 진영에는 대표적으로 2017년 Kotlin 11에 처음 도입된 Kotlin의 coroutine이 존재했습니다 2023년 9월에 정식 출시된 JDK21 에 경량 스레드 모델 Virtual Thread가 정식 Feature로 포함되면서 드디어 Java에서도 경량 스레드 모델을 다룰 수 있게 되었습니다 그렇다면 Virtual Thread에 대해 알아보겠습니다 Virtual Thread란 기존 Java의 스레드 모델은 Native Thread로 Java의 유저 스레드를 만들면 Java Native InterfaceJNI를 통해 커널 영역을 호출하여 OS가 커널 스레드를 생성하고 매핑하여 작업을 수행하는 형태였습니다 Java의 스레드는 IO interrupt sleep과 같은 상황에 blockwaiting 상태가 되는데 이때 다른 스레드가 커널 스레드를 점유하여 작업을 수행하는 것을 컨텍스트 스위치 라고 합니다 이러한 스레드 모델은 기존 프로세스 모델을 잘게 쪼개 프로세스 내의 공통된 부분은 공유하면서 작은 여러 실행단위를 번갈아 가면서 수행할 수 있도록 만들었습니다 스레드는 프로세스의 공통영역을 제외하고 만들어지기 때문에 프로세스에 비해 크기가 작아서 생성 비용이 적고 컨텍스트 스위칭 비용이 저렴했기 때문에 주목받아 왔습니다 그러나 요청량이 급격하게 증가하는 서버 환경에서는 갈수록 더 많은 스레드 수를 요구하게 되었습니다 스레드의 사이즈가 프로세스에 비해 작다고 해도 스레드 1개당 1MB 사이즈라고 가정하면 4GB 메모리 환경에서도 많아야 4000개의 스레드를 가질 수 있습니다 이처럼 메모리가 제한된 환경에서는 생성할 수 있는 스레드 수에 한계가 있었고 스레드가 많아지면서 컨텍스트 스위칭 비용도 기하급수적으로 늘어나게 되었습니다 이런 한계를 겪던 서버는 더 많은 요청 처리량과 컨텍스트 스위칭 비용을 줄여야 했는데 이를 위해 나타난 스레드 모델이 경량 스레드 모델인 Virtual Thread입니다 Virtual Thread는 기존 Java의 스레드 모델과 달리 플랫폼 스레드와 가상 스레드로 나뉩니다 플랫폼 스레드 위에서 여러 Virtual Thread가 번갈아 가며 실행되는 형태로 동작합니다 마치 커널 스레드와 유저 스레드가 매핑되는 형태랑 비슷합니다 여기서 가장 큰 특징은 Virtual Thread는 컨텍스트 스위칭 비용이 저렴 하다는 것입니다 Thread Virtual Thread Stack 사이즈 2MB 10KB 생성시간 1ms 1s 컨텍스트 스위칭 100s 10s Thread는 기본적으로 최대 2MB의 스택 메모리 사이즈를 가지기 때문에 컨텍스트 스위칭 시 메모리 이동량이 큽니다 또한 생성을 위해선 커널과 통신하여 스케줄링해야 하므로 시스템 콜을 이용하기 때문에 생성 비용도 적지 않습니다 하지만 Virtual Thread는 JVM에 의해 생성되기 때문에 시스템 콜과 같은 커널 영역의 호출이 적고 메모리 크기가 일반 스레드의 1에 불과합니다 따라서 Thread에 비해 컨텍스트 스위칭 비용이 적습니다 Virtual Thread 톺아보기 Virtual Thread가 Thread 모델보다 성능이 좋은 이유를 구조와 동작 흐름을 통해 알아보겠습니다 Virtual Thread의 구조 우선 Platform Thread의 기본 스케줄러는 ForkJoinPool을 사용하는데요 스케줄러는 platform thread pool을 관리하고 Virtual Thread의 작업 분배 역할 을 합니다 디버거를 통해 런타임의 Virtual Thread를 살펴보면 VirtualThread는 carrierThread 를 가지고 있습니다 실제로 작업을 수행시키는 platform thread를 의미합니다 carrierThread 는 workQueue 를 가지고 있습니다 VirtualThread는 scheduler 라는 ForkJoinPool을 가지고 있습니다 carrier thread의 pool 역할을 하고 가상 스레드의 작업 스케줄링을 담당합니다 VirtualThread는 runContinuation 이라는 Virtual Thread의 실제 작업 내용Runnable을 가지고 있습니다 이걸 바탕으로 Virtual Thread의 동작 원리도 알아보겠습니다 Virtual Thread의 동작 원리 실행될 virtual thread의 작업인 runContinuation을 carrier thread의 workQueue에 push 합니다 Work queue에 있는 runContinuation들은 forkJoinPool에 의해 work stealing 방식으로 carrier thread에 의해 처리됩니다 처리되던 runContinuation들은 IO Sleep으로 인한 interrupt나 작업 완료 시 work queue에서 pop되어 park과정에 의해 다시 힙 메모리로 되돌아갑니다 기존의 스레드 모델에서 virtual thread의 park unpark 동작을 통해 virtual thread의 컨텍스트 스위칭을 하는 형태로 동작한다는 것을 알 수 있습니다 그렇다면 이런 parkunpark가 실제로 어떻게 동작하는지 확인해 보겠습니다 Virtual Thread의 parkunpark Virtual Thread의 unpark 위의 VirtualThreadunpark 메서드를 살펴보면 unpark 될 수 있는 Virtual Thread 중 submitRunContinuation 메서드를 통해 scheduler를 통해 runContinuation을 execute하는 것을 확인하실 수 있습니다 이렇게 execute된 runContinuation은 carrirer thread의 work queue에 스케줄링 됩니다 기존 스레드 모델과 비교 unpark메서드 왼쪽 JDK17 오른쪽 JDK21 기존의 스레드 모델에서의 parkunpark 개념은 LockSupportclass 를 통해 네이티브 메서드로 제공하던 기능이었습니다 Reactive 환경에서 컨텍스트 스위칭이 발생하는 상황처럼 thread를 대기시키고 다른 thread를 수행해야 하는 상황을 떠올리시면 됩니다 JDK21에서부터는 LockSupport에 Virtual Thread 판단 로직을 추가하여 현재 스레드가 Virtual Thread인 경우 virtual Thread의 parkunpark 되도록 하여 기존 Thread모델과 완벽하게 호환되면서 Virtual Thread 기반 컨텍스트 스위칭이 가능하도록 하였습니다 그렇다면 IO가 발생했을 때 어떻게 경량 스레드가 park가 되는지도 알아보겠습니다 NIOSocketImpl의 park 메서드 왼쪽 JDK17 오른쪽 JDK21 기존 Reactive 방식에서는 Socket 통신 시 park 메서드 내에서 Netpoll 을 통해 커널영역에 스레드 park를 요청함으로써 컨텍스트 스위칭을 수행하였습니다 JDK 21에서는 NIOSocketImplpark 메서드에 virtual thread 판단 로직을 추가하여 현재 스레드가 virtual Thread인 경우 Pollerpoll 을 통해 내부적으로 Virtual Thread의 park를 수행하여 virtual thread 컨텍스트 스위칭을 가능하게 하였습니다 마지막으로 Thread가 Sleep 상태에서도 가상스레드 park가 수행되는지 살펴보겠습니다 Threadsleep 메서드 왼쪽 JDK17 오른쪽 JDK21 Threadsleep 메서드를 살펴보면 마찬가지로 virtual thread 분기가 생겨서 virtual thread인 경우 virtual thread 기반 park가 됩니다 이번 Virtual Thread 기능은 기존 Thread 모델에서 Native 기반으로 동작하던 parkunpark 로직에 대해 Virtual Thread 분기를 추가해 기존 Thread 방식에서 특별한 코드 수정 없이 Virtual Thread기반의 컨텍스트 스위칭을 가능하게 하였습니다 스레드 모델 간 비교 성능테스트 스레드 모델의 특성 사용성 등을 고려해 개인적으로 생각했던 장단점들을 비교해 보았습니다 성능테스트는 Ngrinder를 활용했습니다 Virtual thread의 효과를 극대화하기 위해 약을 잘 팔기 위해 최대한 극한의 상황을 가정했습니다 테스트하기 위해 애플리케이션 스펙을 최소 사양으로 두고 256MB의 힙 사이즈를 사용하도록 설정하였습니다 테스트는 300ms를 sleep하는 API를 3번 호출하는 Request IO Bound 작업 0300000000까지 합을 3번 계산하는 CPU Bound 작업으로 진행하였습니다 실제 서비스 상황과는 다소 차이가 있다는 점 참고하여 가볍게 봐주시면 좋을 것 같습니다 Thread vs Virtual Thread public String ioBound requestSleepblock Threadsleep300 API 호출 requestSleepblock requestSleepblock return ok public Integer cpuBound IntStreamrange0 300000000reduce0 Integersum IntStreamrange0 300000000reduce0 Integersum return IntStreamrange0 300000000reduce0 Integersum IO Bound 작업에서 Virtual Thread의 성능은 Thread 모델에 비해 약 51 이상 향상 되었습니다 실제로는 더 많은 성능차이가 있으나 테스트 과정에서 이슈가 발생하였습니다 우선 적절한 vuser 수를 설정하기 위해 테스트를 해보았는데 Ngrinder의 동시 요청 수를 계속해서 늘리다 보니 vuser가상 사용자 수가 250이 넘어가는 시점부터 Thread 모델에서는 서버가 죽고 응답을 정상적으로 주지 못하는 상황 이 발생하였습니다 반면에 virtual thread를 사용하는 서버에서는 동일한 vuser수에도 장애 없이 정상 처리할 수 있었습니다 vuser를 일반 thread모델이 소화 가능한 낮은 숫자로 설정했기 때문에 더 드라마틱한 차이가 발생하지는 않았습니다 이 테스트를 통해 제한된 성능에서 가상스레드 모델이 더 높은 처리량과 더 빠른 처리속도를 보여준다는걸 확인할 수 있었습니다 반면 CPU Bound 작업에서는 일반 스레드 모델이 성능상 우위를 보였는데요 경량 스레드가 결국 플랫폼 스레드 위에서 동작하기 때문에 CPU Bound 작업과 같은 Virtual Thread가 Switching 되지 않는 경우에는 Platform Thread 사용 비용뿐만 아니라 Virtual Thread 생성 및 스케줄링 비용까지 포함되어 성능 낭비가 발생되기 때문입니다 이를 두고 Java에서는 이렇게 말했습니다 It is more expensive to run a task in a virtual thread than running it in a platform thread Virtual Thread vs Kotlin Coroutine fun ioBound String return CoroutineScopeDispatchersIOasync requestSleepawaitFirstOrNull api call requestSleepawaitFirstOrNull requestSleepawaitFirstOrNull await 코루틴 모델은 앞서 테스트한 스레드 모델보다 더 많은 처리량을 가지기 때문에 이전 테스트의 vuser의 4배인 510으로 두고 IO bound 요청 테스트를 진행하였습니다 성능테스트 결과 Virtual Thread의 성능이 Kotlin coroutine에 비해 37 좋은 성능을 보였습니다 컴파일러가 코루틴을 만드는 방식 Kotlin Coroutine은 virtual thread이 JDK 자체적으로 지원하는 것과는 다르게 Kotlin 컴파일러의 마법으로 가능합니다 함수를 suspend로 선언하게 되면 해당 메서드 블럭은 경량스레드와 유사하게 동작하게 됩니다 간단하게 coroutine이 동작하는 원리를 살펴보자면 Suspend 함수를 Contiuation과 지역변수를 가진 클래스로 만듭니다 첫번째 그림처럼 suspend 메서드 내에 호출하고있는 suspend 함수가 2개의 지점이 있다면 suspend 함수 호출 부분을 기점으로 suspend point로 지정합니다 각각의 suspend point를 기준으로 labelL0 L1 L2을 나눠 switchwhen문으로 finite state machine처럼 코드를 Generate 합니다 fetchUser fetchProfile 과 같은 parkunpark가 필요한 IO 발생지점과 같은 부분은 위에서 보이는 await 와 같은 Kotlin 확장함수를 통해 parkunpark를 가능하게 합니다 Virtual Thread는 기존의 Thread 방식을 완전히 대체하기 때문에 TaskExecutor를 교체하여 어플리케이션 전체에 적용할 수 있습니다 반면 Coroutine은 메서드 단위로 원하는 곳에만 경량스레드를 적용 할 수 있다는 장점이 있습니다 그리고 코루틴은 JDK21 이전의 버전에서도 경량스레드를 적용할 수 있다는 장점을 가지고 있습니다 이는 JDK의 최신버전을 바로 적용하기 어려운 상황에서 최선의 선택이 될 수 있습니다 그러나 Coroutine은 suspend 진입 전 플로우는 경량스레드가 아닌 일반 Thread로 처리되고 IO Block 이나 sleep 같은 Thread parkunpark컨텍스트 스위칭가 필요한 순간마다 Kotlin이 만들어놓은 suspend 확장함수를 사용해야 하므로 프로덕션 코드에 변경 이 필요합니다 그리고 Reactive Streams 패러다임과 마찬가지로 suspend function들은 역시 전염성이 있어서 suspend가 전파될 수 있다는 차이점이 있습니다 이걸 함수의 색 문제 라고도 이야기하는데요 빨간색의 함수는 빨간색 함수만 호출 가능하기 때문에 계속해서 함수의 색을 고려해야 되는 문제입니다 마찬가지로 suspend function도 전파를 막으려면 특정 지점에서 runBlocking을 사용하거나 suspend Controller로 만들어야 하는데 이는 응답을 reactive 응답으로 전환하게 됩니다 마지막으로 Kotlin 러닝커브 가 존재한다는것도 하나의 고려해볼만한 포인트가 될 것 같습니다 Kotlin을 모르는 Java 개발자라면 러닝커브의 부담을 버리고 Virtual Thread를 적용하는게 정신건강에 이로울것 같네요 Virtual Thread vs Reactive Programming public MonoString ioBound return requestSleep flatMapit requestSleep flatMapit requestSleep 리액티브 스레드모델 또한 vuser를 앞서 테스트한것의 4배인 510으로 두고 IO bound 요청을 통해 진행하였습니다 성능테스트 결과 Virtual Thread의 성능이 Reactive에 비해 111 좋은 성능을 보였습니다 Spring의 Reactive 프로그래밍 모델인 WebFlux는 Netty의 event loop 기반으로 동작합니다 Event loop가 중심에서 모든 요청을 처리하고 요청 처리 구간을 callback으로 등록해놓고 worker 스레드 풀이 작업들을 처리하는 형태입니다 Worker 스레드가 작업을 처리하는 과정에서 IO를 마주치게 되면 작업이 park 되면서 컨텍스트 스위칭이 발생합니다 동기 방식과 리액티브 방식의 코드 차이 위 코드를 보면 동기로 짜여있던 코드는 직관적이었던 반면 reactive 프로그래밍으로 짜인 코드는 다소 파편화 되어 있다는 생각이 드시지 않으신가요 if문이나 trycatch구문들이 모두 메서드 단위로 분리되어 있기 때문이죠 이는 Java의 기본적인 syntax를 활용하기 어렵게 하여 코드의 흐름을 이해하기 어렵게 만들 수 있습니다 또한 reactive 프로그래밍은 함수의 색 문제 를 가지고 있어 parkunpark 사용되는 부분마다 Reactive가 적용되어야 하고 이는 플로우 전체에서 reactive streams를 사용해야하는 문제점이 존재합니다 마지막으로 컨텍스트 스위칭시 실제 스레드를 switch 하기 때문에 경량스레드 switch에 비해 성능 낭비 가 존재하고 스레드의 컨텍스트를 상실하기 때문에 스택 트레이스가 유실된다는 단점도 존재합니다 이는 디버그를 어렵게 만들 수 있습니다 Virtual Thread는 JDK 자체적으로 개선되어서 기존 Thread 모델과 완전 호환되기 때문에 기존 코드 그대로 사용할 수 있고 전염성도 존재하지 않으며 스레드 컨텍스트를 온전히 유지할 수 있기 때문에 리액티브 패러다임의 러닝커브가 부담스러우신 분들에게는 좋은 해결책이 될 수 있습니다 여기까지 Virtual Thread을 다른 스레드 모델과 비교하여 개인적으로 생각한 장단점을 제시해드렸습니다 다만 이건 저의 주관적인 시각에 불과하고 Virtual thread의 장점만을 서술하려고 노력했습니다 독자분들이 Virtual Thread에 대해 스스로 판단해주시고 댓글로 많은 의견을 주신다면 감사하겠습니다 그렇다면 Virtual Thread의 주의사항으로 넘어가도록 하겠습니다 주의사항 여기까지 Virtual Thread에 대해 알아보았는데요 끝으로 Virtual Thread의 주의사항을 알려드리고 글을 마무리하려고 합니다 No pooling Virtual Thread는 값싼 일회용품이라고 보시면 됩니다 생성비용이 작기 때문에 스레드 풀을 만드는 행위 자체가 낭비가 될 수 있습니다 필요할 때마다 생성하고 GCGarbage Collector에 의해 소멸되도록 방치해버리시는게 좋습니다 실생활의 일회용품엔 GC가 없으니 방치하지 말아주세요 CPU bound 작업엔 비효율 앞선 테스트에서 봤듯이 IO 작업 없이 CPU 작업만 수행하는것은 플랫폼 스레드만 사용하는것보다 성능이 떨어집니다 컨텍스트 스위칭이 빈번하지 않은 환경이라면 기존 스레드모델을 사용하시는것이 이득입니다 Pinned issue Virtual thread 내에서 synchronized 나 parallelStream 혹은 네이티브 메서드를 쓰면 virtual thread가 carrier thread에 park 될 수 없는 상태가 되어버립니다 이를 Pinned고정된 상태라고 하는데요 이는 예상한 virtual thread의 성능저하를 유발할 수 있습니다 써드파티 앱이나 synchronized 과정 중 수백 밀리초가 걸리는 연산이 존재하는 경우엔 ReentrantLock 으로 대체할 수 있을지 고려해보셔야합니다 Thread local Virtual Thread는 수시로 생성되고 소멸되며 스위칭됩니다 백만개의 스레드를 운용할 수 있도록 설계되었기 때문에 항상 크기를 작게 유지하시는게 좋습니다 다이어트를 하고 다시 살이 찌는걸 요요현상이라고 하잖아요 가상 스레드의 thread local 사이즈가 커질수록 요요현상이 강하게 온다고 생각해주세요 지금까지 Virtual Thread에 대해 알아보았습니다 아직은 낯선 기술이지만 조만간 Java의 주력 기술중 하나가 될 것이라 생각합니다 여러분들의 Java 환경에서도 적절하게 적용해보시는건 어떨까요,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/15398/
Python package 의 build frontend 와 backend,서론 파이썬 패키지 빌드와 관련된 많은 문제들이 있었고 PEP 에서는 이를 해결하기 위한 여러 논의들이 있었습니다 무엇이 문제였고 어떻게 개선되어 왔는지에 대한 글들은 많기 때문에 따로 이야기하지 않겠습니다 이 글에서는 PEP 517과 PEP 518에서 제안하는거의 표준이 되고있는 build frontend 와 backend 에 대해서 설명을 하고 관련된 툴들을 살펴보겠습니다 파이썬 패키지 배포와 설치 일반적으로 파이썬 생태계에서 패키지의 배포와 설치는 아래 그림처럼 이루어집니다 source tree 일반적으로 vcs 에 저장되어 있는 패키지의 소스코드 ex httpsgithubcompypasampleproject source distributionsdist 릴리즈 된 패키지의 소스코드 ex lxml344targz binary distributionwheel 별도의 컴파일 과정 없이 sitepackages 디렉토리에 unpack 만 하면 install 이 끝나는 파일들만을 포함 ex chardet510py3noneanywhl 개발 단계에서는 아래 과정으로 빌드 배포 합니다 source tree 를 Github 에 push cicd 에서 source tree 를 checkout 후 sdist 와 wheel 빌드build PyPI 에 업로드twine 설치 단계에서는 보통 pip 을 이용해서 패키지를 설치하는데 VCS 에서 source tree 를 checkout 받거나 PyPI 에서 sdist 나 wheel 을 다운받아 설치합니다 빌드는 source tree sdist 와 sdist wheel 과정을 말하고 패키지의 개발과 설치 모두에서 발생합니다 Build frontend 와 backend Build frontend 는사용자가 빌드할 때 이용하는 도구로 실제 빌드를 하지않고 build backend 를 호출한다고 되어있지만 이 설명만으로는 무엇을 하는지는 알기 힘듭니다 build frontend 는 다음의 일을 수행해야합니다 pyprojecttoml 의 buildsystem 를 읽고 build requirements 와 buildbackend 를 파싱 빌드를 위한 virtualenv 를 구성하고 build requirements 설치 buildbackend 객체의 buildwheel buildsdist hook 호출 당연하게도 buildbackend 는 buildwheel 과 buildsdist 훅을 구현해야하고 패키지 개발자는 패키지 빌드에 필요한 정보를 pyprojecttoml 의 buildsystem 테이블에 작성해야합니다 어떤 객체든 buildwheel 과 buildsdist 훅만 구현하면 buildbackend 가 될 수 있고 pyprojecttoml 이 아닌 다른 설정 파일을 사용하는 것도 가능합니다 Tools build pypa 에서 배포하는 build frontend setuptools 과거에는 build frontend 역할도 했지만 지금은 build backend 역할만 수행 pyprojecttoml 뿐만 아니라 setuppy 도 사용 가능하고 C extension 을 이용할 경우 setuppy 가 필요합니다 build backend 를 따로 지정하지 않으면 setuptools 를 build backend 로 호출한다 pep 621 지원 flit pypa 에서 배포하는 툴로서 pure python package 를 명령어 하나로 pypi 에 배포합니다 pep 621지원 poetry 파이썬 패키지 관리도구로 개발환경 관리 빌드 및 배포까지 가능하지만 다른 build backend 는 지원하지 않습니다 따라서 build frontend 라고 할 수 없습니다 또한 pep 621를 지원하지 않아 이제는 legacy 가 되어가고 있는 것 같습니다 hatch pypa 의 패키지 관리도구로 비교적 최근인 2022년 초에 처음 릴리즈되었습니다 poetry 와 포지션이 거의 겹치지만 최신 pep 스펙을 준수하고 기능 확장이 쉬워 빠르게 사용자가 늘고 있습니다 black fastapi uvicorn pydantic tox 등의 굵직한 project 에서 이용중입니다 마치며 파이썬 패키지 build frontend 의 경우 pyprojecttoml 의 buildsystem 테이블을 읽고 backend 호출이라는 명확하고 단순한 기능만을 갖고 있어서 개발 단계의 build 와 설치 단계의 pip 외에는 특별히 다른 구현체가 있을 필요가 없을 것 같습니다 build backend 기능을 포함한 파이썬 패키지 관리 도구들이 많이 있는데요 가장 인기있는 도구 중 하나였던 poetry 가 pep 를 준수하지 않고 고집을 부리는 동안 hatch 같은 신생 도구에게 기회를 주고 있는 것 같습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164866&boardType=techBlog&searchData=&page=&subIndex=
삼성 첫 AI 폰 ‘갤S24’ 17일 베일 벗는다...2024 초대장 발송,"삼성전자의 첫 AI 스마트폰이 될 ‘갤럭시 S24′ 시리즈가 오는 17일(현지시각) 미국에서 공개된다.
3일 삼성전자는 세계 주요 언론사 및 파트너사에 영상 초대장을 보내고 오는 17일 미국 캘리포니아주 새너제이에서 ‘삼성 갤럭시 언팩 2024′ 행사를 연다고 밝혔다.
삼성전자 관계자는 “새로운 모바일 AI 경험과 무한한 가능성으로 가득한 갤럭시 AI와 삼성전자의 혁신을 직접 확인하시길 바란다”고 말했다.",https://imgnews.pstatic.net/image/023/2024/01/03/0003808531_001_20240103113001093.jpg?type=w647,https://n.news.naver.com/mnews/article/023/0003808531?sid=105
AKS로 쿠버네티스 시작하기 : 간단한 spring boot 앱 배포하기,AKS로 쿠버네티스 시작하기 argocd툴에 ingress 통해 접속 에 이어서 지난 블로그에서 argocd로 도메인명으로 쉽게 접속할 수 있게 되었습니다 CICD 환경이 어느 정도 구성이 되었으니 현재까지 구성한 클러스터을 기반으로 간단한 앱을 배포해 보도록 하겠습니다 지난 블로그 링크 AKS로 쿠버네티스 시작하기 argocd툴에 ingress 통해 접속 Spring Boot 애플리케이션 소스 준비 pomxml spring boot 소스 indexhtml 로 구성을 합니다 pomxml xml version10 encodingUTF8 project xmlnshttpmavenapacheorgPOM400 xmlnsxsihttpwwww3org2001XMLSchemainstance xsischemaLocationhttpmavenapacheorgPOM400 httpsmavenapacheorgxsdmaven400xsd modelVersion400modelVersion parent groupIdorgspringframeworkbootgroupId artifactIdspringbootstarterparentartifactId version310version relativePath lookup parent from repository parent groupIdcomexamplegroupId artifactIdhellospringartifactId version001SNAPSHOTversion namehellospringname descriptionhellospringdescription properties javaversion17javaversion properties dependencies dependency groupIdorgspringframeworkbootgroupId artifactIdspringbootstarterwebartifactId dependency dependency groupIdorgspringframeworkbootgroupId artifactIdspringbootstartertestartifactId scopetestscope dependency dependencies build plugins plugin groupIdorgspringframeworkbootgroupId artifactIdspringbootmavenpluginartifactId plugin plugins build project HellospringApplicationjava package comexamplehellospring import orgspringframeworkbootSpringApplication import orgspringframeworkbootautoconfigureSpringBootApplication SpringBootApplication public class HellospringApplication public static void mainString args SpringApplicationrunHellospringApplicationclass args indexhtml DOCTYPE html html langen head meta charsetUTF8 titlehellotitle head body h1hello springboot working on k8sh1 body html 서비스 동작 확인 아래와 같이 8080 포트에서 서비스를 제공하는 것을 확인할 수 있습니다 간단한 애플리케이션이 준비가 되었습니다 로컬 환경에서 잘 동작하는 것이 확인이 되었습니다 이번에는 CICD 자동화 이전에 Dockerfile을 활용하여 컨테이너 이미지를 만드는 과정을 포함해서 전체 세부 절차를 확인할 수 있는 내용으로 진행해 보겠습니다 단계별 절차를 이해하면 자동화 시에도 자신의 환경에 맞게 최적화 구성을 할 수 있겠습니다 로컬 환경에서 jar 기동 확인 Spring boot 의 특성상 내장형 톰캣 구조로 로컬 환경에서 아래의 java명령어로 jar로 생성된 애플리케이션을 동작시키는 것도 가능합니다 로컬에서 잘 작동하는 지 먼저 확인하고 컨테이너 이미지를 작성하는 것을 권장합니다 애플리케이션 빌드를 수행하면 target디렉토리에 jar 파일이 생성됩니다 해당 디렉토리에서 아래의 명령어를 수행하면 jar로 생성한 애플리케이션을 동작시킬 수 있습니다 java jar jar Dockerfile 만들기 애플리케이션 빌드환경과 일치하는 openjdk를 base로 해서 애플리케이션 패키징을 통해 생성한 jar파일을 appjar로 복사한 다음 8080포트로 기동시키는 Dockerfile입니다 EXPOSE 8080 기동하는 컨테이너는 8080포트를 통해서 서비스 제공을 합니다 ENTRYPOINT를 통해서 컨테이너가 기동하면 java jar appjar 가 동작할 수 있도록 구성을 할 수 있습니다 FROM openjdk17alpine314 WORKDIR ARG JARFILEjar COPY targetJARFILE appjar EXPOSE 8080 ENTRYPOINT java jarappjar 애플리케이션 이미지 만들고 태깅하기 Dockerfile을 만든 애플리케이션 프로젝트 루트에서 podman build 명령어로 타겟 클러스터 플랫폼리눅스 맞게 acrappazurecriohellospringlatest로 이미지를 만들고 태깅하여 생성합니다 podman build platform linuxamd64 tag acrappazurecriohellospringlatest STEP 16 FROM openjdk17alpine314 STEP 26 WORKDIR 241ccc727841 STEP 36 ARG JARFILEjar efe8fbff9872 STEP 46 COPY targetJARFILE appjar 5b38963da7df STEP 56 EXPOSE 8080 72ca8d7eaf0c STEP 66 ENTRYPOINT java jarappjar COMMIT acrappazurecriohellospringlatest ec212b7dca25 Successfully tagged acrappazurecriohellospringlatest ec212b7dca25e83f1b0e01a5bbd483888e0355abfbd98c67327103e30f57f0b8 ACR로 업로드하기 위에서 생성한 이미지태깅포함를 podman push 명령어로 ACR에 업로드 가능합니다 podman push acrappazurecriohellospringlatest ServiceDeployment yaml 파일 작성하기 ACR에 올려진 이미지를 이제 k8s클러스터에 배포할 차례입니다 k8s에서는 해당 이미지를 사용하여 컨테이너를 구동시켜 서비스를 제공할 수 있게 하기 위해 service오브젝트와 deployment오브젝트를 yaml파일로 작성해야 합니다 apiVersion v1 kind Service metadata name svchellospring namespace hellospring spec type LoadBalancer ports name http port 8080 targetPort 8080 selector app apphellospring apiVersion appsv1 kind Deployment metadata name deployhellospring namespace hellospring spec replicas 2 selector matchLabels app apphellospring template metadata labels app apphellospring spec nodeSelector kubernetesioos linux containers name containerhellospring image acrappazurecriohellospringlatest resources requests cpu 100m memory 256Mi limits cpu 250m memory 512Mi env name TZ value AsiaSeoul ports containerPort 8080 작성한 ServiceDeployment 을 k8s에 배포하기 k9s 툴을 띄워두고 kubectl apply f yaml 파일명 명령어를 통해 배포하면 pod상태를 바로 확인할 수 있습니다 혹시라도 pod 동작에 에러가 발생하면 k9s의 기능을 이용해서 디버깅도 가능합니다 소문자 l 키 logs d 키 describe 참고로 위의 오른쪽 패널의 k apply 명령은 kubectl apply 를 의미합니다kubectl을 k로 alias 해서 사용 배포한 서비스 확인 kubectl get svc n 으로 서비스를 확인할 수 있습니다 n은 namespace를 의미 여기서는 external IP가 2019622889 입니다 k get svc n hellospring NAME TYPE CLUSTERIP EXTERNALIP PORTS AGE svchellospring LoadBalancer 10057154 2019622889 808030720TCP 10m 해당 external IP로 접속해 보면 서비스 동작을 확인할 수 있습니다 정리 소스작성 컴파일 jar생성 java jar로 확인 Dockerfile작성 컨테이너 이미지 생성과 태깅 ACR에 업로드 ServiceDeployment yaml 작성 k8s에 배포 순으로 전체 과정을 간단한 앱을 통해 진행해 보았습니다 해당 절차를 응용하면 컨테이너 동작과 관련한 문제 발생에 단계적으로 확인하여 대응할 수 있습니다 다음은 k8s 구성 최적화 측면과 k8s를 활용한 유용한 usecase 사례를 진행해 보도록 하겠습니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164957&boardType=techBlog&searchData=&page=&subIndex=
@jocoding,"AI 뉴스 - 어도비 MAX, 생각 읽는 AI, 현실 시뮬레이션, 달리 프롬프트 유출, 오픈AI 매출 공개 등",https://img.youtube.com/vi/PZjiiWvK8sU/0.jpg,https://www.youtube.com/watch?v=PZjiiWvK8sU
"""추억의 후뢰시맨 온다""…韓 출시 35주년 팬미팅 NFT 티켓 판매","후뢰시맨 국내 출시 35주년을 기념하는 이번 팬미팅은 오는 4월 20일 서울 강서구 스카이아트홀에서 열린다.
국내 최초로 주연급 일본 배우들이 내한하는 행사로 후뢰시맨 마니아들의 관심이 높아지고 있다.
한편, 팬미팅 티켓은 현장에서 NFC카드가 내장된 실물 카드 티켓으로 교환할 수 있다.",https://imgnews.pstatic.net/image/138/2024/01/04/0002164186_001_20240104100601273.jpg?type=w647,https://n.news.naver.com/mnews/article/138/0002164186?sid=105
[Software Architecutre Pattern 02] Micro Service Architecture,지난번 Software Architecutre Pattern 01 Software Design 원칙 SOLID 알아보기에 이어 Software Architecutre Pattern 관련 두번째 연재글입니다 이번에는 MSAMicro Service Architecture에 대해 알아보고자 합니다 Netfllix Spring Cloud MSA 모델과 Kubernetes Architecutre Spring Cloud Kubernetes를 살펴보려고 합니다 Netflix Spring Cloud MSA Model 스프링 클라우드 아키텍처는 위와 같은 구조를 가집니다 구성요소 설명 Service Discovery Service Discovery는 MSA 에서 각 서비스 목록을 관리하고 health 상태 정보를 저장합니다 Api Gateway Routing API Gateway 는 들어오는 요청uri 에 따라 필요한 Service 로 라우팅 합니다 Service Discovery 이용 Api Gateway Filter API Gateway 는 요청을 필터링 합니다 pre route post error 필터링수행 Client Side Loadbalancer Ribbon Ribbon 은 Client Side LoadBalancer 이며 ServiceDiscovery 로 부터 서비스 목록을 캐싱합니다 Client Side Loadbalancer Feign FeignClient 은 Ribbon을 이용하여 서비스간 Communication 을 간편하게 추상화 합니다 Circuit Breaker CircuitBreaker 은 특정 이슈 상황이 Threadhold 를 넘어서면 CircuitBreaker 가 오픈되며 대응방안이 동작합니다 Circuit Breaker Fallback Fallback 은 정상적인 응답이 불가능할때 대체 응답이 동작하는 방법입니다 Circuit Breaker Retry Retry 는 서비스 응답이 정상적이지 못한경우 혹은 응답을 받지못한경우 재 시도를 통해서 응답을 받습니다 Circuit Breaker Bulkhead Bulkhead 는 여러개의 Thread 혹은 Semaphore 를 이용하여 하나의 장애가 전체 장애가 되지 않도록 격리합니다 Distributed Trace Sleuth 멀티 쓰레드 환경에서 수많은 서비스간의 통신을 하나의 traceId 로 취합할 수 있습니다 Distributed Trace Zipkin Zipkin 은 Sleuth 가 생성한 Trace 정보를 수집하는 서버로 분산 트레이싱을 취합하는 서비스 입니다 Distributed Config 분산 설정 서버는 수십 수백대의 서비스를 일관되게 관리할 수 있는 설정 서버입니다 Streaming Stream 서비스는 EDA Event Driven Architecture로 비동기 요청을 통해 서비스간 의존성을 해소합니다 상기 나열된 각 기능들을 활용하여 Spring Cloud 기반의 MSA 를 구축할 수 있게 됩니다 참고 SpringCloud Architecture 의 자세한 구현체와 설명은 다음 자료를 참조하세요 httpsgithubcomunclebaeSpringCloudCourse Kubernetes Architecture SpringCloud Kubernetes 를 활용하기 위해서 기본적인 Kubernetes Architecture 을 살펴볼 필요가 있습니다 Kubernetes 의 전체적인 아키텍처 개요는 아래와 같습니다 Component Description Master API Server Kubernetes 의 모든 컴포넌트들은 API Server 와 통신합니다 외부 오퍼레이션을 받아들이거나 각 Node 에 Pod 설치 명령을 내리는 역할 각 노드들의 상태를 저장하거나 상태에 따라 새로운 노드를 배포하는 역할을 위해 통신하는 게이트웨이 역할을 합니다 Controller Manager Node Controller노드 다운에 대한 감시 Replication Controller원하는 복제 계수에 대한 모니터링을 수행 Endpoint Controller엔드포인트 객체 노출 서비스파드 조인 Service Account Token Controller기본 어카운트 생성 API 접근을 위한 토큰 생성등을 수행 Scheduler 노드들의 자원 상태를 감시합니다 이를 통해 노드에 새로운 포드를 배포할 수 있도록 정책을 관리합니다 또한 리소스 요구사항을 수집하고 하드웨어 소프트웨어 정책 제한 어피티니안티어피니티 정책 워크로드 인터페이스 등의 팩터들을 수집하여 이를 통해 스케줄링을 합니다 Etcd 고가용성 keyvalue 스토어로 모든 클러스터의 데이터를 저장합니다 또한 ConfigMap Secret 등도 저장합니다 Node Kublet 각 클러스터 노드에서 수행되는 에이전트입니다 파드에서 컨테이너가 수행되도록 하거나 파드 스펙에 따라 정상적으로 수행 되는지를 보장합니다 Pod Kubernetes 에서 배포의 최소단위 요소 입니다 Pod 안에서는 여러개의 컨테이너가 수행될 수 있으나 기본적으로 하나의 Pod에 하나의 컨테이너를 권장합니다 Kube Proxy 네트워크 프록시를 수행하며 각 클러스터 노드에서 동작합니다 이는 노드의 네트워크 룰을 관리합니다 Pod간 네트워크 통신을 담당하거나 내부와 외부 연결을 담당하게 됩니다 Kubectl kubectl 은 admin 하여금 api server 엔드포인트와 통신하도록 해주는 cli 툴 입니다 대부분 이 툴을 이용하여 kubernetes 에 명령을 수행합니다 Ingress Ingress 는 Kubernetes 로 들어오는 네트워크 엔드포인트로 외부 사용자가 kubernetes 의 서비스로 L7 로드 밸런싱을 지원합니다 인입되는 트래픽을 지정된 Rule에 따라서 어떠한 서비스 전달할지 룰을 지정하며 Ingress Controller 가 이 룰을 통해 라우팅을 수행할 수 있도록 해줍니다 간략하게 Kubernetes 의 아키텍처를 알아 보았습니다 위 아키텍처의 구조를 유심히 살펴보면 SpringCloud과 Kubernetes 가 어떻게 통합되었는지 감을 잡을 수 있습니다 SpringCloud vs SpringCloud Kubernetes 각각의 아키텍처를 알아 보았으니 이제 SpringCloud Kuberentes 가 SpringCloud와 어떻게 통합되는지 한번 알아보겠습니다 위 아키텍처에서 어떠한 부분이 다른지 한눈에 파악할 수 있습니다 SpringCloud 에서 제공하던 도구들중에서 Kubernetes 와 통합하면서 변경되는 부분은 다음과 같습니다 ServiceDiscovery 기존 SpringCloud 에서 사용하던 서비스 디스커버리Eureka 대신에 Kubernetes Api Server 을 통해서 각 서비스를 Discovery 할 수 있습니다 ConfigMap ConfigMap 을 통해서 SpringCloud의 ConfigServer 를 대체할 수 있으며 실시간 Refresh 등을 수행할 수 있습니다 Secret Secret 은 Kubernetes 에서 암호화된 환경변수등을 관리하기 위한 도구로 ConfigServer 을 대체합니다 API Gateway SpringCloud 에서 제공하던 Zuul Proxy SpringCloud Api Gateway 는 Kubernetes 의 Ingress 로 대체할 수 있습니다 어떻게 MSA를 쪼갤 것인가 WrapUp 마이크로 서비스의 구성 요소를 살펴 보았다 위 구성요소들은 MSA를 효과적으로 구현하기 위한 요소이며 이들 요소가 초기 Netflix MSA OSS 도구들로 발전하였고 이후 Spring Cloud Kubernetes 최근에는 Kubernetes의 다양한 도구들로 MSA를 효과적으로 구현하기 위한 OSS들이 많이 출시 되었다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165072&boardType=techBlog&searchData=&page=&subIndex=
"KT, '檢 출신' 임원 추가 영입…AI연구소장에 '경쟁사 출신'","허 상무는 사법연수원 33기로 2004년 검사로 임관했으며, 김앤장 법률사무소를 거쳐 법무법인 율정과 법무법인 아인 대표변호사로 활동했다.
앞서 KT는 지난해 말 임원 인사에서도 법무실장에 이용복 부사장을 영입한 바 있다.
앞서 KT는 조직개편 과정에서 AI 연구개발 조직 강화를 위해 기존 배순민 상무가 이끄는 'AI2X랩'에 더해 AI테크랩을 신설한 바 있다.",https://imgnews.pstatic.net/image/008/2024/01/03/0004981592_001_20240104041856060.jpg?type=w647,https://n.news.naver.com/mnews/article/008/0004981592?sid=105
Azure Ignite 2022 Summary,Azure Ignite 2022 Summary Introduce Azure Ignite 2022 세션 중에 기술 관련 주요 내용을 정리해봤습니다 살펴볼 내용은 Azure ARC Elastic SAN Cosmos DB for PostgreSQL Microsoft Designer 입니다 Azure ARC Azure ARC는 Azure Anywhere 컨셉으로 Azure 는 어디에나 있고 모든 제품군에 Azure 에이전트 솔루션을 탑재하여 Azure Public Cloud 환경에서 통합 관리할 수 있는 환경을 제공합니다 Onpremise Edge Device Rasp Jetson Devices Multicloud AWS Azure GCP 등 다양한 환경의 모든 머신들을 서로 연결하고 통합 관리하여 하이브리드 클라우드를 구축하기 위한 기본 인프라 구성 환경을 쉽게 제공합니다 Azure ARC 를 지원하는 다양한 대상들이 존재하고 그 중 SQL Server Kubernetes VM Azure Stack HCI 등 여러가지가 있지만 이 중에 Azure Arcenabled Kubernetes 를 위한 솔루션이 많이 강화되고 있습니다 또한 OnPremise 환경에서 이미 가지고 있는 서버와 네트워크 자원들에 Azure Stack 을 구축할 경우 Azure 에서 제공하는 다양한 클라우스 서비스 즉 VM SQL IotHub EventHub 등을 구축 가능하고 이를 Azure ARC 와 연동하여 하이브리드 클라우드 구성이 가능합니다 즉 모든 곳에 Azure 를 설치하고 P2P 로 연결하고 Azure Public Cloud 환경에서 통합관리하는 것이 Azure ARC의 목적입니다 이러한 철학을 MS 엔지니어 분이 메타버스를 주제로 한 영화 Everything Everywhere All At Once 영화로 표현하네요 멋진 표현인 것 같습니다 개인적으로는 Azure ARCenabled Kubernetes 를 이용하여 모델 학습배포에 활용하고 있습니다 Azure ARCenabled Kubernetes 적용시 여러 사이트에 분산된 Multi Kubernetes 환경을 Azure 환경에서 통합 관리가 가능합니다 이를 잘 활용하면 OnPremise 쿠버네티스 GPU Cluster 환경에서 거대 모델을 학습하고 이후 생성된 모델을 Blob 스토리지에 저장한 후 AMLAzure Machine Learning Online endpoint 를 이용하여 쉽게 Serving 함으로써 고가용성을 제공하는 Inference Service 가 가능합니다 결국 Public Cloud 인터페이스AML Studio 환경에서 학습 코드를 개발하고 학습 과정은 OnPremise Kubernetes GPU Cluster 환경에서 실행 함으로써 비용과 시간을 절약하고 이후 학습과정에서 생성된 모델을 Azure ML Endpoint 고가용성 환경에서 추론 서비스가 가능합니다 이러한 Azure ARC 솔루션의 배경에는 사실 OnPremise 에서의 수많은 요구사항이 퍼블릭 클라우드와 상충하여 경쟁하는 부분이 많았고 현재는 하이브리드 클라우드 컨셉으로 통합하여 흘러가는 틀랜드 입니다 즉 사람들이 클라우드 도입시 비용을 걱정하고 그 중에 특히 GPU 비용은 크리티컬 한데 이러한 고객의 요구사항을 MSP 는 모두 알고 있고 결국 Everything Everywhere All At Once 철학을 통해 고객의 진입장벽을 허물어 버립니다 아래 그림은 이미 학습된 모델이 있다는 전제하에 AML Online Endpoint 서비스를 이용하여 다양한 GPU 컴퓨팅 환경 FN 시리즈 GPU Cluster에 BlueGreen 배포하고 Traffic Management 추론 트래픽 분산 를 통해 추론 테스트하는 과정을 보여줍니다 이 환경을 통해 모델의 지속적인 개발배포가 가능합니다 httpstechcommunitymicrosoftcomt5imageserverpageimageid282439iF10292F821EBDBBBvv2 Elastic SAN AKS 환경에서 Persistent Volume 을 위한 Storage Class 중에 크게 보면 Azure Managed Disk 디스크 볼륨 Azure Files네트워크 스토리지 를 제공하고 있습니다 보통 대용량 데이터의 경우 스토리지 스케일링 보안 고가용성을 위해 Azure Files 를 많이 사용하는데 기존에는 SAN 급의 IOPS 를 지원하는 Storage Class 가 없었고 이번 Ignite 2022에서 Elastic SAN 기능을 제공하는 것 같습니다 결국 AKS 환경에서 CloudNative Fully Managed SAN 을 이용하여 초고속 IOPS 환경에서 PVC 볼륨 마운트 생성이 가능할 것 같습니다 CloudNative Fully Managed SANStorage Area Network 을 통해 빠른 속도 가용성 보안 기능을 제공하여 빅데이터를 위한 NoSQL 모델 학습에 필요한 데이터 저장관리 목적으로 활용 가능합니다 대신 비용이 만만치 않을 것으로 판단합니다 Cosmos DB for PostgreSQL Azure Cosmos DB 하나의 DB Service 내에서 다양한 DB Engine NoSQL MongoDB API 를 제공하고 있었고 이번에 MySQL 대비 복잡한 대용량 쿼리가 가능한 PostgreSQL API 서비스가 GA 된 것 같습니다 Cloud PostgreSQL Managed 서비스가 필요한 분들께서는 참고하시면 도움이 되실듯 합니다 Cosmos DB Microsoft Designer Mirosoft Designer 는 DallE 2 AI 모델에 의해 생성되는 AI 콘텐츠 생성 도구 입니다 최근 Open AI Stable Diffusion 에서 TextToImage 와 같이 키워드 방식으로 AI 이미지를 생성하여 컨텐츠를 디자인하기 위한 도구를 제공하고 있습니다 특히 Stable Diffusion 을 쉽게 사용할 수 있도록 dstablediffusionwebui 오픈소스를 이용하여 로컬환경에 설치하여 테스트가 가능한데 비슷한 컨셉으로 쉽게 UI 웹사이트를 통해 AI 컨텐츠를 생성하고 다지인 하기 위한 도구를 SaaS 형태의 Microsoft Designer 형태로 제공합니다 홈페이지를 통해 MS 계정으로 가입하고 이후 waitlist 에 추가 요청하면 테스트 가능한 환경을 메일로 수신받고 이후 주요 기능 테스트가 가능한 것 같습니다 ETC Azure Syntax AI 기반의 요약 변환 자동 어셈블리 및 주석 기능 포함 대량의 컨테츠를 자동으로 읽어주고 테그 인덱싱 각각의 컨텍스트에 필요한 위치를 표시 O365 Teams 와 연동하여 PDF or Word 문서를 Syntax 즉시 번역을 진행 Microsoft immersive meeting experience for Meta Quest Mesh 메타의 VR 장비와의 시너지 기대 메타퀘스트 디바이스에서 O365 AAD XBox Microsoft Intune Viva 학습 커뮤니케이션 통찰력을 하나의 플랫폼으로 개발하여 직원 경험을 할 수 있는 플랫폼 Viva Sales 대화형 인텔리전스의 고객 통화를 기록하고 하이라이트를 중요 메시지로 제시 CRM 데이터를 Teams 및 Outlook 고객 상호 작용 Microsoft Place 대면 미팅을 위한 AI 미팅 스케쥴러 Ref Azure Ignite 1편 Azure Ignite 2편,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164455&boardType=techBlog&searchData=&page=&subIndex=
"Nocode, Serverless cloud Services for Generative AI : monokode stein / 제1회 kakao tech meet",5월 11일에 진행된 제1회 kakao tech meet의 세 번째 발표 영상과 발표자 인터뷰를 공유합니다 generative AI nocode serverless eventual computing cloud 발표자 소개를 해주세요 또한 VOD 시청을 추천하는 분이 있다면 알려주세요 카카오에서 클라우드를 만들고 있는 공용준andrewkong입니다 뇌과학neuroscience을 전공하고 있어서 클라우드와 알고리즘을 통해서 AI문제를 푸는 것에 관심이 많습니다 취미는 논문 읽기 특기는 책 만들기인 엔지니어입니다 노코드가 궁금한 분들 노코드로 AI 서비스를 만드는 방법에 대해서 궁금한 분들 오픈소스에 관심 있는 분들에게 추천합니다 엔지니어 그리고 과학자로서 이번 세미나 발표 소감을 말씀해 주세요이번 세미나에 발표자로 참여한 것은 개인적으로 큰 영광이었습니다 저는 노코드 플랫폼인 모노코드monokode를 소개하고 이를 통해 애플리케이션 개발의 편의성을 혁신적으로 향상할 수 있는 방식을 보여드렸습니다 참석자들의 관심과 반응이 아주 높아서 보람을 느꼈습니다 이번 발표를 통해 카카오의 기술과 제품을 알리고 참여한 분들에게 적절한 자극을 드린 행사가 되었길 바랍니다 모노코드 슈타인에 대해 얘기해 주세요이번 테크밋에서는 모노코드 슈타인monokode stein을 공개하였습니다 모노코드 슈타인은 채팅앱과 LLM자연어 처리 모델을 학습시키는 과정까지 한 번에 완성할 수 있는 플랫폼입니다 현장에서 참석자들의 관심과 반응이 높았으며 모노코드의 가능성과 강력함을 보여드릴 수 있었습니다 이번 테크밋에 참여해 주신 분들께 한 말씀 부탁드립니다이번 세미나에 참여해 주신 모든 분들께 진심으로 감사드립니다 늦은 시간까지 경청해 주시고 질문과 피드백을 보내주신 참여자들께 감사드립니다 또한 행사를 준비하고 운영해 주신 스태프들에게도 감사의 인사를 전하고 싶습니다 참여자들과 스태프들의 지원과 협력 덕분에 이번 테크밋은 성공적으로 마무리할 수 있었습니다 다음 테크밋에 대한 기대가 있으신가요다시 이 자리에 설 기회가 온다면 더욱 흥미로운 주제와 혁신적인 기술 동향을 소개할 수 있기를 기대합니다 또한 더 많은 개발자들이 참여하여 생태계와 기술 교류의 자리를 더욱 확장시킬 수 있기를 바랍니다 다양한 분야의 전문가들이 모여 지식을 공유하고 협업할 수 있는 멋진 행사가 되기를 희망합니다 발표 영상은 유튜브 kakaotech 채널에서 바로 시청하실 수 있으며 kakao tech meet 재생목록링크에서 모아 볼 수도 있습니다 관련 글 목록 제1회 Kakao Tech Meet 후기ChatGPT로 달라진 개발자의 일상실전 인공지능 챗봇 개발Nocode Serverless cloud Services for Generative AI monokode stein 현재 글 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/05/31/monokode-stein/
"‘AI 스마트폰 시대 개막’ 삼성 갤럭시S24, 17일 美 캘리포니아서 공개… 언팩 초대장 발송","인공지능(AI) 기능 내장한 삼성 최초의 스마트폰
이번 언팩 행사명은 ‘갤럭시 언팩 2024: 모바일 AI 새 시대 개막(Galaxy Unpacked 2024: Opening a New Era of Mobile AI)’이다.
특히 갤럭시S24에 스마트폰 최초로 삼성전자가 자체 개발한 생성형 인공지능(AI) 모델 ‘가우스(Gauss)’ 탑재될 것이라는 게 업계의 관측이다.",https://imgnews.pstatic.net/image/022/2024/01/03/20240103514974_20240103174301688.jpg?type=w647,https://n.news.naver.com/mnews/article/022/0003891054?sid=105
"카카오, CA협의체 개편… 김범수-정신아 공동의장 체제","카카오 CA협의체 공동의장을 맡은 김범수 경영쇄신위원장(왼쪽)과 정신아 대표이사 내정자.
카카오가 지난 2일 김범수 경영쇄신위원장과 13개 협약 계열사 CEO들이 참석한 가운데 회의를 열고 새로운 CA(Corporate Alignment)협의체 구성을 발표했다.
새 CA협의체는 김범수 경영쇄신위원장과 정신아 대표이사 내정자가CA협의체 공동 의장을 맡는다.
정신아 CA협의체 의장 겸 대표이사 내정자는 “CEO들의 위원회 참여를 통해 그룹의 의사결정 맥락 이해를 높이고, 높아진 해상도를 바탕으로 내부 통제를 강화하게 될 것”이라며 “그동안의 느슨한 자율경영 기조를 벗어나 구심력을 높이겠다”고 말했다.",https://imgnews.pstatic.net/image/022/2024/01/03/20240103510065_20240103160203224.jpg?type=w647,https://n.news.naver.com/mnews/article/022/0003891013?sid=105
롯데정보통신 CES 2024서 메타버스 ‘칼리버스’ 첫 공개,"초실감형 메타버스, 1월 9일 글로벌 런칭
롯데정보통신은 1월 9일부터 12일까지 미국 라스베이거스에서 열리는 CES 2024에서 자사의 초실감형 메타버스 플랫폼 ‘칼리버스’를 정식으로 출시한다고 3일 발표했다.
이번 CES2024에서 롯데정보통신은 사실적인 그래픽, 강화된 몰입감, 사용자 참여 콘텐츠 등을 통해 한층 업그레이드된 메타버스를 선보일 예정이며, 1월 9일 글로벌 오프닝을 계획하고 있다.",https://imgnews.pstatic.net/image/009/2024/01/03/0005239033_001_20240103112105182.png?type=w647,https://n.news.naver.com/mnews/article/009/0005239033?sid=105
Custom Controller 3 - CronJob 구현하기,Kubernetes 에는 이미 CronJob 이라는 리소스 타입이 있지만 Kubebuilder 을 이용하여 Custom Controller 로 재작성 해보는 연습을 해보도록 하자 Project 구조 만들기 먼저 Project 구조를 만들기 위해 아래와 같이 kubebuilder init 명령어를 실행한다 mkdir p cronjobkubebuilder cd cronjobkubebuilder kubebuilder init domain tutorialkubebuilderio repo tutorialkubebuilderioproject 도메인을 tutorialkubebuilderio 로 했으므로 모든 API Group 은 grouptutorialkubebuilderio 방식으로 정해지게 된다 또한 특별히 프로젝트 이름은 지정하지 않았는데 projectnamedns1123labelstring 과 같이 옵션을 추가하지 않으면 폴더의 이름이 기본적으로 프로젝트 이름이 된다 여기서 프로젝트명은 DNS1123 label 규칙을 따라야 한다 한가지 주의해야 할 점은 cronjobkubebuilder 디렉토리는 GOPATH 경로 아래에 있어서는 안된다 이는 Go modules 의 규칙 때문인데 좀 더 자세히 알고 싶으면 httpsgodevblogusinggomodules 블로그 포스트를 읽어보자 만들어진 프로젝트의 구조는 다음과 같다 tree L 2 Dockerfile Makefile PROJECT READMEmd cmd maingo config default manager prometheus rbac gomod gosum hack boilerplategotxt 7 directories 8 files gomod 파일은 모듈 디펜던시를 표시하고 Makefile 은 custom controller 를 빌드하고 디플로이 할 수 있다 config 디렉토리 아래에는 Kustomize 로 작성되어 CustomResourceDefinition RBAC WebhookConfiguration 등의 yaml 파일들이 정의되어 있다 특히 configmanager 디렉토리에는 Cluster 에 Custom Controller 를 파드 형태로 배포할 수 있는 Kustomize yaml 이 있고 configrbac 디렉토리에는 서비스 어카운트로 Custom Controller 의 권한이 정의되어 있다 Custom Controller 의 Entrypoint 는 cmdmaingo 파일이다 처음 필요한 모듈을 임포트 한 것을 보면 아래 2개가 보인다 core controllerruntime 라이브러리 기본 controllerruntime 로깅인 Zap package main import flag fmt os k8sioclientgopluginpkgclientauth k8sioapimachinerypkgruntime utilruntime k8sioapimachinerypkgutilruntime clientgoscheme k8sioclientgokubernetesscheme k8sioclientgopluginpkgclientauthgcp ctrl sigsk8siocontrollerruntime sigsk8siocontrollerruntimepkgcache sigsk8siocontrollerruntimepkghealthz sigsk8siocontrollerruntimepkglogzap kubebuilderscaffoldimports 모든 컨트롤러에는 Scheme 이 필요하다 스킴은 Kind 와 Go types 간의 매핑을 제공해 준다 var scheme runtimeNewScheme setupLog ctrlLogWithNamesetup func init utilruntimeMustclientgoschemeAddToSchemescheme kubebuilderscaffoldscheme main function 에는 아래의 내용들이 들어가 있다 기본 플래그 셋업 manager 를 생성하여 모든 Custom Controller 의 실행을 추적하고 shared cache 세팅하고 scheme 을 아규먼트로 넘기주어 클라이언트를 API 서버에 설정한다 manager 를 실행하면 manager 가 모든 컨트롤러와 웹혹을 실행한다 func main var metricsAddr string var enableLeaderElection bool var probeAddr string flagStringVarmetricsAddr metricsbindaddress 8080 The address the metric endpoint binds to flagStringVarprobeAddr healthprobebindaddress 8081 The address the probe endpoint binds to flagBoolVarenableLeaderElection leaderelect false Enable leader election for controller manager Enabling this will ensure there is only one active controller manager opts zapOptions Development true optsBindFlagsflagCommandLine flagParse ctrlSetLoggerzapNewzapUseFlagOptionsopts mgr err ctrlNewManagerctrlGetConfigOrDie ctrlOptions Scheme scheme MetricsBindAddress metricsAddr Port 9443 HealthProbeBindAddress probeAddr LeaderElection enableLeaderElection LeaderElectionID 80807133tutorialkubebuilderio if err nil setupLogErrorerr unable to start manager osExit1 manager 생성 시에 컨트롤러가 특정 네임스페이스의 리소스만을 감시할 수 있도록 할 수 있다 mgr err ctrlNewManagerctrlGetConfigOrDie ctrlOptions Scheme scheme Namespace namespace MetricsBindAddress metricsAddr Port 9443 HealthProbeBindAddress probeAddr LeaderElection enableLeaderElection LeaderElectionID 80807133tutorialkubebuilderio 이렇게 특정 네임스페이스를 지정한 경우에는 권한을 ClusterRole 과 ClusterRoleBinding 에서 Role 과 RoleBinding 으로 변경하는 것을 권장한다 그리고 MutiNamespacedCacheBuilder 를 사용하면 특정 네임스페이스의 묶음의 리소스만을 감시하게 제한할 수 있다 var namespaces string List of Namespaces cacheOptionsNamespaces namespaces mgr err ctrlNewManagerctrlGetConfigOrDie ctrlOptions Scheme scheme NewCache cacheMultiNamespacedCacheBuildernamespaces MetricsBindAddress fmtSprintfsd metricsHost metricsPort Port 9443 HealthProbeBindAddress probeAddr LeaderElection enableLeaderElection LeaderElectionID 80807133tutorialkubebuilderio MultiNamespacedCacheBuilder 는 deprecated api 이므로 cacheOptionsNamespaces 를 사용한다 httpspkggodevsigsk8siocontrollerruntimepkgcacheOptions Groups Versions Kinds and Resources 쿠버네티스에서 API 에 대해서 이야기할 때는 groups versions kinds and resources 4개의 용어를 사용한다 쿠버네티스의 API Group 은 단순히 관련 기능의 모음이다 각 Group 에는 하나 이상의 Version 이 있으며 이름에서 알 수 있듯이 시간이 지남에 따라 API의 작동 방식을 변경할 수 있다 각 API groupversion 에는 하나 이상의 API type 이 포함되며 이를 Kind 라고 부른다 Kind 는 Version 간에 양식을 변경할 수 있지만 각 양식은 어떻게든 다른 양식의 모든 데이터를 저장할 수 있어야 한다데이터를 필드 또는 주석에 저장할 수 있음 즉 이전 API 버전을 사용해도 최신 데이터가 손실되거나 손상되지 않는다 Resource 란 간단히 말해서 API 안에서 Kind 를 사용하는 것이다 종종 Kind 와 Resource 는 일대일로 매핑된다 예를 들어 Pod Resource 는 Pod Kind 에 해당한다 그러나 때로는 여러 Resource 에서 동일한 Kind를 반환할 수도 있다 Scale Kind 는 deploymentsscale 또는 replicasetsscale 과 같은 모든 scale 하위 리소스에 의해 반환된다 이것이 바로 Kubernetes HorizontalPodAutoscaler 가 서로 다른 resource 와 상호 작용할 수 있는 이유다 그러나 CRD를 사용하면 각 Kind 는 단일 resource 에 해당한다 resource 는 항상 소문자이며 관례에 따라 소문자 형태의 Kind를 사용한다 특정 groupversion 에서 어떤 kind 를 지칭할 때는 줄여서 GroupVersionKind 혹은 줄여서 GVK 라고 부른다 같은 방식으로 resource 도 GroupVersionResource 혹은 GVR 이라고 부른다 GVK 는 패키지에서 Go type 에 해당한다 API 는 왜 만들어야 할까 Kind 에 대해서 Custom Resource CR 과 Custom Resource Definition CRD 을 만들어야 한다 그 이유는 CustomResourceDefinitions 으로 Kubernetes API 를 확장할 수 있기 때문이다 새롭게 만드는 API 는 쿠버네티스에게 custom object 를 가리치는 방법이다 기본으로 CRD 는 customized Objects 의 정의이며 CR 은 그것에 대한 인스턴스이다 API 추가 아래 명령으로 새로운 Kind 를 추가하자 kubebuilder create api group batch version v1 kind CronJob Create Resource 와 Create Controller 를 하겠냐고 물으면 y 로 대답한다 tree L 2 Dockerfile Makefile PROJECT READMEmd api v1 bin controllergen cmd maingo config crd default manager prometheus rbac samples gomod gosum hack boilerplategotxt internal controller 이 경우 batchtutorialkubebuilderiov1 에 해당하는 apiv1 디렉토리가 생성된다 apiv1cronjobtypesgo 파일을 보면 모든 쿠버네티스 Kind 에 공통으로 포함된 metadata 를 가리키는 metav1 API group 을 임포트 하고 있다 package v1 import metav1 k8sioapimachinerypkgapismetav1 다음으로 Kind 의 Spec 과 Status 에 대한 type 을 정의 한다 쿠버네티스는 원하는 상태Spec를 실제 클러스터 상태Status 및 외부 상태와 조정한 다음 관찰한 것Status를 기록하는 방식으로 작동한다 따라서 모든 기능 object 는 spec 과 status 를 포함한다 ConfigMap 과 같은 몇몇 타입은 원하는 상태를 인코딩하지 않기 때문에 이 패턴을 따르지 않지만 대부분의 타입은 이 패턴을 따른다 EDIT THIS FILE THIS IS SCAFFOLDING FOR YOU TO OWN NOTE json tags are required Any new fields you add must have json tags for the fields to be serialized CronJobSpec defines the desired state of CronJob type CronJobSpec struct INSERT ADDITIONAL SPEC FIELDS desired state of cluster Important Run make to regenerate code after modifying this file CronJobStatus defines the observed state of CronJob type CronJobStatus struct INSERT ADDITIONAL STATUS FIELD define observed state of cluster Important Run make to regenerate code after modifying this file 실제 Kind 에 해당하는 타입인 CronJob 과 CronJobList 를 정의한다 CronJob 은 루트 타입이며 CronJob kind를 설명한다 모든 쿠버네티스 오브젝트와 마찬가지로 API version 과 Kind 를 설명하는 TypeMeta를 포함하며 name namespace labes 과 같은 것을 보유하는 ObjectMeta 도 포함한다 CronJobList 는 단순히 여러 CronJob 을 위한 컨테이너이다 LIST와 같은 대량 작업에 사용되는 Kind 이다 kubebuilderobjectroottrue kubebuildersubresourcestatus CronJob is the Schema for the cronjobs API type CronJob struct metav1TypeMeta jsoninline metav1ObjectMeta jsonmetadataomitempty Spec CronJobSpec jsonspecomitempty Status CronJobStatus jsonstatusomitempty kubebuilderobjectroottrue CronJobList contains a list of CronJob type CronJobList struct metav1TypeMeta jsoninline metav1ListMeta jsonmetadataomitempty Items CronJob jsonitems 마지막으로 API group 에 Go 타입을 추가한다 이렇게 하면 이 API group 의 타입을 모든 Scheme 에 추가할 수 있다 func init SchemeBuilderRegisterCronJob CronJobList API 설계 쿠버네티스에는 API를 설계하는 방법에 대한 몇 가지 규칙이 있다 즉 직렬화된 모든 필드는 camelCase 여야 하며 JSON 구조체 태그를 사용하여 이를 지정한다 또한 필드가 비어 있을 때 직렬화에서 필드를 생략해야 한다는 것을 표시하기 위해 omitempty 구조체 태그를 사용할 수도 있다 필드는 대부분의 기본 유형을 사용할 수 있다 다만 숫자는 예외이다 API 호환성을 위해 정수의 경우 int32 및 int64 소수의 경우 resourceQuantity 와 같이 3가지 형식의 숫자를 허용한다 Quantity 는 10진수에 대한 특수 표기법으로 머신 간에 이식성을 높이기 위해 명시적으로 고정된 표현을 가지고 있다 예를 들어 2m 값은 십진수 표기법에서 0002 를 의미한다 2Ki 는 십진수로 2048 을 의미하고 2K 는 십진수로 2000 을 의미한다 분수를 지정하려면 정수를 사용할 수 있는 접미사로 전환하면 된다예 25 는 2500m 지원되는 베이스는 두 가지이다 10과 2각각 10진수 및 2진수라고 함이다 10진수는 nomal SI 접미사예 M 및 K로 표시되며 2진수는 mebi 표기법예 Mi 및 Ki으로 지정된다 메가바이트와 메비바이트를 생각하면 된다 우리가 사용하는 또 다른 특수 유형이 하나 더 있는데 바로 metav1Time 이다 이것은 고정된 이식 가능한 직렬화 형식을 가지고 있다는 점을 제외하면 timeTime 과 동일하게 작동한다 package v1 import batchv1 k8sioapibatchv1 corev1 k8sioapicorev1 metav1 k8sioapimachinerypkgapismetav1 EDIT THIS FILE THIS IS SCAFFOLDING FOR YOU TO OWN NOTE json tags are required Any new fields you add must have json tags for the fields to b CronJob 을 세부적으로 살펴보자 먼저 spec 을 보면 spec 에는 원하는 상태가 저장되므로 controller 에 대한 모든 입력 은 여기에 저장된다 기본적으로 크론잡에는 다음과 같은 요소가 필요하다 스케줄 CronJob 내의 cron 실행할 Job 에 대한 template CronJob 내의 job 편하게 만들어줄 몇 가지 추가 기능도 필요하다 job 시작에 대한 deadline 이 deadline 을 놓치면 다음 예정된 시간까지 기다리게 된다 여러 job 이 한 번에 실행될 경우 어떻게 할 것인가기다릴 것인가 기존 job 을 중지할 것인가 둘 다 실행할 것인가 CronJob 에 문제가 있을 경우 CronJob 실행을 일시 중지하는 방법 이전 job 기록에 대한 limit 자신의 상태를 읽지 않기 때문에 job 이 실행되었는지 여부를 추적할 수 있는 다른 방법이 필요하다 이를 위해 적어도 하나의 이전 job 을 사용할 수 있다 CronJobSpec defines the desired state of CronJob type CronJobSpec struct kubebuildervalidationMinLength0 The schedule in Cron format see httpsenwikipediaorgwikiCron Schedule string jsonschedule kubebuildervalidationMinimum0 Optional deadline in seconds for starting the job if it misses scheduled time for any reason Missed jobs executions will be counted as failed ones optional StartingDeadlineSeconds int64 jsonstartingDeadlineSecondsomitempty Specifies how to treat concurrent executions of a Job Valid values are Allow default allows CronJobs to run concurrently Forbid forbids concurrent runs skipping next run if previous run hasnt finished yet Replace cancels currently running job and replaces it with a new one optional ConcurrencyPolicy ConcurrencyPolicy jsonconcurrencyPolicyomitempty This flag tells the controller to suspend subsequent executions it does not apply to already started executions Defaults to false optional Suspend bool jsonsuspendomitempty Specifies the job that will be created when executing a CronJob JobTemplate batchv1JobTemplateSpec jsonjobTemplate kubebuildervalidationMinimum0 The number of successful finished jobs to retain This is a pointer to distinguish between explicit zero and not specified optional SuccessfulJobsHistoryLimit int32 jsonsuccessfulJobsHistoryLimitomitempty kubebuildervalidationMinimum0 The number of failed finished jobs to retain This is a pointer to distinguish between explicit zero and not specified optional FailedJobsHistoryLimit int32 jsonfailedJobsHistoryLimitomitempty ConcurrencyPolicy 는 실제로는 string 이지만 재사용과 유효성 검사를 쉽게 할 수 있으므로 타입을 재정의 했다 ConcurrencyPolicy describes how the job will be handled Only one of the following concurrent policies may be specified If none of the following policies is specified the default one is AllowConcurrent kubebuildervalidationEnumAllowForbidReplace type ConcurrencyPolicy string const AllowConcurrent allows CronJobs to run concurrently AllowConcurrent ConcurrencyPolicy Allow ForbidConcurrent forbids concurrent runs skipping next run if previous hasnt finished yet ForbidConcurrent ConcurrencyPolicy Forbid ReplaceConcurrent cancels currently running job and replaces it with a new one ReplaceConcurrent ConcurrencyPolicy Replace 다음은 관찰된 상태를 저장하는 status 를 디자인해 보자 현재 실행중인 job 목록과 마지막으로 job 을 성공적으로 실행한 시간을 유지한다 그리고 직렬화를 위해서 timeTime 대신 metav1Time 을 사용한다 CronJobStatus defines the observed state of CronJob type CronJobStatus struct INSERT ADDITIONAL STATUS FIELD define observed state of cluster Important Run make to regenerate code after modifying this file A list of pointers to currently running jobs optional Active corev1ObjectReference jsonactiveomitempty Information when was the last time the job was successfully scheduled optional LastScheduleTime metav1Time jsonlastScheduleTimeomitempty Controller 구현 컨트롤러는 쿠버네티스와 모든 operator 의 핵심이다 컨트롤러의 역할은 주어진 오브젝트에 대해 실세계의 실제 상태클러스터 상태와 잠재적으로 외부 상태예 Kubelet의 경우 컨테이너 실행 또는 Cloud Provider 의 경우 로드밸런서가 오브젝트의 원하는 상태와 일치하는지 확인하는 것이다 각 컨트롤러는 하나의 루트 Kind 에 중점을 두지만 다른 Kind 와 상호 작용할 수 있다 이 프로세스를 reconciling 이라고 부른다 controllerruntime 에서 특정 kind 에 대한 reconciling 을 구현하는 로직을 Reconciler 라고 한다 internalcontrollercronjobcontrollergo 파일을 살펴 보자 기본으로 임포트하는 모듈이 있는데 core controllerruntime 라이브러리와 client 패키지 API 타입 패키지가 있다 package controllers import context k8sioapimachinerypkgruntime ctrl sigsk8siocontrollerruntime sigsk8siocontrollerruntimepkgclient sigsk8siocontrollerruntimepkglog batchv1 tutorialkubebuilderioprojectapiv1 컨트롤러의 기본 로직은 다음과 같다 명명된 CronJob을 로드한다 모든 active job 을 나열하고 status 를 업데이트 한다 히스토리 수 제한에 따라 오래된 job 을 정리한다 Suspend 값이 세팅되었는지 확인 값이 세팅된 경우 다른 작업을 수행하지 않음 다음 예약된 실행 가져오기 새로운 job 이 스케줄에 맞고 deadline 이 지나지 않았으며 동시성 정책에 의해 차단되지 않은 경우 실행 실행 중인 job 이 보이거나 자동으로 수행됨 다음 예약된 실행 시간이 되면 Requeue 한다 임포트 모듈을 추가한다 package controller import context fmt sort time githubcomrobfigcron kbatch k8sioapibatchv1 corev1 k8sioapicorev1 metav1 k8sioapimachinerypkgapismetav1 k8sioapimachinerypkgruntime ref k8sioclientgotoolsreference ctrl sigsk8siocontrollerruntime sigsk8siocontrollerruntimepkgclient sigsk8siocontrollerruntimepkglog batchv1 tutorialkubebuilderioprojectapiv1 테스트를 위해서 Clock 을 추가한다 CronJobReconciler reconciles a CronJob object type CronJobReconciler struct clientClient Scheme runtimeScheme Clock type realClock struct func realClock Now timeTime return timeNow clock knows how to get the current time It can be used to fake out timing for testing type Clock interface Now timeTime RBAC 을 위해 batch group 의 job 을 핸들링 할 수 있는 권한을 추가한다 kubebuilderrbacgroupsbatchtutorialkubebuilderioresourcescronjobsverbsgetlistwatchcreateupdatepatchdelete kubebuilderrbacgroupsbatchtutorialkubebuilderioresourcescronjobsstatusverbsgetupdatepatch kubebuilderrbacgroupsbatchtutorialkubebuilderioresourcescronjobsfinalizersverbsupdate kubebuilderrbacgroupsbatchresourcesjobsverbsgetlistwatchcreateupdatepatchdelete kubebuilderrbacgroupsbatchresourcesjobsstatusverbsget annotation 을 위한 변수를 추가한다 var scheduledTimeAnnotation batchtutorialkubebuilderioscheduledat Reconcile is part of the main kubernetes reconciliation loop which aims to move the current state of the cluster closer to the desired state TODOuser Modify the Reconcile function to compare the state specified by the CronJob object against the actual cluster state and then perform operations to make the cluster state reflect the state specified by the user For more details check Reconcile and its Result here httpspkggodevsigsk8siocontrollerruntimev0150pkgreconcile func r CronJobReconciler Reconcilectx contextContext req ctrlRequest ctrlResult error log logFromContextctx 1 이름으로 CronJob 을 로드한다 client 를 사용하여 CronJob 을 가져온다 모든 client 의 메소드에는 취소가 가능하게 context 를 아규먼트로 받는다 var cronJob batchv1CronJob if err rGetctx reqNamespacedName cronJob err nil logErrorerr unable to fetch CronJob well ignore notfound errors since they cant be fixed by an immediate requeue well need to wait for a new notification and we can get them on deleted requests return ctrlResult clientIgnoreNotFounderr 2 모든 active job 을 나열하고 status 를 업데이트 한다 var childJobs kbatchJobList if err rListctx childJobs clientInNamespacereqNamespace clientMatchingFieldsjobOwnerKey reqName err nil logErrorerr unable to list child Jobs return ctrlResult err active job 을 조회했으면 이를 active successful failded job 으로 분류한다 find the active list of jobs var activeJobs kbatchJob var successfulJobs kbatchJob var failedJobs kbatchJob var mostRecentTime timeTime find the last run so we can update the status isJobFinished funcjob kbatchJob bool kbatchJobConditionType for c range jobStatusConditions if cType kbatchJobComplete cType kbatchJobFailed cStatus corev1ConditionTrue return true cType return false getScheduledTimeForJob funcjob kbatchJob timeTime error timeRaw jobAnnotationsscheduledTimeAnnotation if lentimeRaw 0 return nil nil timeParsed err timeParsetimeRFC3339 timeRaw if err nil return nil err return timeParsed nil for i job range childJobsItems finishedType isJobFinishedjob switch finishedType case ongoing activeJobs appendactiveJobs childJobsItemsi case kbatchJobFailed failedJobs appendfailedJobs childJobsItemsi case kbatchJobComplete successfulJobs appendsuccessfulJobs childJobsItemsi Well store the launch time in an annotation so well reconstitute that from the active jobs themselves scheduledTimeForJob err getScheduledTimeForJobjob if err nil logErrorerr unable to parse schedule time for child job job job continue if scheduledTimeForJob nil if mostRecentTime nil mostRecentTime scheduledTimeForJob else if mostRecentTimeBeforescheduledTimeForJob mostRecentTime scheduledTimeForJob if mostRecentTime nil cronJobStatusLastScheduleTime metav1TimeTime mostRecentTime else cronJobStatusLastScheduleTime nil cronJobStatusActive nil for activeJob range activeJobs jobRef err refGetReferencerScheme activeJob if err nil logErrorerr unable to make reference to active job job activeJob continue cronJobStatusActive appendcronJobStatusActive jobRef 디버깅을 위해서 log 를 남긴다 logV1Infojob count active jobs lenactiveJobs successful jobs lensuccessfulJobs failed jobs lenfailedJobs status 를 업데이트 한다 if err rStatusUpdatectx cronJob err nil logErrorerr unable to update CronJob status return ctrlResult err 3 히스토리 수 제한에 따른 오래된 job 삭제하기 NB deleting these are best effort if we fail on a particular one we wont requeue just to finish the deleting if cronJobSpecFailedJobsHistoryLimit nil sortSlicefailedJobs funci j int bool if failedJobsiStatusStartTime nil return failedJobsjStatusStartTime nil return failedJobsiStatusStartTimeBeforefailedJobsjStatusStartTime for i job range failedJobs if int32i int32lenfailedJobscronJobSpecFailedJobsHistoryLimit break if err rDeletectx job clientPropagationPolicymetav1DeletePropagationBackground clientIgnoreNotFounderr nil logErrorerr unable to delete old failed job job job else logV0Infodeleted old failed job job job if cronJobSpecSuccessfulJobsHistoryLimit nil sortSlicesuccessfulJobs funci j int bool if successfulJobsiStatusStartTime nil return successfulJobsjStatusStartTime nil return successfulJobsiStatusStartTimeBeforesuccessfulJobsjStatusStartTime for i job range successfulJobs if int32i int32lensuccessfulJobscronJobSpecSuccessfulJobsHistoryLimit break if err rDeletectx job clientPropagationPolicymetav1DeletePropagationBackground err nil logErrorerr unable to delete old successful job job job else logV0Infodeleted old successful job job job 4 Suspend 값이 세팅되었는지 확인 CronJob 객체에 suspend 값이 세팅되어 있다면 CronJob 을 일시 중단한다 CronJob 을 삭제하지 않고 잠시 멈추고 싶을 때 사용할 수 있다 if cronJobSpecSuspend nil cronJobSpecSuspend logV1Infocronjob suspended skipping return ctrlResult nil 5 다음 예약된 실행 가져오기 잠시 멈춤 상태가 아니라면 다음 스케줄을 가져온다 getNextSchedule funccronJob batchv1CronJob now timeTime lastMissed timeTime next timeTime err error sched err cronParseStandardcronJobSpecSchedule if err nil return timeTime timeTime fmtErrorfUnparseable schedule q v cronJobSpecSchedule err for optimization purposes cheat a bit and start from our last observed run time we could reconstitute this here but theres not much point since weve just updated it var earliestTime timeTime if cronJobStatusLastScheduleTime nil earliestTime cronJobStatusLastScheduleTimeTime else earliestTime cronJobObjectMetaCreationTimestampTime if cronJobSpecStartingDeadlineSeconds nil controller is not going to schedule anything below this point schedulingDeadline nowAddtimeSecond timeDurationcronJobSpecStartingDeadlineSeconds if schedulingDeadlineAfterearliestTime earliestTime schedulingDeadline if earliestTimeAfternow return timeTime schedNextnow nil starts 0 for t schedNextearliestTime tAfternow t schedNextt lastMissed t An object might miss several starts For example if controller gets wedged on Friday at 501pm when everyone has gone home and someone comes in on Tuesday AM and discovers the problem and restarts the controller then all the hourly jobs more than 80 of them for one hourly scheduledJob should all start running with no further intervention if the scheduledJob allows concurrency and late starts However if there is a bug somewhere or incorrect clock on controllers server or apiservers for setting creationTimestamp then there could be so many missed start times it could be off by decades or more that it would eat up all the CPU and memory of this controller In that case we want to not try to list all the missed start times starts if starts 100 We cant get the most recent times so just return an empty slice return timeTime timeTime fmtErrorfToo many missed start times 100 Set or decrease specstartingDeadlineSeconds or check clock skew return lastMissed schedNextnow nil figure out the next times that we need to create jobs at or anything we missed missedRun nextRun err getNextSchedulecronJob rNow if err nil logErrorerr unable to figure out CronJob schedule we dont really care about requeuing until we get an update that fixes the schedule so dont return an error return ctrlResult nil requeue 할 값을 준비만 해 놓는다 scheduledResult ctrlResultRequeueAfter nextRunSubrNow save this so we can reuse it elsewhere log logWithValuesnow rNow next run nextRun 6 새로운 job 이 스케줄에 맞고 deadline 이 지나지 않았으며 동시성 정책에 의해 차단되지 않은 경우 실행 if missedRunIsZero logV1Infono upcoming scheduled times sleeping until next return scheduledResult nil make sure were not too late to start the run log logWithValuescurrent run missedRun tooLate false if cronJobSpecStartingDeadlineSeconds nil tooLate missedRunAddtimeDurationcronJobSpecStartingDeadlineSeconds timeSecondBeforerNow if tooLate logV1Infomissed starting deadline for last run sleeping till next TODOdirectxman12 events return scheduledResult nil figure out how to run this job concurrency policy might forbid us from running multiple at the same time if cronJobSpecConcurrencyPolicy batchv1ForbidConcurrent lenactiveJobs 0 logV1Infoconcurrency policy blocks concurrent runs skipping num active lenactiveJobs return scheduledResult nil or instruct us to replace existing ones if cronJobSpecConcurrencyPolicy batchv1ReplaceConcurrent for activeJob range activeJobs we dont care if the job was already deleted if err rDeletectx activeJob clientPropagationPolicymetav1DeletePropagationBackground clientIgnoreNotFounderr nil logErrorerr unable to delete active job job activeJob return ctrlResult err constructJobForCronJob funccronJob batchv1CronJob scheduledTime timeTime kbatchJob error We want job names for a given nominal start time to have a deterministic name to avoid the same job being created twice name fmtSprintfsd cronJobName scheduledTimeUnix job kbatchJob ObjectMeta metav1ObjectMeta Labels makemapstringstring Annotations makemapstringstring Name name Namespace cronJobNamespace Spec cronJobSpecJobTemplateSpecDeepCopy for k v range cronJobSpecJobTemplateAnnotations jobAnnotationsk v jobAnnotationsscheduledTimeAnnotation scheduledTimeFormattimeRFC3339 for k v range cronJobSpecJobTemplateLabels jobLabelsk v if err ctrlSetControllerReferencecronJob job rScheme err nil return nil err return job nil actually make the job job err constructJobForCronJobcronJob missedRun if err nil logErrorerr unable to construct job from template dont bother requeuing until we get a change to the spec return scheduledResult nil and create it on the cluster if err rCreatectx job err nil logErrorerr unable to create Job for CronJob job job return ctrlResult err logV1Infocreated Job for CronJob run job job 7 실행 중인 job 이 보이거나 자동으로 수행됨 다음 예약된 실행 시간이 되면 Requeue 한다 well requeue once we see the running job and update our status return scheduledResult nil Setup var jobOwnerKey metadatacontroller apiGVStr batchv1GroupVersionString SetupWithManager sets up the controller with the Manager func r CronJobReconciler SetupWithManagermgr ctrlManager error set up a real clock since were not in a test if rClock nil rClock realClock if err mgrGetFieldIndexerIndexFieldcontextBackground kbatchJob jobOwnerKey funcrawObj clientObject string grab the job object extract the owner job rawObjkbatchJob owner metav1GetControllerOfjob if owner nil return nil make sure its a CronJob if ownerAPIVersion apiGVStr ownerKind CronJob return nil and if so return it return stringownerName err nil return err return ctrlNewControllerManagedBymgr Forbatchv1CronJob OwnskbatchJob Completer Webhook 생성 kubebuilder create webhook group batch version v1 kind CronJob defaulting programmaticvalidation apiv1cronjobwebhookgo 파일이 생성된다 해당 파일에 체크 로직을 추가한다 Default implements webhookDefaulter so a webhook will be registered for the type func r CronJob Default cronjoblogInfodefault name rName if rSpecConcurrencyPolicy rSpecConcurrencyPolicy AllowConcurrent if rSpecSuspend nil rSpecSuspend newbool if rSpecSuccessfulJobsHistoryLimit nil rSpecSuccessfulJobsHistoryLimit newint32 rSpecSuccessfulJobsHistoryLimit 3 if rSpecFailedJobsHistoryLimit nil rSpecFailedJobsHistoryLimit newint32 rSpecFailedJobsHistoryLimit 1 var webhookValidator CronJob ValidateCreate implements webhookValidator so a webhook will be registered for the type func r CronJob ValidateCreate error cronjoblogInfovalidate create name rName return rvalidateCronJob ValidateUpdate implements webhookValidator so a webhook will be registered for the type func r CronJob ValidateUpdateold runtimeObject error cronjoblogInfovalidate update name rName return rvalidateCronJob ValidateDelete implements webhookValidator so a webhook will be registered for the type func r CronJob ValidateDelete error cronjoblogInfovalidate delete name rName TODOuser fill in your validation logic upon object deletion return nil func r CronJob validateCronJob error var allErrs fieldErrorList if err rvalidateCronJobName err nil allErrs appendallErrs err if err rvalidateCronJobSpec err nil allErrs appendallErrs err if lenallErrs 0 return nil return apierrorsNewInvalid schemaGroupKindGroup batchtutorialkubebuilderio Kind CronJob rName allErrs func r CronJob validateCronJobSpec fieldError The field helpers from the kubernetes API machinery help us return nicely structured validation errors return validateScheduleFormat rSpecSchedule fieldNewPathspecChildschedule func validateScheduleFormatschedule string fldPath fieldPath fieldError if err cronParseStandardschedule err nil return fieldInvalidfldPath schedule errError return nil func r CronJob validateCronJobName fieldError if lenrObjectMetaName validationutilsDNS1035LabelMaxLength11 The job name length is 63 character like all Kubernetes objects which must fit in a DNS subdomain The cronjob controller appends a 11character suffix to the cronjob TIMESTAMP when creating a job The job name length limit is 63 characters Therefore cronjob names must have length 631152 If we dont validate this here then job creation will fail later return fieldInvalidfieldNewPathmetadataChildname rName must be no more than 52 characters return nil Controller 배포 및 실행 CR 과 CRD yaml 을 만드는 명령어를 수행한다 make manifests CRD 를 배포한다 make install WebHook 를 로컬에서 다른 터미널로 실행한다 export ENABLEWEBHOOKSfalse make run configsamplesbatchv1cronjobyaml 파일에 값을 추가한다 apiVersion batchtutorialkubebuilderiov1 kind CronJob metadata labels appkubernetesioname cronjob appkubernetesioinstance cronjobsample appkubernetesiopartof cronjobkubebuilder appkubernetesiomanagedby kustomize appkubernetesiocreatedby cronjobkubebuilder name cronjobsample spec schedule 1 startingDeadlineSeconds 60 concurrencyPolicy Allow explicitly specify but Allow is also default jobTemplate spec template spec containers name hello image busybox args binsh c date echo Hello from the Kubernetes cluster restartPolicy OnFailure Reference site httpsbookkubebuilderiocronjobtutorial cronJob webhook source httpsgithubcomkubernetessigskubebuilderblobmasterdocsbooksrccronjobtutorialtestdataprojectapiv1cronjobwebhookgo Writing a Kubernetes Operator from Scratch Using Kubebuilder httpswwwyoutubecomwatchvLLVoyXjYlYMlistPL8pIiPkgexmmHAppEre9eHAiYqLxGFbOYindex7 PVC Operator Sample Source httpsgithubcomcivooperatordemoblobmaincontrollersdemovolumecontrollergo elastic cloudonk8s source httpsgithubcomelasticcloudonk8sblobmainpkgapiselasticsearchv1elasticsearchtypesgo httpsgithubcomelasticcloudonk8sblobmainpkgcontrollerelasticsearchelasticsearchcontrollergo Develop on Kubernetes Series Operator Dev Understanding and Dissecting Kubebuilder httpsyashkukreja98mediumcomdeveloponkubernetesseriesoperatordevunderstandinganddissectingkubebuilder4321d3ecd7d6 Learning Concurrent Reconciling httpsopenkruiseiobloglearningconcurrentreconciling Operator Pattern httpskubernetesiodocsconceptsextendkubernetesoperator Best practices for building Kubernetes Operators and stateful apps httpscloudgooglecomblogproductscontainerskubernetesbestpracticesforbuildingkubernetesoperatorsandstatefulapps Kubernetes Controllers at Scale Clients Caches Conflicts Patches Explained httpsmediumcomtimeberttkubernetescontrollersatscaleclientscachesconflictspatchesexplainedaa0f7a8b4332 Kubernetes API guidelines httpsgithubcomkubernetescommunityblobmastercontributorsdevelsigarchitectureapiconventionsmd Golang controllerruntime httpspkggodevsigsk8siocontrollerruntime Extend the Kubernetes API with CustomResourceDefinitions httpskubernetesiodocstasksextendkubernetescustomresourcescustomresourcedefinitions,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165109&boardType=techBlog&searchData=&page=&subIndex=
"네이버, 뉴스부문 조직개편…최수연 대표가 직접 챙긴다","뉴스 서비스 최수연 대표 직속 편제
네이버가 뉴스 서비스 부문을 최고경영자(CEO)인 최수연 대표 직속으로 개편했다.
4일 IT 업계에 따르면 네이버는 지난 1일 조직개편을 통해 뉴스 서비스 부문을 최 대표 직속으로 편제했다.
최 대표 직속으로 운영하는 조직으로 AI 안전성을 연구해 책임감 있는 AI 개발을 이끈다.",https://imgnews.pstatic.net/image/277/2024/01/04/0005362871_001_20240104110501352.png?type=w647,https://n.news.naver.com/mnews/article/277/0005362871?sid=105
"제2회 카카오테크밋 후기 – Open, Share and Grow Together",공개기술세미나 카카오테크밋Kakao Tech Meet의 두 번째 행사를 7월 18일 화요일 저녁 카카오판교아지트에서 진행했습니다 이번에도 많은 분들이 신청해 주시고 참석하셨는데요 현장의 뒷이야기를 몇 가지 나눠보겠습니다 오프라인에서만 진행되었던 첫회차와 달리 2회 차에는 ZOOM으로 라이브 스트리밍을 진행했어요 참가신청을 해주셨지만 오프라인으로 모시지 못한 분들께 신청 시 작성해 주셨던 이메일 주소로 온라인 접속 정보를 보내드렸어요오프라인에서도 더 많은 분들을 카카오판교아지트로 초대하고자 장소를 옮겨서 준비해 보았습니다 1회 차보다 많은 좌석을 준비했는데도 공간을 더 꽉 채울 만큼 많은 분들께서 참석해 주셨어요 일정을 기억하고 발걸음 해주신 온오프라인 참석자 한 분 한 분께 감사합니다 그리고 카카오테크밋 행사의 목표와 가치를 담은 브랜드 아이덴티티를 새롭게 개발하고 적용했어요개발자 커뮤니티의 일원으로 기술을 투명하게 오픈하여 그 안에서 서로 시너지를 내며 함께 성장하고자 하는 카카오 기술의 문화와 태도를 담아내는 것이 가장 큰 목표였어요 그렇기에 소통을 상징하는 말풍선이 만나 투명하게 겹쳐지도록 표현하고 그 안에 카카오테크밋의 가치를 표현하는 이스터에그 메시지를 담아 이러한 의도를 전달하고자 했습니다 카카오테크밋이 지향하는 핵심 가치 세 가지는 1 Open 2 Share 3 Grow Together인데요 각 키워드를 생각하며 이번 행사를 회상하려 합니다 이번에도 많은 개발자분들께서 온오프라인으로 찾아와 주셨어요 앞으로도 테크밋을 더 많은 주제로 더 많은 개발자분들에게 활짝 오픈할 수 있도록 다양한 고민을 하고 있어요 서로 다른 배경의 개발자들이 더 많이 모이면 더 새로운 관점들을 만들어낼 수 있을 거라고 기대하기 때문이에요오픈마인드 즉 열린 생각은 듣는 마음인 것 같아요 운영진의 역할에도 적용하여 설문이나 문의로 보내주시는 다양한 피드백에 대해 듣는 마음으로 늘 고민해서 참가자분들께 더 즐거운 경험을 열어드리고 싶습니다 리팩토링 배치 애자일의 주제로 세 분의 발표자가 지식과 경험을 공유했어요 가장 활발한 교류가 일어나는 패널토의는 아직은 오프라인으로만 진행하고 있습니다 참가자분들의 다양한 사전질문을 추리고 묶어서 FAQ 등을 먼저 다루었고요 당일에 오픈채팅방과 현장에서 질문해 주신 내용에 대한 토의도 했습니다 소속도 다르고 하시는 일도 다양한 개발자분들이지만 공통의 관심사 하나로 모여서 늦은 시간까지 대화하시는 모습이 인상적이었어요 참가자분들이 테크밋에 가장 기대하셨던 것은 기술적인 세션 내용과 질의응답이었고 이 부분에 대한 만족도가 가장 높으셨어요 그래서 테크밋 준비에서 가장 중요한 것은 결국 발표자분들이 준비해 주신 콘텐츠라는 당연한 사실을 새삼 알게 되었습니다 다음 회차들에서 발표자 크루들이 더 좋은 콘텐츠를 준비하고 더 전달력 있게 소통하실 수 있도록 잘 준비하고 돕겠습니다 테크밋의 본질은 기술에 대한 지식과 경험을 공유하는 것이니까 본질에서 초점을 잃지 않을게요 행사를 마치고 오픈채팅방에 발표자료와 VOD를 문의하시는 글이 올라왔는데요 한 참가자분께서 정성스럽게 자신이 알고 계신 정보들을 정리하여 글을 남겨주신 것이 인상적이었습니다 참가자분들과 발표자분들이 서로 더 많은 접점에서 건강한 질문과 도움을 주고받으며 함께 성장하실 수 있는 문화를 만들어 갈게요 성장은 과정이니까 함께 성장한다면 서로를 지켜봐야 한다는 생각을 했습니다 많은 분들이 설문조사에 또 올게요 또 오고 싶어요 매회 참석할게요와 같은 메시지를 남겨주셨어요 한 번의 인연으로 끝나지 않고 여러분과 지속적으로 만나며 같이 성장하길 기대합니다 두 번째 테크밋도 카카오테크 기술블로그와 페이스북을 통해서 소식을 전해드렸어요 많은 분들께서 이 기술블로그를 보고 행사에 참여해 주셨다고 말씀하셨는데요 사실 가장 많은 분들은 지인의 소개추천을 통해 테크밋을 알게 되셨다고 설문조사에 응답하셨어요 이제 2회 차일 뿐인데 벌써 주변에 테크밋을 소개해주시는 분들께 감사드립니다 추천해 주신 분과 추천받으신 분들이 모두모두 함께 오실 수 있을 만큼 테크밋의 오프라인 규모도 더 성장할 수 있으면 좋겠습니다테크밋의 심볼과 같이 카카오의 크루들과 외부 개발자분들이 열린 마음으로 서로 만났을 때 새로운 관점과 시너지가 창출되는 자리로서 앞으로도 카카오테크밋을 만들어가겠습니다 물론 함께 만들어갈 거예요2회 차 VOD도 얼른 기술블로그 및 유튜브 채널을 통해 공유드리겠습니다 카카오톡 Java App Server Refactoring 후기 Spring Batch 애플리케이션 성능 향상을 위한 주요 팁애자일로 개발하면 빨리 끝난다던데 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/07/27/2nd-kakao-tech-meet-review/
Anaconda 꼭 사서 쓰세요. 아니라면 conda-forge!,안녕하세요 Python 개발 환경 만들면서 Anaconda 많이 사용하시지 않나요 Python은 간단한 업무 자동화부터 데이터 분석 인공지능 학습 모델링 작업 등에 많이 사용되고 있는데요 여러 Python 프로젝트 개발을 수행하다 보면 package 버전이 충돌하는 불편함이 생길 수 있습니다 Anaconda는 개발 프로젝트별로 가상 환경을 제공하여 버전 충돌을 방지할 수 있다는 장점이 있으며 홈페이지에서 쉽게 다운받아 설치가 간단하여 널리 사용되고 있습니다 httpswwwanacondacom 그런데 Anaconda는 사서 쓰셔야 합니다 2020년 9월 Anaconda사는 서비스 약관Terms of Service를 변경하여 200명 이상의 직원이 있는 기업 또는 정부 조직이 Anaconda Repository를 사용하는 경우 유료로 구매하게 하였습니다 따라서 200명 이상의 기업에서 근무하는 개발자라면 Anaconda 웹사이트에서 Pro 이상의 라이선스를 구매해야 합니다 httpswwwanacondacompricing 조금 자세히 살펴보면 일반적으로 Anaconda 설치를 위해서는 Anaconda 홈페이지에서 Anaconda Distribution을 무료로 다운 받을 수 있습니다 httpswwwanacondacomproductsdistribution 이를 설치하면 conda package manager와 더불어 Python 및 150여개의 package가 함께 설치되어 손쉽게 개발 환경을 구성할 수 있습니다 Anaconda사는 Anaconda Repository를 호스팅하며 8천 개 이상의 오픈소스 package를 제공하고 사용자는 conda install PACKAGENAME 명령어로 이들 package를 안정적으로 설치관리 할 수 있습니다 httpsrepoanacondacom 그런데 바로 이 Anaconda Repository의 서비스 약관이 2020년 9월에 변경된 것이고 commercial activity 목적일 때에는 Anaconda Repository의 무료 사용이 불가능해졌습니다 많은 개발자가 Anaconda Distribution을 쉽게 다운받아서 사용하지만 이 과정에서 Anconda Repository를 사용하게 되는데요 200명 이상 기업의 개발자라면 의도하지는 않았지만 Anaconda의 서비스 약관을 위반하게 되는 것이고 이를 방지하기 위해서는 꼭 Anaconda Pro 이상을 구매하셔야 합니다 참고로 Miniconda는 Anaconda와 마찬가지로 conda package manager와 Python 및 최소한의 dependency를 설치하는 소프트웨어 패키지인데요 Miniconda를 사용하면서도 마찬가지로 Anaconda Repository에 Access하여 package를 다운 받아 사용하게 되며 Anaconda와 동일하게 유료 구매 대상으로 간주될 수 있습니다 httpsdocscondaioenlatestminicondahtml 결국 200인 이상 기업의 개발자가 Anaconda Distribution을 무료로 다운받아서 사용하더라도 당장 비용이 청구되거나 기능이 막히지는 않겠지만 Anaconda의 안정적인 발전을 위해서도 200인 이상의 기업 개발자는 자발적으로 구매하여 사용하는 것이 좋을 것 같습니다 물론 어느 순간 회사로 라이선스 위반 통지 및 비용 청구서가 날아올 수도 있습니다 대안은 있습니다 condaforge Anaconda사는 conda라는 package manager를 오픈소스로 공개하여 관리하고 있습니다 conda 자체는 BSD3Clause 라이선스로 공개된 오픈소스여서 기업이 무료로 사용하는 데 문제되지 않습니다 httpsgithubcomcondaconda conda는 package 설치관리를 위해 설치할 package를 찾기 위한 저장소 위치가 필요한데요 이를 channel이라고 칭합니다 기본 channel이 바로 Anaconda Repository입니다 그런데 community 기반의 repository가 또 있습니다 바로 condaforge인데요 httpscondaforgeorg conda를 설치하고 channel에 condaforge를 추가할 수 있습니다 conda config add channels condaforge conda config set channelpriority strict 이렇게 하면 Anaconda Repository를 사용하지 않기 때문에 위에서 설명한 서비스 약관을 위반하지 않고 conda를 사용할 수 있습니다 Anaconda사의 CEO인 Peter Wang은 Miniconda를 다운 받아서 conda config를 condaforge로 변경할 경우 무료로 사용할 수 있다고 직접 밝힌 바 있습니다 httpswwwredditcomrPythoncommentsiqsk3ycommentg4xuabr Anaconda Repository를 가리키는 defaults channel을 아예 삭제해 버리면 보다 확실하게 Anaconda Repository 사용을 제한할 수 있습니다 conda config remove channels defaults 원하는대로 channel이 변경되었는지는 아래 명령어로 확인할 수 있습니다 변경 전 conda config show channels channels defaults 변경 후 conda config show channels channels condaforge Miniforge는 설치 시 channel에 condaforge 를 추가합니다 한 걸음 더 나아가서 Miniforge는 conda 설치를 위한 최소의 installer를 제공하는 오픈소스 프로젝트로 기본 설치 시 channel에 condaforge를 추가합니다 또한 Miniforge는 Apple M1을 포함한 다양한 CPU 아키텍처를 지원한다고도 알려져 있습니다 httpsgithubcomcondaforgeminiforge 따라서 Anaconda 대신 Miniforge를 설치한다면 비교적 수월하게 라이선스 위반 없이 conda package manager로 개발 환경을 구성할 수 있는 것으로 보입니다 한 가지 특이한 점은 condaforge 운영에는 막대한 호스팅 비용이 필요한데 이를 Anaconda사가 지불하고 있다는 점입니다 Anaconda사는 condaforge를 계속 무료로 유지하기 위해서라도 Anaconda Repository의 서비스 약관 변경을 통한 수익이 필요했다고 설명합니다 결론적으로 개발 편의성과 안정성을 고려하여 가급적 Anaconda Pro를 구매하여 사용하는 것이 좋겠습니다 구매 전까지 라이선스 이슈 방지를 위해 Miniconda condaforge 조합 혹은 Miniforge를 대안으로 고려할 수 있을 것 같습니다 혹시 틀린 내용이 있다면 알려주시면 감사하겠습니다 감사합니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164615&boardType=techBlog&searchData=&page=&subIndex=
오픈소스화로 독점하기 (feat. 경제적 관점),SW로 독점하기 제일 쉬운 방법 기업이 특정 기술을 독점 하려면 보통 특허를 내거나 철저히 비밀로 유지해 숨깁니다 그러나 SW의 경우 기술의 독점을 하려면 오픈소스화가 좋은 전략입니다 오픈소스로 만들어 기술 생태계를 만들고 더 많은 사람을 끌어들이면 대체 불가능해집니다 비슷한 기술을 개발하고 있던 기업들도 결국 오픈소스를 쓰게 되는거죠 하지만 공개를 해도 그 기술을 제일 잘 활용하는 기업은 보통의 경우 공개한 기업일 것입니다 Google이 개발한 Kubernetes 소스코드를 공개하는 것은 경제적으로도 합리적인 것입니다 SW 개발자의 노력을 누군가는 대가없이 쓰는데도 왜 그런걸까요 그것은 사용자의 관심과 들이는 시간이 보이지 않는 대가이기 때문입니다 한번쓰면 바꾸기 힘들다 전환비용 사용자가 오픈소스를 쓰게 되면 다른 제품으로 전환하기 힘들어집니다 먼저 오픈소스에서 유료 제품을 쓰게되면 라이센스 비용이 발생할 수 있습니다 다른 오픈소스로 전환하려고 해도 새로운 프레임워크라이브러리 학습 비용이 듭니다 혼자가 아니라 팀원 전체 오픈소스 생태계의 방대한 자료도 포기해야 합니다 왜 과거에는 폐쇄소스가 나았을까 현재는 폐쇄소스가 대세였던 과거 2000년대와는 달라진 점이 많이 있습니다 첫번째로 Git으로 대표되는 협력 툴의 발전이 있었습니다 대규모로 개발 인력이 붙어도 효율적으로 개발할 수 있게 된 것이죠 컴퓨터 HW 비용이 줄고 인터넷의 보급률도 올라갔습니다 그래서 대중들의 프로그래밍 접근성도 많이 올라갔습니다 또한 Python JavaScript와 같이 외부 라이브러리를 쉽게 쓸 수 있는 언어가 떠올랐습니다 pip npm LLM의 승자는 오픈소스 LLM chatGPT로 대표되는 LLM Large Language Model은 기술 경쟁이 치열합니다 그러던 와중 메타에서 개발하던 LLaMa 모델의 가중치가 2023년 3월경 유출됐습니다 이 유출로 오히려 메타가 덕을 봤다는 평이 많은데요 LLaMa 기반 모델이 커뮤니티에 의해 엄청난 속도로 발전하게 됐죠 Google의 리서처는 오픈소스 기반 LLM이 결국 승자가 될거라 말했습니다 googlewehavenomoatandneither 누구나 오픈소스에 기여할 수 있다 지금까지 오픈소스 전략에 대한 이해를 높여봤습니다 이제 오픈소스 생태계에 참여해봐겠죠 오픈소스 기여에 어려움을 느낄 필요는 없습니다 아마도 알게 모르게 이미 기여를 했을겁니다 블로그 운영하시나요 제일 쉬운 기여 방법은 커뮤니티에 참여하는 것입니다 블로그에 오픈소스 사용기를 포스팅해볼 수 도 있고 StackOverflow 같이 오픈소스 질문에 답변을 남길 수 도 있습니다 기능을 추가하거나 버그를 수정해 Pull request 하는 것만이 오픈소스에 기여하는 방법은 아닙니다 SK하이닉스에서의 오픈소스 활용 SK하이닉스에서도 오픈소스를 적극 활용중입니다 저는 Django 기반의 웹앱과 Django REST Framework 를 이용한 API 개발을 하고 있습니다 사내 생태계를 키우기 위해 내부 구성원을 위한 Django 교육 프로그램도 만들고 있습니다 저와 같이 SW엔지니어링으로 반도체 업계를 더 키워볼 분들 환영합니다 입사하면 연락주세요,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165121&boardType=techBlog&searchData=&page=&subIndex=
카프카 커넥트를 데이터 파이프라인으로 사용하는 이유? kafka-sink-connector 오픈소스 언빡싱!,들어가며안녕하세요 광고추천팀에서 데이터 엔지니어로 일하고 있는 cory 입니다 저는 광고추천팀에서 카프카Kafka 기반 스트림 데이터 플랫폼을 개발 및 운영하고 있습니다 광고추천팀에서는 노출impression 클릭click 전환conversion 등의 광고 로그 데이터를 원천 데이터라고 부르며 이 원천 데이터 분석을 기반으로 개인화된 광고를 서빙Serving하는 작업을 진행합니다 팀 내 데이터 사이언티스트 크루들이 원활하게 데이터를 분석할 수 있도록 하려면 데이터 처리 프로세스Extraction Transformation Load 이하 ETL가 선행되어야 합니다 제가 속한 광고추천데이터플랫폼파트에서는 대용량 대규모 데이터인 광고 로그 데이터를 원활히 ETL 프로세싱하기 위해 제네시스라는 이름의 카프카 기반 데이터 플랫폼을 운영하고 있습니다이번 글에서는 제네시스 플랫폼에서 활용하고 있는 커스텀 커넥터인 kafkasinkconnector를 개발한 배경과 현재 광고추천팀에서 사용하고 있는 방법 그리고 활용도에 대해서 이야기해 보려 합니다 kafkasinkconnector는 2023년 1월에 깃허브에 오픈소스로 공개되었고 개인 및 회사에서 자유롭게 사용할 수 있습니다 이 글 마지막에는 kafkasinkconnector의 사용 방법도 간략히 설명합니다 kafkasinkconnector Github repository httpsgithubcomkakaokafkasinkconnector 광고 스트림 데이터의 지면별 처리광고에서는 지면placement 또는 at이라고 불리는 단위가 있습니다 지면은 웹사이트 또는 모바일 애플리케이션의 특정 영역을 뜻하며 지면에서 광고를 게재합니다 대부분의 지면에는 개인화 추천 과정을 거쳐 pCTRpredicted clickthrough rate 예상 클릭률이 높은 추천 점수를 받은 광고를 노출합니다 광고추천팀에서 사용하는 광고 데이터는 지면이라고 불리는 개별 광고의 노출impression 클릭click 전환conversion 데이터를 ETL 과정을 거친 후 모델 학습 수행에 사용하고 있습니다 문제는 데이터가 지면 단위로 분리되어 제공되지 않는다는 점입니다 광고가 게시될 수 있는 인터넷 세상에는 엄청나게 방대한 양의 지면이 존재하며 지면들은 시시각각 줄어들고 늘어나기를 반복하고 있습니다 그렇기 때문에 원천 광고 스트림 데이터라고 불리는 거대한 규모의 스트림 데이터를 카프카 토픽을 통해 제공되고 있으며 해당 데이터를 실시간 모델 학습용으로 사용하고 있습니다 카프카는 대규모 이벤트 데이터를 실시간으로 처리하기 적합한 플랫폼이기 때문에 실시간 광고 데이터 처리에 적극적으로 사용하고 있습니다이런 과정을 거쳐 제공하는 원천 광고 스트림 데이터를 가공하지 않고 그대로 실시간 모델 학습용으로 사용할 수도 있겠지만 다음과 같은 두 가지 요구사항을 모두 만족하려면 필터링과 분기 같은 작업이 필요합니다지면별 모델과 입찰 전략을 다르게 가져감모델별 성능을 최적화 하기 위해 학습에 꼭 필요한 데이터만 존재해야 함 위 두 가지 요구사항을 만족시키기 위해서는 지면별로 학습에 필요한 스트림 데이터가 별도로 필요합니다 즉 원천 광고 스트림 데이터를 소스source로 하고 필요한 데이터만 추출sink 하는 스트림 데이터 파이프라인 이 필요하다는 것이죠 이미 눈치채신 분도 계시겠지만 이런 지면별 스트림에서 데이터를 추출할 때 데이터의 분기와 필터링의 생성 수정 및 삭제는 빈번히 일어납니다 이런 상황에서 스트림 데이터 파이프라인의 효율적인 운영과 플랫폼화를 위해 카프카 커넥트를 기반으로 하는 제네시스 플랫폼과 kafkasinkconnector를 만들었습니다제네시스 플랫폼제네시스는 카프카 커넥트 기반 데이터 플랫폼으로서 기존 로그스태시 기반 파이프라인을 개선하기 위해 신규 개발한 플랫폼입니다 특히 파이프라인의 오너십 모니터링 배포 데이터 리니지lineage 계보를 달성하기 위해 만들어졌습니다 자세한 내용은 제네시스 광고추천팀의 카프카 기반 스트리밍 데이터 플랫폼링크를 참고해 주세요kafkasinkconnector 소개제네시스의 핵심 도구인 카프카 커넥트는 카프카와 외부 시스템데이터베이스 등과 연동 목적으로 사용하는 도구로 그 사용 범위를 한정지어 설명하곤 합니다 하지만 데이터베이스와 연동이 아닌 다른 방식으로도 사용할 수 있지 않을까 하고 고민하기 시작했습니다 많은 기능들 중에서도 카프카 커넥트가 가지고 있는 확장성 재사용성 편리한 운영과 관리 및 고가용성 특징을 고려하면 카프카 카넥트는 스트림 데이터 파이프라인 그 자체로서도 완벽하다고 생각했습니다이런 생각 끝에 우리는 로그스태시Logstash의 필터링 기능을 대체할 카프카 커스텀 커넥터Kafka Custom Connector를 만들기로 결심했습니다 그 결과물이 바로 kafkasinkconnector입니다 kafkasinkconnector는 특정 카프카 토픽을 소스로 데이터를 가져와서 필터링 샘플링 타임스탬프 주입 메시지 키 주입 등의 기능을 수행할 수 있는 싱크 커넥터입니다커스텀 커넥터란 오픈소스 아파치 카프카에서 공식적으로 제공하는 인터페이스를 사용하여 개발자가 만든 싱크 커넥터 또는 소스 커넥터 플러그인 JAR 파일을 생성하고 카프카 커넥터에 생성한 JAR 파일을 포함시켜 실행하면 커넥터를 반복해서 생성되는 파이프라인 도구로 사용할 수 있음흔히 주변에서 많이 사용하는 오픈소스 커넥터들과 다르게 커스텀 커넥터는 파이프라인을 직접 정의하고 코드를 작성하여 개발해야 합니다 그렇기 때문에 커스텀 커넥터의 내부 동작을 제대로 이해하고 개발하는 것이 중요합니다커스텀 커넥터를 개발하는 방법오픈소스 아파치 카프카에서 공식적으로 제공하는 SinkTask SinkConnector 인터페이스를 사용하면 라이센스를 걱정할 필요 없이 커스텀 커넥터를 개발할 수 있습니다 커스텀 커넥터를 개발하는 방법에 대해 kafkasinkconnector의 소스코드를 보며 짧게 설명드리겠습니다 먼저 buildgradle에 connectapi를 추가합니다 connectapi 라이브러리에는 커스텀 커넥터를 개발하기 위한 인터페이스들이 존재합니다 다음으로 커넥터에서 사용할 컨피그를 설정합니다 connectapi 라이브러리는 AbstractConfig 클래스를 제공합니다 이 클래스를 상속받는 컨피그 클래스를 다음 예시 코드처럼 설정합니다 컨피그를 설정할 때 필요한 정보로는 컨피그 이름 컨피그 타입 컨피그 기본 값 컨피그 중요도 그리고 컨피그 설명이 있습니다 컨피그가 중요한 이유는 커넥터를 파이프라인 용도로 반복적으로 생성할 때 컨피그를 사용하기 때문입니다 그렇기 때문에 요구사항 중에서 동적 해결할 수 있는 요구사항이 있다면 최대한 컨피그에 선언하여 생성합니다 다음으로 작성할 것은 SinkConnector 와 SinkTask 클래스를 상속받는 커스텀 커넥터 클래스입니다 SinkConnector는 SinkTask를 실행시키고 컨피그를 분배하기 위한 도구로 사용하며 실질적인 데이터 처리와는 무관합니다 그렇기 때문에 데이터를 처리하는 로직은 SinkTask에 위치한다고 볼 수 있습니다 SinkTask 클래스를 상속할 경우 다음과 같은 3가지 메소드를 작성해야 합니다 각 메소드에 대한 설명은 주석을 참고해 주시기 바랍니다 SinkTask 클래스를 상속받은 KafkaSinkTask 클래스의 내부 로직 중 put 내부 구문은 다음과 같습니다 SinkRecord에서 필요한 데이터를 추출 필터링 샘플링하여 sendRecord를 통해 또 다른 클래스로 전달하는 것을 알 수 있습니다 위 코드를 보면 커스텀 커넥터라는 단어에 겁먹을 필요가 없다는 걸 알 수 있습니다 여러분들이 이제껏 많이 개발했거나 보았던 카프카 컨슈머의 코드와 비교해 보세요 매우 비슷한 부분도 있고 더 쉬워 보이는 부분도 있지 않나요 위 코드를 보면 컨슈머의 poll로 리턴되는 ConsumerRecords 대신 Collection로 가져와서 로직을 수행하는 것을 볼 수 있습니다 다른 부분은 컨슈머는 그 자체로서 애플리케이션 실행 단위가 되지만 커넥터는 스레드 단위로 커넥트 클러스터 위에서 태스크로 실행된다는 점입니다 kafkasinkconnector 설명 kafkasinkconnector는 다음과 같은 기능을 가지고 있습니다 레코드를 다른 카프카 클러스터의 특정 토픽으로 전달 JSON 기반 데이터 필터링 샘플링 타임스탬프 파싱 및 주입 메시지 키 파싱 및 주입 고성능 프로듀서 옵션lingerms batchsize 위 기능들은 언제 어떻게 사용 가능한지 사례별로 응용 방법을 소개하겠습니다 kafkasinkconnector 응용 방법 1 필터링과 분기 데이터 필터링 분기는 앞서 설명드린 광고추천팀 사례처럼 거대한 원천 데이터에서 필요한 데이터만 추출할 때 유용합니다 kafkasinkconnector는 jsonPath를 사용하여 JSON 기반 필터링을 수행합니다 예를 들어 다음과 같은 데이터들이 원천 토픽레코드의 메시지 값에 존재한다고 가정해 봅시다 위 데이터 중 색이 빨간 과일을 특정 토픽으로 전달해야 할 때는 다음과 같은 컨피그로 커넥터를 생성할 수 있습니다 이후에 fruits 토픽에 colorred 인 데이터가 들어간다면 모두 redcolorfruit 토픽으로 전달되겠죠 싱크 되는 데이터의 토픽은 kafkasinktopic으로 설정하고 해당 토픽이 위치하는 카프카 클러스터는 kafkasinkbootstrap으로 설정합니다 만약 데이터양이 많아져서 병렬 처리 성능을 높이고 싶다면 tasksmax와 해당 토픽여기서는 fruits의 파티션 개수를 늘리면 됩니다 다음은 광고추천팀에서 원천 데이터를 필터링 분기하려고 kafkasinkconnector를 사용하여 생성한 파이프라인을 계보로 시각화한 스크린샷입니다 보안 사유로 인해 토픽 이름을 가린 점은 양해 부탁드립니다 2 코파티셔닝코파티셔닝coparititoning은 카프카 스트림즈를 활용할 때 가장 많이 접하게 되는 용어입니다 스트림스트림 조인join 스트림테이블 조인과 같이 KStream KTable을 활용하여 스트림 데이터를 처리할 때 코파티셔닝이 되어 있는 두 개의 토픽이 필요합니다 코파티셔닝이란 서로 다른 두 개의 토픽이 파티션 개수가 같고 동일한 파티셔닝 전략partition strategy을 사용하면서 조인이 되고자하는 데이터가 메시지 키에 주입된 상태를 뜻합니다 이렇게 두 개의 토픽이 코파티셔닝된 상태여야만 카프카 스트림즈 내부의 태스크에서 조인을 수행할 수 있습니다 그래서 코파티셔닝은 조인을 하기 전 반드시 선행되어야만 합니다문제는 우리가 사용하는 모든 토픽들이 코파티셔닝된 상태라고 보장할 수 없다는 점입니다 우리가 직접 프로듀스하고 있는 토픽을 기반으로 데이터를 처리한다면 이 문제는 생각보다 쉽게 해결될 수 있습니다 파티션 개수를 맞추고 파티셔닝 전략을 맞추면 대부분의 일이 끝나기 때문이죠 문제는 다른 팀 다른 조직에서 제공하는 외부 토픽을 사용하는 경우입니다 이러한 경우 코파티셔닝을 맞추기가 불가능 한 경우가 종종 존재합니다 또한 스트림 데이터 처리를 위해 레코드에 이벤트 시간을 기준으로 타임스탬프를 주입하는 게 필요할지도 모르겠군요이런 문제를 해결하려면 kafkasinkconnector의 타임스탬프메시지 키 파싱 및 주입 기능을 사용하면 됩니다 예를 들어 다음과 같은 데이터가 원천으로 들어온다고 가정해 봅시다 여기서 레코드의 타임스탬프를 이벤트 발생 시간으로 맞추고 조인이 되어야 하는 유저의 키를 메시지 키로 주입하려면 다음과 같이 설정하면 됩니다 이 스트림 데이터 파이프라인을 통해 추출된 useractionbuycopartition 토픽은 스트림 데이터 처리를 위한 모든 준비가 완료되었습니다 이제 카프카 스트림즈를 사용하여 스트림 프로세싱을 진행하면 됩니다 3 미러링과 샘플링 데이터 미러링을 하려면 최신 카프카 커넥트가 제공하는 미러메이커2MirrorMaker2를 사용해도 됩니다 그러나 미러메이커2를 사용하는 방법은 과하다고 느끼는 분도 있을 겁니다 미러메이커2는 재해복구Disaster recovery에 좀 더 적합하고 관련 기능을 많이 내재하고 있기 때문입니다 그래서 컨슈머 그룹의 오프셋과 토픽의 오프셋 타임스탬프까지 완벽하게 동기화sync 할 필요 없이 단순히 데이터를 다른 카프카에 전달만 해야 할 경우에는 kafkasinkconnector가 오히려 적합할 수 있습니다 예를 들어 상용 환경에 있는 useraction 이벤트 데이터 중 일부를 분석하기 위해 50 샘플링하여 분석 전용 카프카 클러스터에 가져온다고 가정해 봅시다 그러한 경우에는 다음과 같이 커넥터를 설정하여 만들 수 있습니다 맺음말 이상으로 kafkasinkconnector에 대해 알아보았습니다 더불어 커넥터를 활용할 수 있는 방안도 컨피그와 함께 자세히 살펴보았습니다 오픈소스로 공개된 kafkasinkconnector는 카프카 커넥트를 이미 사용하고 있는 조직은 파이프라인을 손쉽게 확장할 수 있도록 커넥트를 도입하지 않은 조직은 커넥트를 쉽게 도입할 수 있도록 도와줍니다 이 글을 읽으면서 kafkasinkconnector가 할 수 있는 기능이 요구사항을 해결하는데 적합하다는 생각이 들었다면 한번 사용해 보시는 건 어떠신가요 아무쪼록 kafkasinkconnector가 카프카와 함께 확장성이 높은 비상태Stateless 기반 스트림 데이터 파이프라인을 만들고 운영하는 데 큰 도움이 되었으면 좋겠습니다 감사합니다 이 자리를 빌려 오픈소스를 발행하고 오픈하는데 큰 도움을 준 광고추천팀 광고추천데이터플랫폼파트 ethan1752박경환께 감사의 인사를 드립니다 Connect with Kakao Tech,https://tech.kakao.com/storage/2022/02/kakao-tech-logo-1-e1649831117387.png,https://tech.kakao.com/2023/01/12/introduce-kafka-sink-connector/
Spring Cloud 와 Spring Boot 로 AWS SecretsManager 이용하기,Spring Cloud 에서 AWS SecretsManager 를 사용한 설정하기 AWS SecretsManager는 소스 상에 노출된 민감한 정보 데이터베이스 사용자 이름 암호 엔드포인트 및 암호화 키 정보 등을 저장하는 용도로 제공된 클라우드 리소스이다 이러한 민감 정보는 소스 상의 설정파일에 노출하면 바로 보안 취약이 발생하며 한번 소스관리툴에 공개되어 올라가면 정보를 즉각 변경하지 않으면 치명적인 위협으로 다가올 수 있어 주의가 필요하다 이 SecretsManager 에 저장된 민감 정보들은 다양한 방법으로 어플리케이션에서 사용할 수 있다 AWS Cli를 이용하여 CICD 에서 직접 환경 변수를 교체할 수 있다 Spring Cloud 에서 제공하는 configuration 서버처럼 SecretsManager 를 이용할 수 있다 단 값의 변경이 실시간으로 반영이 되지 않는다 우리는 여기서 Configuration Server로서 SecretsManager를 사용하는 방법에 대해서 알아볼 것이다 SecretsManager 에 시크릿 생성하기 우선 민감 정보를 SecretsManager 에 저장하기 위해서는 다음과 같은 과정으로 SecretsManager를 생성하자 다음과 같이 새 보안 암호 저장 버튼을 클릭한다 저장할 민감정보를 아래와 같이 지정한다 키값 형식으로 지정한다 시크릿 이름을 myprojectschooldevopsdev 으로 생성한다 AWS Credential 저장하기 AWS Console에서 IAM 사용자 보안자격증명 액세스 키 만들기 버튼을 통해서 credential 을 생성한다 aws configure AWS Access Key ID TVYO AWS Secret Access Key MX0 Default region name apnortheast2 Default output format json credential 을 정상으로 지정했다면 이제 어플리케이션에서 AWS에 직접 접근할 수 있게 된다 참고로 해당 사용자에 대해서 SecretsManager 에 대한 권한을 SecretsManagerReadWrite 권한이 부여되어 있어야한다 다양한 접근 권한에 대해서는 AWS Credential 이외에도 AssumeRole 특정 EC2 인스턴스에 정책 부여를 통해서 SecretsManager에 접근을 할 수 있으니 AWS 메뉴얼을 참조하자 SpringBoot 프로젝트 생성하기 with Spring Cloud 위와 같이 startspringio 에 접속하여 새로운 프로젝트를 생성한다 생성된 이후 꼭 spring boot와 spring cloud의 버젼을 매핑해야한다 주의 Spring Cloud 와 Spring Boot 호환버젼 체크하기 from httpsspringioprojectsspringcloud Compatibility Matrix Spring Cloud Spring Boot 202103 aka Jubilee 26x 27x 20210x aka Jubilee 26x 202003 aka Ilford 24x 25x 20200x aka Ilford 24x Below Spring Cloud versions have all reached end of life status and are no longer supported Spring Cloud Spring Boot HoxtonSR5 22x 23x Hoxton 22x Greenwich 21x Finchley 20x Edgware 15x Dalston 15x CamdenSR5 14x 15x Camden 14x Brixton 13x 14x Angel 12x 위와 같이 호환성을 우선 체크하고 나서 스프링부트 작업을 수행해야한다 버젼이 맞지 않으면 어플리케이션이 정상동작 하지 않는다 의존성 파일 추가하기 우리가 사용할 버젼은 다음과 같다 Spring Boot 256 Spring Cloud 303 pomxml 파일에 다음과 같이 추가하자 dependency groupIdorgspringframeworkcloudgroupId artifactIdspringcloudstarterbootstrapartifactId version303version dependency dependency groupIdorgspringframeworkcloudgroupId artifactIdspringcloudstarterawssecretsmanagerconfigartifactId version226RELEASEversion dependency 시크릿 매니저 설정 추가하기 시크릿 매니저 설정을 위해서 다음 내용을 bootstrapyaml 파일에 추가하자 aws secretsmanager name schooldevopsdev prefix myproject cloud aws region static apnortheast2 우리는 사전에 secretsManqger 이름을 myprojectschooldevopsdev 로 작성했다 prefix 시크릿 매니저에서 생성한 prefix 이름이다 반드시 를 추가해 주는 것을 확인하자 지정하지 않는다면 sectets 가 기본값이다 name name은 첫번째 delimeter 이후의 이름을 작성해준다 cloud 의 region 을 apnortheast2 로 서울 리젼으로 설정했다 시크릿을 생성한 리젼을 작성해 주는 것을 잊지 말자 Test를 위한 Controller 작성하기 TestControllerjava 파일을 생성하고 다음과 같이 코드를 작성하자 package comccsexamplesecretmanager import orgspringframeworkbeansfactoryannotationValue import orgspringframeworkwebbindannotationGetMapping import orgspringframeworkwebbindannotationRestController RestController public class TestController ValueDBURL String dbpassword ValueDBPORT String dbusername GetMappingtest public String getValue return dbpassword dbusername 위 코드에서 핵심 사항은 ValueSecretsManager에 지정한 키값 이다 이렇게 되면 인스턴스가 기동될때 AWS SecretsManager로 부터 해당 시크릿을 검색하고 키에 해당하는 값을 변수에 할당하게 된다 테스트하기 서버가 기동되고 이후에 다음과 같이 커맨드를 실행하면 우리가 원하는 값을 확인할 수 있다 curl localhost8080test 1270013306,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164482&boardType=techBlog&searchData=&page=&subIndex=
"KT, AI 테크랩장에 SKT 출신 윤경아 상무 영입","윤경아 KT 기술혁신부문 AI 테크랩장.ⓒKT
[데일리안 = 남궁경 기자] KT가 기술혁신부문 AI테크랩(AI Tech Lab)장에 윤경아 상무를 영입했다.
KT는 기술혁신부문 AI테크랩장에 윤경아 상무를 임명했다고 3일 밝혔다.
KT는 또 이날 검사 출신 법조계 인사인 추의정 전무를 감사실장으로, 허태원 상무를 컴플라이언스추진실장으로 각각 영입했다고 밝혔다.",https://imgnews.pstatic.net/image/119/2024/01/03/0002786149_001_20240103171701211.jpeg?type=w647,https://n.news.naver.com/mnews/article/119/0002786149?sid=105
이상 탐지 3부-머신 러닝으로 이상 탐지하기,들어가며 안녕하세요 클라우드 AI팀 박현목입니다 지난 글에서는 통계적 기법을 활용한 이상 탐지에 대해 소개해 드렸습니다 1부와 2부는 아래 경로에서 다시 읽어 보실 수 있습니다 1부정상과 비정상 그리고 이상 탐지 2부통계적 기법으로 이상 탐지하기 3부머신 러닝으로 이상 탐지하기 4부딥 러닝으로 이상 탐지하기 이번 글은 위와 같이 4부로 나눈 글 중 3부머신 러닝으로 이상 탐지하기에 대한 이야기입니다 1 Machine learning 사실상 현재 개발되고 있는 최신 이상 탐지 모델들은 대부분 머신 러닝에 기반을 두고 있습니다 뉴럴 네트워크 기반의 딥 러닝 기법들은 4부에서 더 자세하게 설명해 드릴 예정이기에 지금은 고전 머신 러닝 위주로 소개 드리겠습니다 대표적인 머신 러닝 기반 이상 탐지에는 분류classification nearestneighbor clustering 등이 있습니다 그리고 추가로 reconstructionbased라는 기법이 있는데 현재 가장 대중적으로 쓰이는 방법 중 하나입니다 11 Classification 분류classification를 간략하게 한 문장으로 요약하면 경계를 긋는 일이라고 할 수 있습니다 이상 탐지는 정상이상 데이터를 구분하는 경계를 찾는 이진 분류 문제라고 볼 수 있습니다 다만 이상 탐지에서 분류를 적용하기에는 큰 제약 사항이 하나 있습니다 지난 글에서도 말했지만 이상 탐지는 데이터의 불균형이 매우 심한 분야입니다 그런데 분류는 데이터가 균등할 때 잘 동작하는 알고리즘입니다 그런 이유로 데이터 불균형을 해결하기 위해 하나의 클래스만 학습시키는 방법들이 연구되었지만 그럼에도 분류는 이상 탐지에서 활발하게 쓰이지는 않습니다 대표적인 분류 이상 탐지 모델로는 support vector machine이하 SVM이 있습니다 SVM의 과정은 간략하게 설명한다면 데이터 공간에 클래스별로 라벨링된 데이터가 있습니다 여기서는 간단하게 2차원 공간과 11 라벨링만 고려하겠습니다 우리는 클래스들을 가장 잘 나눌 수 있는 경계면hyperplane을 찾는 것이 목적입니다 이때 고려해야 할 사항은 얼마나 클래스를 잘 나누었는가 그리고 경계면과 데이터들이 충분히 떨어져 있는가margin입니다 아래 오른쪽의 그림을 보시면 SVM에 대해 쉽게 이해하실 수 있습니다 초록색 실선으로 표현된 hyperplane이 파란색 원 빨간색 사각형을 나눌 수 있는 경계면이 되어 주고 있으며 이때 색칠된 데이터들과 hyperplane 사이의 거리를 margin이라고 부릅니다 그리고 색칠된 데이터들을 support vector라고 부르며 hyperplane 형성에 가장 큰 영향을 주는 데이터들입니다 그럼 이상 탐지는 어떻게 SVM으로 novelty detection을 하기 위해 나온 것이 one class support vector machine이하 OCSVM입니다 OCSVM의 기본적인 동작 원리는 SVM과 동일합니다 하지만 이름에서 알 수 있듯이 OCSVM은 하나의 클래스만을 학습합니다 대신에 hyperplane을 형성하는 기준에 원점이 추가되었습니다 학습 데이터와 원점을 가장 잘 나눠 주는 hyperplane을 찾는 것이 OCSVM의 목적입니다 비슷한 모델로 support vector data description이하 SVDD이라고 하는 분류 모델도 있습니다 SVDD는 OCSVM과 성격이 매우 유사한데 이번에는 hyperplane이 아닌 hypersphere를 찾는 게 목적입니다 아래의 그림을 보시면 OCSVM SVDD의 동작 과정을 쉽게 알 수 있습니다 주의 사항으로 이상 탐지에서는 두 방법 모두 정상 데이터만을 학습해야 합니다 둘 중에 무엇이 더 좋다고 단언하기는 어렵고 데이터를 바라보는 관점이 다르다 정도로 이해하시면 될 것 같습니다 위의 3가지 기법들은 모두 kernel trick이라는 데이터 확장 기법과 함께 사용하는 일이 많습니다 Kernel trick은 데이터의 차원을 확장하여 비선형 분류 문제를 선형 문제로 바꿔 주는 기법입니다 아래의 그림처럼 2차원 공간에서는 경계면을 찾기 어려운 문제를 kernel trick을 통해 3차원으로 데이터를 확장하면 쉽게 경계면을 찾을 수 있습니다 단점 분류 이상 탐지 모델들은 준수한 성능을 보여 주었고 특히 SVDD와 딥 러닝을 결합한 DeepSVDD는 2018년에 발표되어 현재까지도 많은 연구의 베이스라인으로 활용되고 있습니다 하지만 분류 모델은 아래와 같은 단점 때문에 실제 활용이 어렵다는 의견이 많습니다 라벨링된 데이터를 필요로 한다 비선형 분류 문제를 다루지 못한다 OCSVM SVDD는 라벨링 데이터를 요구하지 않지만 좋은 이상탐지 모델을 만들기 위해서는 모두 정상 데이터만 학습해야 한다는 추가 제약 사항이 있습니다 두 번째는 비선형 분류 문제를 다루기 어렵다는 것입니다 Kernel trick을 쓴다면 선형 문제로 변환할 수는 있지만 언제나 만능은 아닙니다 12 NearestNeighbor NearestNeighbor 방법은 하나의 가정에서 출발합니다 정상적인 데이터들이라면 데이터 차원상에서 서로 밀집되어 있을 것이라는 가정입니다 쉽게 말한다면 끼리끼리 논다 정도로 표현할 수 있을 것 같습니다 데이터의 분포에 대한 어떠한 가정도 하지 않으며 순수하게 데이터 자체만을 보기 때문에 모델의 효율성에 따라 이상 탐지 성능이 크게 달라질 수 있습니다 이웃 데이터들의 거리 정보distance와 밀도 정보density를 활용하여 데이터마다 점수를 부여하고 임곗값을 넘으면 이상 데이터로 간주하는 방식입니다 Distancebased 이웃한 데이터들과의 거리정보를 고려하여 점수 부여 ex KNearest NeighborKNN Densitybased 이웃한 데이터들의 밀도를 고려하여 점수 부여 ex Local Outlier Factor 거리 기반 이상 탐지의 대표적인 예시인 KNN은 인접한 K개의 데이터까지의 거리의 평균 합 최댓값 최솟값 등을 고려해서 데이터에 점수를 부여합니다 이 점수가 임곗값을 넘으면 이상 데이터로 간주합니다 굉장히 직관적이면서도 간단한 원리로 동작하는 방법입니다 하지만 거리 기반 이상 탐지에는 한 가지 단점이 존재합니다 아래의 오른쪽 그림을 보시면 쉽게 이해하실 수 있습니다 빨간색 원으로 표시된 O1 O2 데이터는 둘 다 outlier입니다 사람의 눈으로 관찰한다면 두 데이터 모두 outlier라는 걸 쉽게 알 수 있지만 거리 기반 이상 탐지를 한다면 모델은 O2 데이터를 정상으로 판단할 가능성이 높습니다 왜냐하면 C1으로 표현된 군집으로 인해서 O2와 C2 군집까지의 거리가 충분히 가깝다고 판단하기 때문입니다 그래서 제안된 방법이 인접한 데이터들의 밀도까지 고려하는 Local Outlier FactorLOF 입니다 LOF를 자세히 다루면 글을 따로 하나 더 작성해야 할 정도로 내용이 많기 때문에 밀도를 고려한다는 것 정도만 알고 계시면 좋을 것 같습니다 13 Clustering Clustering 방법은 NearestNeighbor와 유사한 느낌이 있지만 최종 목표가 군집을 찾는 것이라는 차이점이 있습니다 그리고 이상 데이터라면 어느 군집에도 속하지 않을 것이라고 가정합니다 대표적인 예시로 KMeans가 있습니다 NearestNeighbor와 동일하게 라벨링된 데이터를 필요로 하지 않는다는 장점이 있지만 군집의 개수를 미리 지정해야 한다는 점 알고리즘의 효율성에 따라 성능이 크게 차이가 난다는 점 등의 단점이 있습니다 Clustering 방법 자체가 이상 탐지에 적절하지 않다는 의견도 있습니다 왜냐하면 대부분의 군집화 알고리즘들은 모든 데이터가 어느 군집이든 무조건 포함되도록 하는 경우가 많은데 이것이 이상 탐지의 취지와 맞지 않다는 것입니다 14 Reconstructionbased Reconstructionbased 방법은 현재 이상 탐지의 대세 중 하나입니다 Reconstructionbased 방법은 주어진 데이터에 숨어 있는 유효한 특성feature을 추출한 후에 다시 데이터를 복원했을 때 정상 데이터라면 본래대로 복원되지만 비정상 데이터라면 제대로 복원되지 않을 것이라고 가정합니다 대표적으로 방법으로 principal component analysis이하 PCA가 있습니다 PCA는 본래 고차원 데이터를 다루기 위한 차원 축소법으로 주어진 데이터를 분산이 가장 커지는 축으로 환원하는 기법입니다 이때 데이터를 축소하는 변환 정보를 기억해 둔다면 본래의 데이터를 어느 정도 다시 복원할 수 있습니다 이 개념을 응용해서 비정상 데이터라면 제대로 복원되지 않을 것이라는 게 이론적 배경입니다 주의 사항으로는 데이터를 축소할 때 정상 데이터만을 사용해야 한다는 점입니다 하지만 PCA는 선형 변환이라는 한계점이 존재했고 다차원 데이터 간의 연관성을 다루기에는 부족함이 있었습니다 그래서 사용된 것이 바로 뉴럴 네트워크 기반의 오토인코더입니다 오토인코더도 데이터를 압축한 후에 복원한다는 점에서 PCA와 동일하지만 비선형 변환이 가능하다는 차별점이 있습니다 오토인코더는 지금까지도 최신 모델에 많이 적용되고 있을 정도로 성능이 입증된 모델이기 때문에 기억해 두시는 걸 추천합니다 2 고전 머신 러닝 vs 딥 러닝 21 왜 딥 러닝을 써야 할까 위에서 소개한 모델들은 일부를 제외하고 대부분 고전 머신 러닝 모델들입니다 고전 모델들도 좋은 성능을 보여 주었지만 현재는 딥 러닝 기반 모델들이 많이 사용되고 있습니다 다양한 이유가 있지만 근본적인 이유는 데이터의 양이 점차 많아지고 있기 때문입니다 데이터의 양도 문제지만 데이터의 차원이 높아지고 차원 간의 연관성도 높아지면서 고전 머신 러닝 모델들은 충분한 성능을 보여 주지 못하는 상황입니다 하지만 고전적인 접근법이 전혀 사용되지 않는 것은 아닙니다 고전 머신 러닝의 개념과 딥 러닝을 결합한 모델들이 많이 연구되고 있으며 그 중에서는 뛰어난 성능을 보여주는 모델도 많습니다 22 시계열 이상 탐지에서의 딥 러닝 시계열 이상 탐지도 위에서 소개해 드린 방법들을 적용할 수 있습니다 하지만 아쉽게도 성능이 그다지 좋지 않습니다 시계열 데이터에 이상 탐지를 적용하려면 추가적인 고려 사항이 있습니다 데이터 차원 간의 연관성 데이터 시간 축에서의 연관성 첫 번째 고려 사항은 사실 모든 이상 탐지 데이터에서 고려해야 할 사항입니다 하지만 두 번째 고려 사항은 확실히 시계열 데이터에서 나타나는 특성입니다 위에서 소개해 드린 기법들은 별도의 연산을 추가하지 않는다면 시계열 축에서의 연관성을 찾아내지 못합니다 딥 러닝에서는 시간 축의 연관성을 찾아내기 위해서 TemporalCNN RNN Transformer 등 시퀀스 데이터에 특화된 모델을 많이 사용합니다 현재 시계열 이상 탐지는 데이터의 불균형 라벨링 데이터 부족 등의 문제로 정상 데이터만을 학습시키는 비지도 학습법이 대세를 이룹니다 그 안에서도 다양한 범주가 존재하지만 가장 큰 틀에서 모델들은 나눈다면 prediction reconstructionbased로 나눌 수 있습니다 각 방법론에 대한 자세한 설명과 예시는 다음 글에서 소개하도록 하겠습니다 Predictionbased 주어진 시계열을 분석하여 다음에 올 값을 예측하고 실제 값과의 차이를 시계열 이상 탐지에 사용하는 방법입니다 Reconstructionbased 예측과 달리 주어진 시계열 그 자체를 그대로 복원하여 그 차이를 시계열 이상 탐지에 사용하는 방법입니다 둘 중에서 어느 것이 더 성능이 뛰어나다고 말하기는 어렵지만 predictionbased는 예측을 시작한 시점에서 시간이 지날수록 오차가 점차 누적될 가능성이 있습니다 그런 이유로 일반적으로 reconstructionbased 방법이 더 안정적인 성능을 보여 줍니다 나가며 2 3부에 걸쳐 전통적인 이상 탐지 기법들에 대해 소개해 드렸습니다 다음 글은 시리즈의 마지막 이야기인 4부딥 러닝으로 이상 탐지하기로 최신 딥 러닝 이상 탐지 모델들과 제가 직접 경험했던 시계열 데이터를 다룰때 주의해야 할 점들에 대해 소개해 드리겠습니다 이미지 출처 및 참고 자료 Sherenaz W AlHaj Baddar 외 Anomaly Detection in Computer Networks A StateoftheArt Review ResearchGate December 2014 Pier Paolo Ippolito Support Vector Machines June 2019 Grace Zhang What is the kernel trick Why is it important November 2018 Markus M Breunig 외 LOF Identifying DensityBased Local Outliers Erik Bernhardsson Nearest neighbor methods and vector models part 1 September 2015 javaTpoint KMeans Clustering Algorithm javatpointcom Victor Dibia 외 Deep Learning for Anomaly Detection January 2020 Safaa Laqtib 외 A deep learning methods for intrusion detection systems based machine learning in MANET ResearchGate October 2019,https://i.ibb.co/v4t0vVt/2024-01-04-173902.png,https://meetup.nhncloud.com/posts/368
병렬 처리가 가능한 Python 라이브러리 Dask,Python으로 데이터 분석 및 개발을 하는 사람들에게 Dask를 소개하고자 합니다 대규모 데이터 처리 업무를 하시는 분들이라면 Spark을 기본적으로 사용하고 계실 텐데요 Dask는 Spark 정확히는 PySpark 과 비슷한 형태와 서비스를 가지고 있습니다 Python에 친숙한 분들 PySpark을 사용해 보신 분들이 쉽게 이해되고 사용하는데 어려움이 없으실 것 같습니다 Dask 소개 Dask is a flexible library for parallel computing in Python 대규모 데이터 처리를 위해서 분산컴퓨팅으로 사용할 수 있는 라이브러리를 제공한다 Pandas Numpy 데이터 처리가 병렬처리 환경에서 동작하는 것을 지원한다 Delayed 함수를 제공해서 병렬처리를 지원한다 최적화된 Dynamic task scheduling을 지원한다 정리하자면 python이 분산 환경에서 동작 할 수 있게 지원한다 가 될 것 같습니다 Dask 모듈 소개 Dask Array Numpy 로 구현되어 있어 numpy api를 그대로 사용할 수 있습니다 Dask DataFrame Pandas의 DataFrame과 같은 구조이고 병렬처리를 지원합니다 import만 변경하면 pandas api를 그대로 사용하실 수 있습니다 Dask Bag Python 객체에 map filter fold groupby 와 같은 operation을 제공합니다 Dask Delayed 멀티 프로세싱 환경에서 graph를 통해 순서를 정의할 수 있고 그 연산을 효율적으로 할 수 있습니다 Dask Clusters Dask를 단일 머신에서도 효율적으로 동작하고 다중 머신으로 묶여 있는 클러스터 환경으로 확장할 수 있습니다 단일 머신에서 Default scheduler daskdistributed 시스템을 이용 할 수 있습니다 분산 컴퓨팅 환경에서는 PBS SLURMLSF SGE 같은 리소스 매니저를 통해 클러스터 구축이 가능합니다 쿠버네티스 환경 위에서도 클러스터 구축이 가능합니다 spark 사용자에게 친숙한 yarn 클러스터에서도 구축이 가능합니다 AWS GCP AZURE 와 같은 클라우드 환경에서도 클러스터 구축이 가능합니다 마치며 ChatGPT로 더 많은 사람들이 AI 기술에 관심을 가지고 Python 언어로 코딩을 시작하실 것 같습니다 더 많은 데이터를 처리할 때 사용 가능한 라이브러리로 Dask를 알고 계시면 규모 있는 데이터 및 서비스를 개발하고 운영하기에 용이할 것 같습니다 아직은 PySpark보다 커뮤니티가 부족해서 한국어 정보는 많이 찾아볼 순 없지만 Dask를 한번 도입해 보고 프로젝트의 데이터 규모와 성격에 따라 비교 테스트를 해보시는 것은 어떨까요 참고자료 httpswwwnvidiacomenusglossarydatasciencedask httpsdocsdaskorgenstable httpsgithubcomdask 4 reasons you should use Dask,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164852&boardType=techBlog&searchData=&page=&subIndex=
“우리 말 쓰는 AI 만들자”... 유럽·中·중동까지 개발 뛰어들었다,"현재 AI 산업은 거대 자본과 우수 인력을 앞세운 미국 소수 빅테크에 집중돼 있다.
이 때문에 미국과 기술 경쟁을 벌이는 중국은 물론 중동, 유럽도 정부가 나서 자국어 AI 개발에 인력과 자본을 투입하고 있다.
중국은 자국 AI 개발에 심혈을 기울이고 있다.
두 나라 모두 AI 주권 확보를 목표로 막대한 돈을 쏟아붓고 있다.
중동·동남아 등에서는 빅테크 AI보다는 비영어권 국가인 한국의 AI 기술을 활용해 자국어 AI를 개발하는 것을 선호하는 경향이 강하다.
AI 업계 한 관계자는 “한국 기업들은 한국어 AI를 개발하고 테스트하는 과정에서 빅테크와는 다르게 다양성을 중시하는 방식의 접근을 하는 경우가 많다”면서 “각 나라 사정에 맞는 AI를 개발하기에 최적의 조건”이라고 했다.
네이버는 대만·태국·일본에서 AI 서비스 시장 진출을 시도하고 있다.
☞인공지능 주권(AI sovereignty)
인공지능 주권(AI sovereignty)은 해외 기업이나 빅테크에 종속되지 않고 국가별로 자체 언어 데이터를 기반으로 AI 모델을 갖춘 것을 의미한다.
AI 주권을 지키기 위해서는 자주적인 AI(sovereign AI)가 필요한데, 이는 자국 데이터를 기반으로, 자국 언어와 문화로 학습하고 자국 규제가 가능한 AI 기술을 의미한다.",https://imgnews.pstatic.net/image/023/2024/01/04/0003808696_001_20240104100423838.jpg?type=w647,https://n.news.naver.com/mnews/article/023/0003808696?sid=105
"[다시 보기] 6월 우아한테크세미나: GenAI의 시대, 치열해진 시장의 미래 전략",itemname subname 6월 우아한테크세미나 다시보기 일정 2023 6 29목 저녁 7시 약 90분간 우아한테크 YouTube 채널 실시간 스트리밍 httpsyoutubecomwoowatech 주요 내용 Generative AI와 GPT4의 등장 Generative AI이하 GenAI와 튜링 테스트 중국어 방 실험 무서운 AI의 발전 속도 놀라운 GenAI 성능 Stable Diffusion GenAI는 어떻게 인간처럼 대답할 수 있을까 모두를 놀라게 한 GPT4 GPT5는 어때요 Generative AI 서비스와의 조우 GenAI의 발전 타임라인 갑자기 등장한 게 아니에요 GenAI의 애플리케이션 시장의 경쟁 어떤 서비스들이 있나 Large Language Model이 중요하긴 한데 정작 중요한 건 기업들의 GenAI에 대한 평가와 전략 콜센터 문서 검색 회의 요약 CoPilot 그것만으로도 쓸만하다 프롬프트 엔지니어링이 중요하다며 교육부터 시작하자 어떤 Foundation과 Prompt를 보유하느냐가 기업의 경쟁력 결국은 보안이 문제 나만의 LLM을 가지고 싶은 욕망 QA 추천 대상 Generative AI에 대해 많이 들어보기는 했으나 어떤 원리로 동작하는 것인지 잘 몰랐던 분 GenAI가 비즈니스 현장에서 기업들에게 개발자들에게 어떻게 사용되고 있는지 궁금한 분 개발자가 아니라 고객 입장에서 비즈니스 관점에서 GenAI에 대한 도입 전략을 고민하는 분 ChatGPT와 Bard를 써봤지만 신기한 신문물일 뿐 프로그래밍의 근본은 바꾸지 못한다고 믿으시는 분 비추천 LLM과 Fine Tuning을 통해 나만의 자비스Jarvis를 만드는 AI 핸즈온을 기대하는 분 강연자 삼성 SDS 조남호 삼성SDS 한 회사에서 23년째 근무 중입니다 물류부터 모바일 MES까지 여러 업종을 경험하고 사내 창업도 했습니다 삼성페이부터 Product Manager로 일하다 지금은 개발자가 일하기 좋은 대기업을 만드는 DevRel을 합니다 우아한형제들 DevRel 공통 관심사를 가진 개발자들이 기술 교류 활동을 통해 함께 성장할 수 있기를 희망합니다 기술블로그 우아한테크세미나 우아한스터디 우아콘에서 자주 만나요 facebookcomwoowahanTech,https://i.ibb.co/NtYzdMY/2024-01-04-174021.png,https://techblog.woowahan.com/12745/
@jocoding,코딩 교육 30년차가 알려주는 진짜 개발자 되는 방법 (ft. 이민석 교수님),https://img.youtube.com/vi/-ae2zvNl978/0.jpg,https://www.youtube.com/watch?v=-ae2zvNl978
@jocoding,AI 커버 사이 숨은 진짜 가수 찾기 (feat. 폴킴) | AI클론싱어 폴킴편,https://img.youtube.com/vi/cKvmcKP8p6s/0.jpg,https://www.youtube.com/watch?v=cKvmcKP8p6s
"KT, AI테크랩장에 SKT 출신 윤경아 상무 영입","윤경아 KT AI테크랩(AI Tech Lab)장 상무
3일 업계에 따르면 KT는 이날 기술혁신부문 AI테크랩(Lab)장에 윤경아 상무를 임명했다.
AI·빅데이터 거버넌스 수립과 미래 핵심기술 개발을 담당하는 기존 AI2XLab와 달리 초거대 AI '믿음'을 사업화에 필요한 AI 응용 기술과 서비스·플랫폼 개발을 수행한다.
검사 출신 허 상무는 김앤장을 거쳐 법무법인 율정과 아인에서 대표변호사로 활동했다.",https://imgnews.pstatic.net/image/030/2024/01/03/0003170467_001_20240103184427474.jpg?type=w647,https://n.news.naver.com/mnews/article/030/0003170467?sid=105
개발자가 알아야할 DNS 동작,개발자가 알아야할 DNS동작에 대해 알아보자 그리고 클라우드 네임서버도 테스트하며 이해해 보자 클라우드 사업자의 네임서버 장애 클라우드 사업자의 가용영역AZ장애의 경우 대형 장애로 이어진다 네임서버와 GSLB의 이해를 통해 해당 장애로 영향 받는 부분을 줄일수 있도록 다양한 시도를 하고 해당 테스트 내용을 작성해보고자 한다 다음은 개인적으로 정리한 내용이라 틀릴 수 있습니다 용어 이해 네임서버 도메인 등록 대행기관 캐시 DNS 도메인 등록 대행 기관에서 도메인 구매하고 네이버 클라우드에서 네임서버 운영하기 도메인 등록 대행 기관의 네임서버 사용하기 도메인 등록 대행기관 네임서버 장애시변경 사이트 접근 가능 네이버 클라우드 네임서버로 이전하기 도메인 등록 대행기관 네임서버 장애시변경 사이트 접근 불가 네이버 클라우드 네임서버로 이전하기 네이버 클라우드 DNS 장애 시 도메인 등록 대행기관으로 네임서버로 이전하기 네이버 클라우드 DNS 장애 시에도 네임서비스 계속 되게 하기 보안 도메인 등록 대행 기관 로그온시 2단계 인증 필수로 사용하기 보안 클라우드 서비스 업체 로그온시 2단계 인증 필수로 사용하기 같이 보면 좋을 자료 1 용어 이해 네임서버 도메인 등록 대행기관 캐시 DNS 네임서버 해당 도메인에 대한 IP정보를 가진 서버 도메인 등록 대행 기관 가비아 아이네임즈 후이즈 등 도메인 등록을 대행해 준다 디폴트로는 네임서비스 역할을 하지는 않는다 실제 서비스 되는 시스템과는 연관이 없다 네임서버 정보만 상위 기관에 전달해준다 네임서버가 없는 경우 도메인 등록 대행 기관의 네임 서버 서비스를 이용할 수도 있다 캐시 DNS 캐시 정보를 가진 DNS ISP 이통사 DNS를 캐시 DNS라고 한다 사용자는 이 캐시 DNS의 정보를 가지고 온다 TTL 타임 투 라이브 살아 있는 시간 캐시 DNS에서 TTL 시간이 지나야 사용자는 정보를 다시 받아 온다 CSP Cloud Service Provider 클라우드 서비스를 제공하는 기업이다 AWS Azure GCP Naver 등이다 기타 DNS관해 더 알고 싶으시면 httpsbrunchcokrtopasvga1387 2 도메인 등록 대행 기관에서 도메인 구매하고 네이버 클라우드에서 네임서버 운영하기 도메인 등록 대행기관에서 도메인 구매 클라우드에서 네임서버 사용할시에는 가비아에서 구매시 네임서버 변경 예 네이버 클라우드 네임서버 사용할시 My가비아 관리 네임서버 설정 1차 ns11nsncloudcom 2차 ns12nsncloudcom 3 도메인 등록 대행 기관의 네임서버 사용하기 네임서버를 사용하는 여러 가지 방법이 존재한다 그중 도메인 등록 대행기관가비아 아이네임즈 등의 네임서버를 사용해 네임서비스를 해보자 도메인 등록 대행기관가비아 아이네임즈 등에 도메인을 등록한다 등록 시 도메인 등록 대행기관가비아 아이네임즈 등의 네임 서버를 이용으로 해서 사용한다 가장 손쉬운 방법이다 예 가비아의 경우 My가비아 도메인 관리 설정 전체 가비아 네임서버 사용 nsgabiacokr ns1gabiacokr nsgabianet 소유자 인증 적용 도메인 정보 변경 DNS관리 설정 레코드 수정 레코드 추가 A 레코더를 등록한다 www 10112 확인 저장 제약 사항 가비아 A레코더는 디폴트 TTL 180030분 최소 6005분으로 조정가능 4 도메인 등록 대행기관 네임서버 장애시변경 사이트 접근 가능 네이버 클라우드 네임서버로 이전하기 이전 시간 가비아에서 다른 네임서버 변경하는 데는 1일이 걸린다 NS TTL이 1일 이기 때문에 Cache DNS에서는 1일 이후 NS정보가 반영된다 작업 순서 도메인 등록 대행기관 사이트 접속하여 A레코드를 확인한다 1주에 한번 주기적으로 A 레코드를 백업받아 놓자 엑셀 다운로드 기능이 있다 My가비아 관리 DNS정보 DNS관리 도메인 클릭 설정 설정된 호스트 확인 네이버 클라우드 로그인 Global DNS에서 DNS설정한다 가비아에서 확인한 A레코드 등록 설정 적용 배포까지 해야 한다 가비아 사이트에서 네임 서버를 네이버 네임서버로 변경한다 My가비아 관리 네임서버 설정 1차 ns11nsncloudcom 2차 ns12nsncloudcom 소유자 인증 적용 가비아 NS TTL은 1일 86400이다 따라서 최대 1일이 지나야 네임서버 변경 된 부분이 반영된다 NS에 대한 TTL 설정하는 부분은 없어 변경은 안된다 AUTHORITY SECTION serverchkcom 76457 IN NS ns1gabiacokr serverchkcom 76457 IN NS nsgabiacokr serverchkcom 76457 IN NS nsgabianet 확인해 보자 dig wwwserverup11com 5 도메인 등록 대행기관 네임서버 장애시변경 사이트 접근 불가 네이버 클라우드 네임서버로 이전하기 이전 불가 네임서버 정보를 변경하지 못하므로 이전이 불가하다 6 네이버 클라우드 DNS 장애 시 도메인 등록 대행기관으로 네임서버로 이전하기 네임서버NS TTL시간으로 인해 바로 이전되지 않는다 디폴트 NS TTL 은 1일24시간이다 빠른 이전을 원한다면 네이버 클라우드에서 NS TTL을 최소 1일 전에 36001시간 정도로 미리 줄여 놓자 roots186 bab4 bb68 dig serverup11com ns DiG 9114P2RedHat911416P2el786 serverup11com ns global options cmd Got answer HEADER opcode QUERY status NOERROR id 16252 flags qr rd ra QUERY 1 ANSWER 2 AUTHORITY 0 ADDITIONAL 3 OPT PSEUDOSECTION EDNS version 0 flags udp 4096 QUESTION SECTION serverup11com IN NS ANSWER SECTION serverup11com 86286 IN NS ns11nsncloudcom serverup11com 86286 IN NS ns12nsncloudcom ADDITIONAL SECTION ns11nsncloudcom 24924 IN A 492361566 ns12nsncloudcom 24924 IN A 492361576 Query time 0 msec SERVER 169254169535316925416953 WHEN Tue Mar 07 180438 KST 2023 MSG SIZE rcvd 125 roots186 bab4 bb68 네이버 클라우드에서 A레코더 확인 Excel 다운로드 기능이 있다 1주에 1번은 백업하자 도메인 등록 대행기관에서 운영할 거라면 A 레코더를 등록한다 My가비아 관리 DNS정보 DNS관리 도메인 클릭 설정 도메인 등록 대행기관에서 네임서버를 아래로 변경 예 가비아로 변경시 nsgabiacokr ns1gabiacokr nsgabianet 디폴트 NS TTL 은 1일24시간이다 확인 최소 1일이 지나야 네이버 클라우드 NS TTL이 갱신된다 while true do dig wwwserverup11com echo sleep 5 done 7 네이버 클라우드 DNS 장애 시에도 네임서비스 계속 되게 하기 CSP네임서버 장애나 Ddos공격으로 장애 시 서비스연속성 보장하는 법 다른 CSP 네임 서비스를 활성화하여 사용하는 법 네이버 클라우드 Global DNS에 설정된 도메인의 NS TTL을 3600초1시간로 줄여 놓는다 TTL을 줄이는 이유는 나중에 NS 추가 시 Cache DNS들에서 NS정보를 캐슁 하므로 갱신이 필요하다 서비스 장비의 IP가 변경되지 않는 경우는 무관하나 IP가 변경되는 경우는 반영해야 하기 때문이다 네이버 클라우드 DNS에서 A레코더 백업받은 파일을 찾는다 B CSP의 DNS서버스를 시작한다 백업받은 파일로 A레코더를 등록한다 평소 네이버 클라우드 DNS변경 요청 시 B CSP DNS서비스에도 레코더를 추가 작업한다 1번 2번은 사전에 작업한다 1일 자동 백업이 되도록 한다 B CSP 네임서버에 자동으로 설정이 되도록 한다 네이버 클라우드 DNS 장애 시 도메인 등록 대행 기관 사이트에 접속하여 B CSP의 네임서버를 4개를 추가한다 맨 마지막 점 은 뺀다 while true do dig wwwserverup11com echo sleep 3 done 참고 AWS NS TTL은 1728002일이다 수정 가능하다 기타 AWS로만 운영하다 AWS DNS장애 시 다른 CSP 네임서버를 추가하는 경우 NS TTL을 줄이자 8 보안 도메인 등록 대행 기관 로그온시 2단계 인증 필수로 사용하기 도메인 등록 대행기관가비아 아이네임즈 등 사이트 로그인 시 2단계 인증 필수로 사용하자 가비아의 경우 My정보 관리 보안 설정 로그인 2단계 인증OTP에서 설정한다 httpsmygabiacommyinfosecurity 타 사이트 계정 탈취 당해 해당 도메인 등록 대행기관 사이트도 해킹 당하는 경우 도메인 사용이 중지되어 모든 서비스가 안 되는 경우가 발생한다 9 보안 클라우드 서비스 업체 로그온시 2단계 인증 필수로 사용하기 CSP인 네이버 클라우드 서비스 사용 시 2단계 인증을 필수로 사용하자 마이페이지 계정관리 보안설정에서 2단계 인증을 설정한다 3가지 방법이 가능하다 휴대전화 번호 추가 이메일 주소 추가 구글 OTP추가 타 사이트 계정 탈취 당해 네이버 클라우드 탈취 당하면 모든 서비스가 중단된다 해당 시스템에 개인정보가 있는 경우 모두 탈취 당해 큰 문제가 될 수 있다 10 같이 보면 좋을 자료 httpsbrunchcokrtopasvga1927 감사합니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=164603&boardType=techBlog&searchData=&page=&subIndex=
도심항공모빌리티를 위한 UX,안녕하세요 에스케이텔레콤의 김정석입니다 회사에서도 그리고 정부에서도 계속 강조하고 있는 것이 2025년에는 전기로 구동하는 수직 이착륙기로 도심을 가로지르는 새로운 교통 수단이 상용화 될것이라는 것입니다 이번 글은 도심항공모빌리티 자체에 대한 내용이라기 보다는 학생들의 시각에서 어떻게 해석되었는지 공유하기 위하여 쓰게 되었습니다 프롤로그 오픈업과 인연으로 경북대학교 학생들의 텀 프로젝트의 멘토 활동을 한지가 거의 5년 정도 되어가는 듯합니다 멘토멘티 프로그램에서는 소프트웨어 개발 프로세스를 학생들이 경험할 수 있도록 하는데 집중을 하였고 완성된 소프트웨어와 학부생 경진대회 그리고 논문 등을 지도했고 좋은 성과들이 내주어서 매우 뿌듯해하고 있었습니다 개인적인 욕심으로는 제가 지도한 학생들이 5년 10년 후 그때 기억은 잘나진 않지만 어떤 멘토링 프로그램에서 이야기했던 아니면 경험했던 그 내용이 아하 이게 그거였구나 라고만 떠올라 주었으면 좋겠습니다 하지만 이러한 멘토링 프로그램은 항상 부족한 것이 시간입니다 한학기 약 3개월의 시간이 지나면 이 후 수행한 프로젝트는 학생들에게는 성적으로 남고 논문을 쓴 학생들이라면 가끔식 구글에 검색되어 나오는 논문들로 추억하는 것이 전부였습니다 그러나 올해는 특별히 소프트웨어 논문 그리고 마지막으로 이글을 쓰는 이유인 학생들의 손으로 재 해석하고 도심항공모빌리티에서 필요한 요소에 대해 작성한 글을 소개하고자 합니다 도심항공모빌리티에서 기체는 정해진 항로를 따라 이동해야만하며 이 항로를 이탈하는 경우는 기존에 운항하는 비행기들이 사용하는 관제 영역으로 침범을 하게 됩니다 그렇기 때문에 사전에 계획한 항로를 따라서 정상적으로 이동을 하는지 여부가 도심항공모빌리티 교통 관제에서 중요한 부분이 됩니다 또한 기존 항공기 이동과 다른점은 운용 고도가 약 300미터에서 600미터 구간에서 비행할 예정이고 지상 교통 수단에 비해서 매우 빠른 이동 속도 300Kmh로 이동하기 때문에 상공에서 내려다본 2차원적인 요소위도 경도뿐아니라 고도의 정보도 매우 민감하게 모니터링해야한다는 것이라고 할 수 있습니다 이번 학기에 참여한 학생들은 도심항공모빌리티의 교통 관제에서 이 부분이 특히 중요하다고 생각하고 실시간 이동체 궤적과 공간에 대한 모니터링 시스템을 개발하고 구현하였습니다 짧은 시간 동안 대부분의 개념을 정립하고 실제 상용화에서 고민하는 문제를 풀어보았다는 것이 가장 큰 의의라고 할 수 있습니다 아래는 학생들이 직접 작성한 글입니다 프로젝트 팀 소개 논문 투고 및 수상 프로젝트를 바탕으로 고도와 경로를 활용한 UAM 모니터링 시스템 개발 논문을 작성했고 해당 논문을 통해 2023년 한국정보기술학회 하계종합학술대회 및 대학생논문경진대회에 참가하여 우수논문상동상을 수상하게 되었습니다 실시간 이동체 궤적과 공간에 대한 모니터링 시스템 연구 및 개발 안녕하세요 저희는 경북대학교 컴퓨터학부 4학년에 재학 중인 학부생들입니다 한 학기 동안 SKT 김정석 매니저님과 함께 실시간 이동체 궤적과 공간에 대한 모니터링이라는 주제로 산학 협력 프로젝트를 진행하였습니다 해당 포스트를 통해 저희 프로젝트에 대해 다뤄보겠습니다 연구 주제 및 배경 소개 저희의 연구 주제는 실시간 이동체 궤적과 공간에 대한 모니터링 시스템 연구 및 개발입니다 좀 더 쉽게 주제를 설명하자면 UAM 모니터링 시스템 연구 및 개발이라고 할 수 있습니다 그렇다면 UAM이란 무엇일까요 UAM은 Urban Air Mobility의 약자인데요 뜻을 직역해 보면 도시 공중 이동체입니다 직역한 그대로 UAM은 도시 공간에서 공중 이동을 가능하게 해줍니다 이를 통해 교통체증 문제를 해결할 수 있고 도시의 공간 이용 효율성을 제공할 수 있습니다 그래서 UAM은 현재 전 세계가 주목하는 차세대 교통수단입니다 하지만 UAM은 기존 항공기의 관제 시스템을 사용할 수 없습니다 항공기는 넓은 운행 범위를 운행하며 항공기들이 서로 낮은 밀집도를 가지고 있습니다 반면에 UAM은 도시 항공을 운행한다는 점에 있어 좁은 운항 범위와 높은 밀집도를 가진다는 특성이 있고 이를 고려한 모니터링 시스템이 필요합니다 저희 프로젝트는 UAM 관제 시스템 즉 모니터링 시스템에 대한 연구를 진행하였습니다 UAM은 운행 경로에 대해 서로 높은 밀집도를 지닌 점에 있어 3차원 공간을 모니터로 관제할 수 있게 도와주는 새로운 모니터링 시스템이 필요하다고 느꼈고 이에 대해 연구를 진행했습니다 연구 진행 과정 저희는 가시성 및 정보 전달력이 모니터링 시스템에서 가장 중요하다고 생각했습니다 그리고 아래 시스템 요구사항에 맞추어 연구 및 설계를 진행했습니다 시스템의 요구사항 모니터링 시스템은 UAM의 경도 위도 고도 정보를 파악할 수 있어야 한다 3D 모니터링 시스템에 대한 부정적인 의견을 멘토님을 통해 듣고 2D인 모니터링 화면에서 3D 공간을 활공하는 UAM의 위치를 쉽게 파악할 방안에 대한 방안을 연구했습니다 모니터링 시스템은 UAM의 실시간 이동 정보를 다루어야 한다 시스템은 실시간으로 전달되는 UAM의 이동 정보들을 다룰 수 있어야 했습니다 그러기 위해서 대량의 데이터를 전달할 수 있는 파이프라인 구축이 필요하다고 판단했습니다 또한 안정성이 제일 중요하다고 판단하였기에 전달되는 데이터 손실을 최소화하여야 했습니다 모니터링 시스템은 웹 애플리케이션으로 구현되어야 한다 프로젝트 기간은 3개월로 한정되어 있었습니다 그래서 시스템은 빠른 프로토타입을 위해 생산성을 중요히 판단했습니다 시연이 간단하며 빠르게 개발을 할 수 있는 웹 어플리케이션을 통해 모니터링 시스템의 목업을 제작하기로 하였습니다 한계 사항 및 극복 프로젝트 진행에 있어 한계점도 존재하였습니다 UAM에 관련된 연구가 활발히 진행되고 있다 하더라도 새로운 연구 주제에 관하여 학부생 수준에서 자료를 얻는 것은 한계가 존재하였습니다 또한 UAM에 관련된 기술들에 대한 표준이 정해져 있지 않았기 때문에 더욱 연구에 차질이 생겼습니다 1 활용 데이터의 부족 새로운 분야에 대한 연구였기 때문에 활용할 데이터들이 없어 새로 정의 및 생성이 필요했습니다 그래서 저희는 항공기의 데이터 표준 포맷인 FIXM과 ADSB를 활용하여 UAMFIXM과 UAMADSB를 정의했습니다 아래는 항공기의 표준 데이터 포맷입니다 Flight flightIdentifier aircraftIdentificationUAL123aircraftIdentification flightIdentifier departure airport airportIATALAXairportIATA airport runway runwayDirection runway25Lrunway runwayDirection runway plannedDepartureTime Date20230215Date Time140000Time TimeReferenceUTCTimeReference plannedDepartureTime departure arrival airport airportIATAJFKairportIATA airport plannedArrivalTime Date20230215Date Time200000Time TimeReferenceUTCTimeReference plannedArrivalTime arrival route waypoint waypointLocation location longitude1184081longitude latitude339416latitude location waypointLocation waypointTime Date20230215Date Time140000Time TimeReferenceUTCTimeReference waypointTime waypoint waypoint waypointLocation location longitude825531longitude latitude350442latitude location waypointLocation waypointTime Date20230215Date Time180000Time TimeReferenceUTCTimeReference waypointTime waypoint waypoint waypointLocation location longitude737781longitude latitude406413latitude location waypointLocation waypointTime Date20230215Date Time200000Time TimeReferenceUTCTimeReference waypointTime waypoint route Flight MSG51150000000FFB8A802023021513452359020230215134523357BAW123170005131026060964245 해당 항공기 포맷에서 몇가지 정보를 추출하여 새롭게 UAM만의 데이터 포맷을 정의하였습니다 다음은 항공기의 포맷을 활용해 새로 만든 저희 시스템의 데이터 포맷입니다 UAMFIXM 주요 포맷 세부 포맷 식별자 UAM 식별자 출발 도착 버티포트 위도 경도 출발 도착 예정시각 위도 경도 flightIdentifier uamIdentification BKDG001 departure vertiport longitude 128606638 latitude 35887983 plannedDepartureTime date 20230515 time 140000 timeReference UTC arrival vertiport longitude 128627656 latitude 35879337 plannedArrivalTime date 20230515 time 140145 timeReference UTC route longitude 1286118925 latitude 358858215 longitude 128617147 latitude 3588366 longitude 1286224015 latitude 358814985 UAM ADSB 주요 포맷 세부 포맷 식별자 UAM 식별자 현재 시각 날짜 시간 시간대 현재 위치 위도 경도 고도 flightIdentifier uamIdentification BKDG001 currentTime date 20250525 time 140000 timeReference UTC currentPosition altitude 200 longitude 128606638 latitude 35887983 현재 UAM이 비행하는 영역은 대구의 경도 위도를 기준으로 진행이 됩니다 또한 고도는 한국무역협회 국제무역통상연구원 Trade Focus 2021년 22호 도심 항공 모빌리티UAM 글로벌 산업 동향과 미래 과제에서 UAM의 예상 비행 고도로 판단하는 600m까지로 설정했습니다 2 시각화 방법 3D공간을 2D 공간에 표현해야 한다는 점도 큰 어려움을 제공했습니다 한 화면에 UAM의 모든 정보를 표시하기 위하여 두 개의 간단한 그래프를 한 화면에 표현하기로 하였습니다 3D공간이 가지는 정보인 위도 고도 경도를 2D화면에서 나타내기 위해 고도 그리고 위도 경도 이렇게 두 가지로 나누어 구현하였습니다 아래는 저희 시스템의 mockup 화면 입니다 3 통신 프로토콜 정의 UAM과 모니터링 시스템 서버간의 안정적이면서 확장과 축소에 용이한 데이터 송수신이 필요했습니다 또한 대용량의 UAM 데이터들을 수신해야한다고 가정하였습니다 저희는 안정적인 데이터 송수신을 유실될 염려가 적은 데이터 송수신으로 정의 하였고 확장과 축소에 용이한 데이터 송수신을 UAM과 모니터링 시스템 서버의 제약없는 통신 참여와 이탈로 정의하였습니다 통신에 필요한 조건들을 만족하는 통신 방법으로 REST API를 생각해보았으나 다음과 같은 어려운 점이 존재했습니다 대용량의 UAM 데이터 처리가 필요합니다 요청응답 기반의 HTTP 프로토콜을 활용하는 REST API를 활용한다면 데이터 처리에 있어 사이드 이펙트가 생길 것이라 판단했습니다 UAM 데이터를 수신하는 측의 IP가 변경될 경우 UAM 데이터를 송신하는 측의 IP를 전부 수정 해야 한다는 단점이 존재합니다 UAM 데이터를 수신하는 측에서 서버의 연결이 끊길 경우 UAM 측에서 송신하는 데이터가 모두 유실된다는 단점이 존재합니다 UAM 데이터를 수신하는 서버가 추가 될 경우 UAM 데이터를 송신하는 측에서 동일한 데이터를 반복적으로 보내야하는 작업이 발생합니다 이러한 단점을 극복하기 위해 저희는 UAM 데이터에 대한 통신 프로토콜로 websocket을 활용하기로 하였고 kafka를 활용해 대규모 데이터를 스트리밍하기로 결정하였습니다 websocket과 kafka를 사용함으로써 다음과 같은 이점을 얻을 수 있습니다 websocket을 사용하면 UAM과 모니터링 시스템 서버간 양방향 통신이 가능해져 데이터 송수신이 탁월하게 이루어질 것입니다 kafka를 사용하면 데이터의 중복이나 유실을 막을 수 있고 송신과 수신 간의 안정적인 데이터 통신을 보장할 것입니다 kafka는 높은 내결함성faulttolerance과 확장성scalability을 제공하기 때문에 UAM과 모니터링 시스템의 확장 및 축소를 용이하게 합니다 Kafka를 활용한다면 UAM 데이터를 수신하는 측의 변경사항이 생겨도 UAM 데이터를 송신하는 측에서는 수정사항 없이 그대로 통신 할 수 있습니다 Kafka를 활용해 추상화 효과를 줄 수 있습니다 UAM 데이터를 송수신하는 서버가 여러 개로 늘어나도 UAM데이터를 수신하는 측과 송신하는 측 모두 추가되는 서버에 대해서 신경 쓸 필요가 없어집니다 결과적으로 websocket과 kafka를 활용해 UAM과 모니터링 시스템 서버간의 안정적이면서 확장과 축소에 용이한 데이터 송수신을 달성할 수 있었습니다 이를 통해 저희가 설정한 조건들을 충족하는 효율적인 통신을 구축할 수 있게 되었습니다 아래는 간단한 구현 코드입니다 모니터링 시스템 서버와 UAM서버간의 통신을 카프카를 통해서 하기에 앞서 먼저 카프카자체의 설정 코드를 설치되어있는 서버의 공인 IP로 변경하였습니다 advertisedlistenersPLAINTEXT서버 IP 주소9092 이후 Spring 서버 내부에서 Kafka로부터 받아오는 데이터를 저희가 정의한 adsbDTO 클래스로 Deserialized하여 처리할 수 있게끔 다음과 같이 설정 코드를 작성하였습니다 Configuration public class adsbDtoConsumerConfig private final String bootstrapServer 카프카가 설치 되어있는 서버의 IP Bean public MapStringObject adsbDtoConsumerConfigs return CommonJsonDeserializergetStringObjectMapbootstrapServer Bean public ConsumerFactoryString adsbDTO adsbDTOConsumerFactory return new DefaultKafkaConsumerFactory adsbDtoConsumerConfigs new StringDeserializer new JsonDeserializeradsbDTOclass false Bean public ConcurrentKafkaListenerContainerFactoryString adsbDTO adsbDTOListener ConcurrentKafkaListenerContainerFactoryString adsbDTO factory new ConcurrentKafkaListenerContainerFactory factorysetConsumerFactoryadsbDTOConsumerFactory factorygetContainerPropertiessetAckModeContainerPropertiesAckModeRECORD return factory Bean public StringJsonMessageConverter jsonConverter return new StringJsonMessageConverter 모든 설정을 완료한 후 KafakaListener로 카프카에 새로운 메시지가 오는지 확인하는 KafkaListener를 작성하였습니다 이때 인자에 Payloadrequired false 를 추가하여 UAM서버 내부의 패키지 구조를 몰라도 데이터를 받아 올수 있게 하였습니다 KafkaListenertopics TOPIC groupId GROUPID public void receiveKafkaMessagePayloadrequired false ConsumerRecordString String record throws IOException servicereceiveKafkaMessagerecordvalue loginfo메시지 수신 성공 소켓 송신 완료 receiveKafkaMessage recordvalue 연구 산출물 다음은 프로젝트 연구를 진행하며 작성한 문서들입니다 모니터링 시스템 모니터링 시스템의 시스템 아키텍처입니다 이를 통해 저희 시스템의 전체적인 흐름을 파악할 수 있습니다 이동체인 UAM이 데이터 서버로 비행 정보를 보내면 데이터 서버에서 이를 카프카를 통해 모니터링 시스템 서버로 전달해 처리하게 되고 관리자는 클라이언트를 통해 UAM의 비행 정보를 파악할 수 있습니다 해당 시스템의 소프트웨어 아키텍처를 위와 같이 설계하게 되었습니다 비행 정보 데이터를 모아 Kafka로 보내기 위한 비행체 서버와 이를 처리하기 위한 데이터 처리 서버로 나누어 두 서버를 운용했습니다 이때 데이터 서버는 대량의 데이터를 안정적으로 처리하기 위해 Kafka 메시징을 이용했습니다 타 메시징 시스템과 달리 Kafka는 보관 주기동 안 디스크에 메시지를 저장하기 때문에 트래픽이 안정적이지 못한 상황에서 컨슈머의 처리가 늦어져도 메시지 손실 없이 안전하게 처리할 수 있는 장점이 있었습니다 위는 최종적으로 구현한 모니터링 시스템의 화면입니다 화면 왼쪽에는 UAM 고도 그래프를 통해 각 UAM의 고도를 오른쪽에는 UAM의 경로 그래프를 통해 각 UAM의 경로를 파악할 수 있습니다 또한 앞서 mockup 화면에서와 같이 고도가 비슷해 충돌 위험이 있는 UAM은 같은 색상으로 표현되어 한눈에 현재 비행 상황을 파악할 수 있게 하였습니다 현재는 대구광역시를 기준으로 UAM 버티포트 위치와 UAM 임시 운행 경로를 활용해 시스템이 작동하지만 추후 입력정보에 따라 시스템이 해당 조건에 맞게 운용될 수 있습니다 구현 방법 해당 시스템을 구현하며 사용했던 기술 중 핵심이 되는 요소들에 대해 설명드리겠습니다 시각화 프로토타입을 빠르고 가볍게 만들기 위하여 아래와 같은 라이브러리를 활용했습니다 npm 생태계에서 많이 활용이 되고 안정성이 있는 라이브러리들을 활용했습니다 axios 서버와의 통신을 위해 활용 d3 자유로운 그래프 커스터마이징을 통해 UAM의 위치 시각화를 기대할 수 있어 활용 react 높은 렌더링 성능 및 방대한 생태계가 존재하여 빠른 개발 및 성능을 기대할 수 있어 활용 recoil 개발자의 익숙함 및 상당한 보일러플레이트 코드가 존재하는 redux보다 빠른 개발을 위해 활용 styledcomponents 간단한 css 커스텀을 위해 활용 그래프 구성 그래프는 d3 라이브러리를 활용해 svg 태그에 벡터를 구성하는 방식으로 구현했습니다 svg refchartRef widthwindowinnerWidth 015 heightwindowinnerHeight ref를 활용해 svg 벡터 값을 조정했습니다 차트의 크기는 사용자의 윈도우에 맞게 설정되도록 window 객체의 속성을 활용했습니다 간단하게 고도 그래프 부분을 예시를 들겠습니다 우선적으로 그래프의 범위를 설정합니다 그래프의 높이는 프로젝트에서 설정한 데이터에 맞게 설정되어 있습니다 코드의 보완 방법으로는 상수로 설정하는 것이 아닌 수신되는 데이터에 맞게 유동적으로 그래프의 높이를 구성할 수 있습니다 const yScale d3 scaleLinear domain0 600 rangewindowinnerHeight 0 const makeCanvas yScale const svg d3selectchartRefcurrent let yAxis svgselectyaxis if yAxisempty yAxis svgappendgattrclass yaxis yAxis calld3axisLeftyScaletickValues100 200 300tickSize0 stylefontsize 16px yAxisattrtransform translate400 y축 생성 이후 UAM들의 고도를 표시를 나타내는 함수입니다 const makeUamAltitudeLine data id yScale const svg d3selectchartRefcurrent SVG 요소 선택 const line d3 선을 그리기 위한 D3 곡선 생성 정의 line xd i i windowinnerWidth datalength 1 yd yScaled const path svgselectlinepath id 데이터 ID에 따른 선 요소 선택 const color red 색상 설정 예시를 위해 빨강으로 임의로 지정 선 요소가 없는 경우 선과 텍스트 요소 생성 if pathempty svg appendpath datumdata attrclass linepath id attrid linepath id attrstrokewidth 5 attrstroke color attrfill none svg appendtext appendtextPath attrhref linepath id UAM 경로 표시 요소와 텍스트의 ID 연결 attrstartOffset 55 attrtextanchor start attrfontsize 1rem attrfill ffffff textid else 선 요소가 있는 경우 데이터와 스타일 업데이트 pathdatumdataattrstroke colorattrd line 이와 같이 간단하게 그래프를 표시할 수 있었고 모니터링 시각화 그래프를 구성했습니다 만약 해당 모니터링 시스템을 고도화한다면 더욱 데이터 확장성을 고려한 그래프를 구성하는 재미가 있을 것으로 예상됩니다 회고 및 결과 좋았던 요소들 UAM이라는 생소한 주제에 대한 프로젝트를 경험해 매우 뜻 깊었습니다 팀원 모두 프로젝트를 진행하며 도메인 지식의 중요성에 대해 깨닫는 경험을 하였고 주어진 환경 속 최적의 결과를 낼 수 있는 공학에 대한 많은 고민을 하며 성장했습니다 맹목적인 기술 도입은 오히려 해가 될 수 있다는 것을 깨달았습니다 프로젝트 초기 맹목적으로 컨테이너 기술을 도입하였지만 시간이 지날수록 컨테이너화로 얻을 수 있는 장점이 뚜렷하게 보이지 않았으며 저희 팀의 시간을 앗아갔습니다 그리하여 저희 팀원들은 컨테이너 기술도입의 문제에 대해 상의하고 불필요한 기술을 덜어내었습니다 이후 저희는 프로젝트에서 필요한 기술 구현에 더 집중할 수 있었습니다 아쉬웠던 요소들 kafka의 동작 방식에 대해서 제대로 공부하지 못한채 바로 구현을 한 점이 아쉽게 느껴졌습니다 길지 않은 프로젝트 기간에 빠른 결과를 내야 했기에 심도있는 공부를 하지 못한 점이 아쉬웠습니다 프로젝트 상황에 맞는 신 기술 도입에 대해 생각해보는 계기가 되었습니다 두번째로는 실제 UAM에서 송출하는 데이터를 사용하지 못한 점이 아쉬웠습니다 실제 운항 환경과 조건을 완벽히 고려하지 못한 점이 아쉬웠습니다 이를 통해 시스템의 다양한 상황을 고려하는 테스트 주도 개발의 중요성을 깨달았습니다 이후 기대할 사항들 복잡한 공중 공간에서 운행되는 UAM의 특성상 운항 안전과 공중 교통 통제가 매우 중요합니다 UAM 모니터링 시스템은 지역 내의 모든 UAM의 위치 정보와 이동 경로를 감시하며 비상 상황 발생 시 이를 빠르게 인지하여 후속 처치를 할 수 있도록 도울 수 있습니다 추후 지자체 발전에도 기여할 수 있는 방안을 찾아볼 예정입니다 대구시는 2023년 4월 20일 대구 UAM 육성협의회를 출범시키는 등 UAM의 고도화를 위해 많은 노력을 하고 있습니다 모니터링 시스템을 지자체 사업에 활용하여 도시 교통 혼잡도나 이동 패턴을 파악하여 효율적인 도시 교통 관리를 할 수 있습니다 모니터링 시스템에 의해 안정적인 비행이 보장되는 경우 UAM의 활용성은 무궁무진해집니다 새로운 대중교통으로서의 역할 뿐 아니라 응급 환자를 빠르게 병원으로 이송하거나 화물을 운반하는 역할도 수행할 수 있습니다 전통적인 지상 교통 시스템의 한계를 극복하고 도시 교통 체증에 대한 해결책으로 주목받는 UAM은 이를 보조하는 모니터링 시스템과 함께 운용될 때 더욱 시너지가 날 것으로 기대됩니다,https://i.ibb.co/Xz71My9/2024-01-04-173243.png,https://devocean.sk.com/blog/techBoardDetail.do?ID=165167&boardType=techBlog&searchData=&page=&subIndex=
